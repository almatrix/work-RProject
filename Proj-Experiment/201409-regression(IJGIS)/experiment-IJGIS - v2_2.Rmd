---
title: "Understanding Contextualized Mobility Patterns from Location-based Social Networks"
author: "Ming Li"
date: "Wednesday, September 10, 2014"
output: html_document
---

before trying infering mobility patterns from the dataset, the question must be answered: 

to what extend can mobility patterns be revealed from the dataset? 
is it predictable? how predictable is it?

```{r,echo=FALSE}
library(rgeos)
library(rgdal)
library(scales)
library(reshape2)
library(ggplot2)
library(gridExtra)
library(SDMTools)
#library(TSA)
library(ca)
library(plyr)
#library(MASS)#fitdistr

source("../../global/functions/prepare.checkin.R")
source("../../global/functions/basic.stats.plots.R")
source("../../global/functions/spatial.analysis.R")
source("../../global/functions/etc.R")
source("../../global/functions/truncated.power.law.R")

# ############
# global variable
ppi=300

crs.wgs84.str = "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"

city.guide=data.frame("city"=c("Chicago","Los Angeles","New York City"),
                      "coord.x"=c(-87.92,-118.64,-74.23),
                      "coord.y"=c(41.7,33.82,40.85),
                      "spatial.attr" = c("ZIP","Zip_Num","POSTAL"),
                      "shapefile.boundary" = c("bounds_ChicagoCity_WGS84",
                                               "bounds_LA_City_WGS84","NYC_borough_boundaries_WGS84"),
                      "shapefile.zip" = c("Chicago-ZipCodes","LA_Zipcodes","NYC_zipcode"),
                      "csvfile"=c("ChicagoCity","LosAngelesCity","NewYorkCity"),
                      "label"=c("Area:234.0 sq mi\nPopulation:2,695,598(2010)\nCheck-ins:183,837",
                                "Area:503 sq mi\nPopulation:3,884,307(2013)\nCheck-ins:138,211",
                                "Area:468.9 sq mi\nPopulation:8,405,837(2013)\nCheck-ins:579,786"),
                      stringsAsFactors=FALSE)

```


#### 1. some statistical results of the dataset

##### 1.1 Trajectory
```{r,echo=FALSE, fig.width=6, fig.height=6}
checkin.poly.list = lapply(1:3, function(city.index){
    checkin = prepare.checkin(paste0("../../global/data/csv-raw/",
                                     city.guide$csvfile[city.index],".csv"),
                             is.raw=TRUE, weather.data=NA, 
                             convert.time=TRUE, add.history=FALSE)
    SPDF = readOGR(dsn = "../../global/data/shapefiles", 
                   layer = city.guide$shapefile.zip[city.index])
    
    na.omit(point.in.poly(checkin, SPDF, copy.attr=city.guide$spatial.attr[city.index]))
})

save(checkin.poly.list,file="data/checkin.poly.list.Rda")
load("data/checkin.poly.list.Rda")

######
gg.maps = lapply(1:3, function(city.index){
    point.data = checkin.poly.list[[city.index]]
#     sample.index = sample(1:nrow(point.data),floor(nrow(point.data)*0.05))
    point.plot(point.data,
               alpha=0.3,size=1,fill=NA,color="#333333",
               basemap = map.plot(mapdir="../../global/data/shapefiles",
                                  maplayer=city.guide$shapefile.boundary[city.index],
                                  size=0.2,color="grey",fill=NA))+
        ggtitle(city.guide$city[city.index])+
        xlab("Longitude")+ylab("Latitude")+theme_bw(base_size = 10)+
        annotate("text",x=city.guide$coord.x[city.index],y=city.guide$coord.y[city.index],
                 size=3,hjust=0,label=city.guide$label[city.index])
})
tiff("plots/fig1.data.review.tiff", width=10*ppi,height=4*ppi,res=ppi)
grid.arrange(gg.maps[[1]],gg.maps[[2]],gg.maps[[3]],
             nrow=1, ncol=3, widths=c(1,0.85,1.15))
dev.off()
#####
```

##### 1.2 Data sparsity 

The check-in dataset, like other social media datasets, is sparse. Most of the records are created by only a small propotion of user. 

```{r}
personal.stat.frequency<-lapply(1:3,function(city.index){
    checkin = checkin.poly.list[[city.index]]
    # the observation of personal checkin frequency
    user.freq = as.data.frame(table(checkin$user_id)) 
    user.freq$city = city.guide$city[city.index]

    user.freq
})

personal.stat.density<-lapply(1:3,function(city.index){
    user.freq = personal.stat.frequency[[city.index]] # personal checkin frequency
    count.freq = as.data.frame(table(user.freq$Freq)) # stats on the observed frequncy
    colnames(count.freq)=c("Freq","Freq.Freq")
    count.freq$Freq=as.integer(as.character(count.freq$Freq)) # back to numbers
    count.freq$prob=count.freq$Freq.Freq/sum(count.freq$Freq.Freq) # probability of observing this frequency
    count.freq$city = city.guide$city[city.index]
    
    count.freq
                           
})

# cdf
personal.stat.cdf <- lapply(1:3,function(city.index){
    user.freq = personal.stat.frequency[[city.index]]
    cdf = summarize(user.freq,Frequency = unique(Freq), ecdf = ecdf(Freq)(unique(Freq)))
    cdf$city = city.guide$city[city.index]
    cdf
})

# simulating
truncated.para = lapply(1:3,function(city.index){
    model = nls(ecdf~truncated.plfit.cdf(Frequency,x0,beta,ka),
        data=personal.stat.cdf[[city.index]],trace=TRUE,algorithm="port",
        start=list(x0=0,beta=1,ka=100),
        lower=c(x0=0,beta=1,ka=100),upper=c(x0=5,beta=3,ka=800))
    summary(model)$parameters[,1]
    
})
truncated.para = data.frame(do.call(rbind,truncated.para),"city"=city.guide$city)
save(truncated.para,file="data/truncated.para.Rda")
load("data/truncated.para.Rda")
truncated.para$city=city.guide$city

truncated.simu = ddply(truncated.para,.(city),summarize,
    x = c(1:1000,2000,4000),
    fx = truncated.plfit.df(c(1:1000,2000,4000),x0,beta,ka),
    Fx = truncated.plfit.cdf(c(1:1000,2000,4000),x0,beta,ka))
truncated.eqs = ddply(truncated.para,.(city),function(i){
    truncated.plfit.print(i$x0,i$beta,i$ka)
})

######################

gg.fx = ggplot(do.call(rbind,personal.stat.density))+
    geom_line(data=truncated.simu,aes(x=x,y=fx),size=0.8,color="#999999")+
    geom_point(aes(x=Freq,y=prob),size=1.2)+
#     geom_smooth(aes(x=Freq,y=prob,linetype="Power Law"),se=F,color="blue",alpha=0.8,
#                 formula=y~x,method=lm)+
    scale_x_log10(breaks=c(10,30,100,1000))+scale_y_log10(limits=c(0.00001,0.4),label=percent)+
    xlab("Personal check-in frequency [x]")+ylab("PMF [Pr(X=x)]")+
    geom_text(data=truncated.eqs,x=1,y=-1,aes(label=V1),
              inherit.aes=FALSE,parse = TRUE,hjust=0,size=2.2)+
    facet_wrap(~city)+theme_bw(base_size = 10)
#     theme(strip.text=element_text(size=8),axis.text = element_text(size=8),
#               axis.title = element_text(size=8),plot.title = element_text(size=10),
#           plot.margin=unit(c(.05,.05,.05,.05),"npc"))
gg.Fx=ggplot(do.call(rbind,personal.stat.cdf))+
    geom_line(data=truncated.simu,aes(x=x,y=Fx),size=0.8,color="#999999")+
    geom_point(aes(x=Frequency,y=ecdf),size=1.2)+
#     geom_smooth(aes(x=Frequency,y=ecdf,linetype="Power Law"),se=F,color="black",alpha=0.8,
#                 formula=y~x,method=glm,family=gaussian(link='log'))+
    scale_x_log10(breaks=c(10,30,100,1000))+scale_y_continuous(label=percent)+
    labs(x="Personal check-in frequency [x]",y="CDF [Pr(X<=x)]")+
    facet_wrap(~city)+theme_bw(base_size = 10)
#     theme(strip.text=element_text(size=8),axis.text = element_text(size=8),
#               axis.title = element_text(size=8),plot.title = element_text(size=10),
#           plot.margin=unit(c(.05,.05,.05,.05),"npc"))
# tiff("plots/truncated.plfit.tiff",width=8*ppi,height=3.5*ppi,res=ppi)
# svg("plots/truncated.plfit.svg",width=4*ppi,height=1.75*ppi)
# pdf("plots/fig2.truncated.plfit.pdf",width=7,height=4,family="Arial",colormodel="grey")
cairo_ps("plots/fig2.truncated.plfit.ps",width=7,height=4,antialias="gray")
grid.arrange(gg.fx,gg.Fx,nrow=2)
dev.off()

```


##### 1.3 User similarity

1. each user is represented by a N-dimension vector describing the venues he/she has been to.
2. clustering the points in the N-dimension space

```{r}
nr.cls.candidates = list(c(200,500,1000,2500,5000,7500,10000,12500,15000),
                   c(200,500,1000,2000,3000,4000,5000,6000),
                   c(200,500,1000,2000,3000,4000,5000,6000))
clusters.list = lapply(1:3,function(city.index){ # for each of the three datasets
     
    checkin = checkin.poly.list[[city.index]] # get the ith checkin dataset
    
    user.list.all = split(checkin,checkin$user_id)
    # create user vector
    counter.reset(); time=Sys.time();
    point.vectors = sapply(user.list.all,function(user){
        counter.print(500)
        table(user$cate_l2)
    })
    time.print(Sys.time(), time, paste("creating user vectors for city",
                                       city.index,"takes"))
    
    # now we should discuss how we decide the number of clusters.
    # It should be a compromise between the between_variances and 
    # the records in each cluster. We should keep as much variance 
    # as possible, while make sure there are still enough records
    # in each cluster for valid statistics.
    # ! computational heavy !
    nr.clusters = nr.cls.candidates[[city.index]]
    clusters = lapply(nr.clusters,function(i){
       time = Sys.time();
       clusters = kmeans(t(point.vectors),centers=i,iter.max = 1000)
       time.print(Sys.time(), time, "creating user clusters takes")
            
       pct.ss = clusters$betweenss / clusters$totss
       user.sizes = sapply(user.list.all,function(user){
           nrow(user)
       })
       agg.sizes = as.data.frame(xtabs(data=data.frame("size"=user.sizes,
                                                       "cls"=clusters$cluster),
                                       size~cls))
            
       list(clusters,pct.ss,agg.sizes)
    })
    # save the result out because it is quite heavy computation
    save(clusters,file=paste0("data/clusters_",city.index,".Rda"))
    
    clusters        
})
save(clusters.list, file="data/clusters.list.Rda")
load("data/clusters.list.Rda")
gg.usercls = list();gg.stat=list();
theme_new<-theme_set(theme_bw(base_size = 10))
theme_new<-theme_update(legend.justification=c(0,0),legend.position=c(0,0),
                        legend.text = element_text(size=9),
                        legend.title=element_text(size=8),
                        legend.key.height=unit(1,"line"),
                        legend.key.width=unit(1,"line"))
lapply(1:3,function(city.index){
    stats=data.frame()
    gg.usercls[[city.index]] <<- lapply(clusters.list[[city.index]],function(i){
    #     idx<<-idx+1
        
        gg<-ggplot(i[[3]])+
            geom_histogram(aes(x=Freq,y=..density..),binwidth=0.05,fill=NA,color="grey")+
            geom_density(aes(x=Freq),adjust=2)+
    #         scale_x_log10(breaks=c(1,10,30,xintercepts[idx],100,1000),limits=c(2,3000))+
            scale_x_log10(breaks=c(10,30,100,1000),limits=c(2,3000))+
            scale_y_continuous(limits=c(0,1.8))+
            xlab("Number of records per cluster")+
            ylab("Probability density")+
#             theme(axis.text.x=element_text(face=c("plain","bold","plain","plain")))+
    #         geom_vline(xintercept=xintercepts[idx])+
            annotate("text",x=100,y=1.6,size=2.7,
                      label=paste("Between SS / Total SS:",format.percent(i[[2]]),
                                  "\nClusters (> 30 records):",
                                  format.percent(length(which(i[[3]]$Freq>=30))/nrow(i[[3]]))))+
            ggtitle(paste0(nrow(i[[3]])," Clusters (",city.guide$city[city.index],")"))
#             theme(strip.text=element_text(size=8),axis.text = element_text(size=8),
#                       axis.title = element_text(size=8),plot.title = element_text(size=10),
#                   plot.margin=unit(c(.05,.05,.05,.05),"npc"))
    
        stats <<- rbind(stats,data.frame("Clusters"=nrow(i[[3]]),
                            "heteg.preserv."=i[[2]],
                            "over.30"=length(which(i[[3]]$Freq>=30))/nrow(i[[3]])))
        gg                     
    })
    
    
    gg.stat[[city.index]] <<- ggplot(melt(data=stats,id.vars="Clusters"),aes(x=Clusters,y=value))+
        geom_line(aes(group=variable))+geom_point(aes(shape=variable,group=variable))+
    #     theme_grey(base_size = 10)+
        ggtitle(paste0("Trend (",city.guide$city[city.index],")"))+
        xlab("Number of clusters [k]")+
        ylab("Proportion")+
        scale_shape_discrete(name="Criteria")+
        scale_y_continuous(labels=percent)
#         theme(legend.justification=c(0,0),legend.position=c(0,0),
#               legend.text = element_text(size=6),legend.title=element_text(size=8),
#               legend.key.height=unit(1,"line"),legend.key.width=unit(1,"line"))

#         theme(strip.text=element_text(size=8),axis.text = element_text(size=8),
#               axis.title = element_text(size=8),plot.title = element_text(size=10),
#               
#               plot.margin=unit(c(.05,.05,.05,.05),"npc"))
    
    NA

})
cairo_ps("plots/fig3.user.agg.ps",width=8,height=4.5,antialias = "gray")
# pdf("plots/fig3.user.agg.pdf",width=8,height=4.5,colormodel="grey")
grid.arrange(gg.usercls[[1]][[6]],gg.usercls[[1]][[3]],gg.usercls[[1]][[1]],
             gg.stat[[1]]+annotate("rect",xmin=800,xmax=1200,ymin=0.9,ymax=1.05,fill=NA,color="grey"),
             gg.stat[[2]]+annotate("rect",xmin=800,xmax=1200,ymin=0.9,ymax=1.05,fill=NA,color="grey"),
             gg.stat[[3]]+annotate("rect",xmin=2100,xmax=2900,ymin=0.9,ymax=1.05,fill=NA,color="grey"),
             ncol=3,nrow=2)
dev.off()

# choose the number of clusters based the plotting result
nr.cls.final.id = c(4,3,3) # 2500, 1000, 1000
```
```{r}

# user aggregation and respliting
user.list.ucls = lapply(1:3,function(city.index){
#     counter.reset();
#     checkin = checkin.poly.list[[city.index]]
#     checkin$cluster.id = sapply(checkin$user_id,function(user){
#         counter.print(4000)
#         clusters.in.city = clusters.list[[city.index]]
#         clusters.final = clusters.in.city[[nr.cls.final.id[city.index]]]
#         clusters.final[[1]]$cluster[as.character(user)] # the cluster id for that user
#     })
#     save(checkin,file=paste0("data/checkin.poly.cls_",city.index,".Rda"))
    load(paste0("data/checkin.poly.cls_",city.index,".Rda"))
    split(checkin,checkin$cluster.id)
})


```


#### 2. Entropy

##### 2.1 calculation of entropies

```{r,eval=FALSE}
##########################
# computing 
# random entropy
get.rand.entropy.etc = function(data){
    N = length(unique(data$venue_id))
    entropy = log2(N) 

    pi.max = get.pi.max.from.entropy(entropy, N)
    
    pi.min = 1 / N
    
#     u2 = 1 - 1 / N
    
    data.frame("Condition"="Random","Entropy"=entropy,
      "Pi.max"=pi.max,"Pi.min"=pi.min)
}

# uncorrelated entropy (heterogeneity pattern)
get.unc.entropy.etc = function(data){
    data$venue_id=factor(data$venue_id) # do not statistic none-used venue
    venue.freq = as.data.frame(table(data$venue_id))
    
    p = venue.freq$Freq / sum(venue.freq$Freq)
    entropy = -1 * sum(p * log2(p))
    
    N = length(levels(data$venue_id))
    pi.max = get.pi.max.from.entropy(entropy, N)
    
    pi.min = max(venue.freq$Freq) / sum(venue.freq$Freq)
    
    u2 = 1 - sum(p^2)
    
    data.frame("Condition"="Uncorrelated","Entropy"=entropy,
      "Pi.max"=pi.max,"Pi.min"=pi.min,
      "Unalikeability"=u2)
}

# uncorrelated entropy with small sample size
get.unc.entropy.etc.v2 = function(data,prior.freq){
    # get the inital probability distribution
    venue.freq = get.freq.dataframe(data=data,"venue_id")
    N = sum(venue.freq$Freq)
    L = length(unique(data$venue_id))
    
    # get the adjusted probabilty distribution
    prior.freq = merge(x=venue.freq, y=prior.freq, by=c("venue_id"),all.x=T)
    prior.freq$p.adjusted = with(prior.freq,get.p.adjusted(p.joint.x, N, p.joint.y))
    
    # calculate entropy based on probability
#     entropy.init = with(venue.freq, sum(-1 * p.joint * log2(p.joint) ))
#     entropy = with(prior.freq, sum(-1 * p.adjusted * log2(p.adjusted) ))
    entropy = with(venue.freq, sum(-1 * p.joint * log2(p.joint) + 1/2/N))
    
    # get pi.max based on entropy
#     pi.max.init = get.pi.max.from.entropy(entropy.init, L)
    pi.max = get.pi.max.from.entropy(entropy, (L+1))
    
    # get pi.min based on probability disitribution
    pi.min = max(prior.freq$p.adjusted)
#     pi.min.init = max(venue.freq$p.joint)
    
#     u2 = 1 - sum(p^2)
    
    data.frame("Condition"="Uncorrelated","Entropy"=entropy,
      "Pi.max"=pi.max,"Pi.min"=pi.min)
}

# conditional entropy 
get.conditional.entropy.etc = function(data,condition,condition_name){
    data$venue_id=factor(data$venue_id) # do not statistic none-used venue
    freq = as.data.frame(xtabs(data=data,
                               as.formula(paste("~",condition,"+venue_id"))))
    freq = freq[freq$Freq>0,]
    freq = ddply(freq,condition,function(i){
        i$marg = sum(i$Freq)
        i$max = max(i$Freq)
        i$p = i$Freq / i$marg 
        
        i$entropy = -1 * sum(i$p * log2(i$p))
        i$pi.max = get.pi.max.from.entropy(i$entropy,nrow(i))
        i$pi.min = i$max / i$marg
        i$u2 = 1 - sum(i$p^2)
        
        i$condition = condition_name
        
        i
    })
    freq$cluster.id = data[1,"cluster.id"]
    colnames(freq)=c("Condition_ID",colnames(freq)[2:12])
    
    p.joint = freq$Freq / sum(freq$Freq)
    entropy = sum(p.joint * log2(freq$marg / freq$Freq))

    N = length(levels(data$venue_id))
    pi.max = get.pi.max.from.entropy(entropy,N)
    pi.min = with(freq,weighted.mean(pi.min,Freq))
    u2 = 1 - sum(p.joint^2)
    

    list("mean"=data.frame("Condition"=condition_name,"Entropy"=entropy,
                  "Pi.max"=pi.max,"Pi.min"=pi.min,"Unalikeability"=u2),
         "detail"=freq)
}

# conditional entropy adjusted to small sample size
get.conditional.entropy.etc.v2 = function(data,cond.col,condition.name,prior.freq){

    freq.joint = get.freq.dataframe(data=data,"venue_id",cond.col,
                                    p.cond=T,p.joint=F)
    
    N = sum(freq.joint$Freq)
    L = length(unique(data$venue_id))
    
    in.condition = ddply(freq.joint, .(condition), function(i){
        n = sum(i$Freq)
        l = length(unique(i$venue_id))

        prior.freq = merge(x=i, y=prior.freq, by=c("obs"),all.x=T)
        prior.freq$p.adjusted = with(prior.freq, 
                                     get.p.adjusted(p.cond.x, n, p.cond.y) )
#         # adjusted for small samples (Mueller, 1955)
        H.v = with(i,sum( -1 * p.cond * log2(p.cond) + 1/2/n))
#         # adjusted for small samples ("entropy estimates of small datasets")
#         H.v2 = sum(apply(i,1,function(j){
#             ni = as.integer(j["Freq"]);n=as.integer(j["marg.freq"])
#             (ni+1)/(n+2) * sum(1/c((ni+2):(n+2)))
#         }))
#         H.v.init = with(i,sum( -1 * p.cond * log2(p.cond) ))
#         H.v = with(prior.freq, sum(-1 * p.adjusted * log2(p.adjusted)))

        pi.max = get.pi.max.from.entropy(H.v, l+1) # one for "others"
#         pi.max.init = get.pi.max.from.entropy(H.v.init, l+1)

        pi.min = max(prior.freq$p.adjusted)
#         pi.min.init = max(prior.freq$p.cond.x)

        data.frame(#"condition.value"=as.character(i[1,"condition"]),
                   "entropy"=H.v,
                   "pi.min"=pi.min,
                   "pi.max"=pi.max,
                   "record"=n)
    })
    
    H = sum(with(in.condition, record / N * entropy))
    pi.max = get.pi.max.from.entropy(H, N)
    pi.min = with(in.condition, weighted.mean(pi.min, record))
    
    list("mean"=data.frame("Condition"=condition.name,"Entropy"=H,
               "Pi.max"=pi.max,"Pi.min"=pi.min),
         "detail"=in.condition)
}

# get.pi.min.from.freq = function(freq,L,confidence=0.95){
#     # pi.min should also consider the small sample correction
#     # using Wilson score interval
#     # extended from Wilson 1927
#     if(sum(freq$Freq)>0){
#         p = max(freq$Freq) / sum(freq$Freq)
#         n = sum(freq$Freq) # number of total observations
# #         l = ifelse(nrow(freq)<2,2,nrow(freq)) # number of unique options
#         z = qnorm(1-1/2*(1-confidence))  # confidence interval 
#         
#         # CI should be weighted average of p and 1/l
#         n/(n+z^2) *p + (z^2)/(n+z^2) * 1/L
#     }else{NaN}
# }

get.p.adjusted = function(p,n,prior,confidence=0.95){
    # pi.min should also consider the small sample correction
    # using Wilson score interval
    # extended from Wilson 1927
#     if(n>0){
#         p = freq$Freq / sum(freq$Freq)
#         n = sum(freq$Freq) # number of total observations
#         l = ifelse(nrow(freq)<2,2,nrow(freq)) # number of unique options
        z = qnorm(1-1/2*(1-confidence))  # confidence interval 
        
        # CI should be weighted average of p and prior 
        n/(n+z^2) *p + (z^2)/(n+z^2) * prior
#     }else{NaN}
}

get.pi.max.from.entropy = function(entropy,N){
    x = seq(from=0,to=1,length=1001)
    y = rep(0,1001)
    if(N!=1){
        y[1]=log2(N-1)
        y[2:1000] = (1-x[2:1000])*log2(N-1) - x[2:1000]*log2(x[2:1000]) - (1-x[2:1000])*log2(1-x[2:1000]) 
    }
    yoffset = y[2:length(y)]
    yoffset[length(y)]=-0.001
    
    if(length(entropy)==1){
        ans = ifelse(entropy<=max(y),x[which( y>=entropy & yoffset<entropy )],
           x[which(y==max(y))] )
    } else{
        ans = sapply(entropy,function(i){
            ifelse(i<=max(y),x[which( y>=i & yoffset<i )],
                   x[which(y==max(y))] )
        })
    }
    
    ans
}

get.freq.dataframe = function(data,obs.col,cond.col=NA,p.joint=T,p.cond=F,p.marg=F){
    
    if(length(cond.col)==1){
        if(is.na(cond.col)){
            freq = as.data.frame(xtabs(data=data,paste("~",obs.col),drop.unused.levels=T))  
        }else{
            data$new.col = paste(data[,obs.col],data[,cond.col],sep="@")
            freq = as.data.frame(xtabs(data=data,~new.col,drop.unused.levels=T))
            col.info = data.frame(do.call(rbind,strsplit(as.character(freq$new.col),
                                                       "@",fixed=TRUE)))
            colnames(col.info)=c(obs.col,"condition")
            freq = cbind(col.info,freq)
            colnames(freq)=c(obs.col,"condition","obs","Freq")
            
            marg.freq = ddply(freq,.(condition),function(in.condition){
                sum(in.condition$Freq)
            })
            colnames(marg.freq)[2]="marg.freq"
            
            freq = merge(x=freq,y=marg.freq,all.x=T)
    }}else{
        data$condition = do.call(paste,lapply(cond.col,function(col){
            data[,col]
        }))
        data$new.col = paste(data[,obs.col],data$condition,sep="@")
        
        freq = as.data.frame(xtabs(data=data,~new.col,drop.unused.levels=T))
        col.info = data.frame(do.call(rbind,strsplit(as.character(freq$new.col),
                                                   "@",fixed=TRUE)))
        freq = cbind(col.info,freq)
        colnames(freq)=c(obs.col,"condition","obs","Freq")
        
        marg.freq = ddply(freq,.(condition),function(in.condition){
            sum(in.condition$Freq)
        })
        colnames(marg.freq)[2]="marg.freq"
        
        freq = merge(x=freq,y=marg.freq,all.x=T)
    }
    
 
#     freq = as.data.frame(xtabs(data=data,...))
#     if(drop.unused){
#         freq = subset(freq, Freq>0)
#     }

    
    if(p.joint){
        freq$p.joint = with(freq, Freq / sum(Freq))
    }
    
    if(p.cond){
        if(length(cond.col)==1 && is.na(cond.col)){
            stop("you must specify the condition for conditional probability.")}
        freq$p.cond = with(freq, Freq / marg.freq)
    }
    
    if(p.marg){
        if(length(cond.col)==1 && is.na(cond.col)){
            stop("you must specify the condition for marginal probability.")}
        freq$p.marg = with(freq, marg.freq / sum(Freq))
    }
    
    freq
}

######
entropies.etc.list = lapply(1:3,function(city.index){
    user.list = user.list.ucls[[city.index]]
    data.global = checkin.poly.list[[city.index]]
    spatial.attr = city.guide$spatial.attr[city.index]
    
    prior.unc = get.freq.dataframe(data=data.global,"venue_id")
    prior.time = get.freq.dataframe(data=data.global,"venue_id","hour",
                                    p.joint=F,p.cond=T)
    prior.space = get.freq.dataframe(data=data.global,"venue_id",spatial.attr,
                                     p.joint=F,p.cond=T)
    prior.st = get.freq.dataframe(data=data.global,"venue_id",c("hour",spatial.attr),
                                  p.joint=F,p.cond=T)


    counter.reset()
    lapply(user.list,function(i){
        counter.print(10)

        rand = get.rand.entropy.etc(i)
        unc = get.unc.entropy.etc.v2(i,prior.unc)
        time = get.conditional.entropy.etc.v2(i,"hour","Hour",prior.time)
        space = get.conditional.entropy.etc.v2(i,spatial.attr,"Space",prior.space)
        st = get.conditional.entropy.etc.v2(i,c("hour",spatial.attr),"ST",prior.st)
        
        total.records = nrow(i)
        unique.venues = length(unique(i$venue_id))
        unique.users = length(unique(i$user_id))

        df = rbind(rand,unc,time$mean,space$mean,st$mean)
        df$Cluster.id = i[1,"cluster.id"]
        df$Records = nrow(i)
        df$Venues = length(unique(i$venue_id))
        df$Users = length(unique(i$user_id))
        df$City = city.guide$city[city.index]
        
#         time.detail = as.data.frame(time$detail)
#         time.detail$city = city.guide$city[city.index]
#         space.detail = as.data.frame(space$detail)
#         space.detail$city = city.guide$city[city.index]
        st.detail = as.data.frame(st$detail)
#         st.detail=unique(st$detail[,-c(2,3,6)])
        st.detail$city = city.guide$city[city.index]

        
        list(df,st.detail)
    })
    
})
```

load entropy, and some related computation
```{r}
#save(entropies.etc.list,file="data/entropies.etc.list.Rda")
load("data/entropies.etc.list.Rda")

entropies.etc.df=do.call(rbind,lapply(1:3,function(city.index){
    entropies.etc.city = entropies.etc.list[[city.index]]
    df = do.call(rbind,lapply(1:length(entropies.etc.city),function(i){
        entropies.etc.city[[i]][[1]]
    }))
    df$City = city.guide$city[city.index]
    df
}))
entropies.etc.df$delta = with(entropies.etc.df,(Pi.max-Pi.min)/Pi.min)

entropies.etc.st.df=do.call(rbind,lapply(1:3,function(city.index){
    entropies.etc.city = entropies.etc.list[[city.index]]
    df = do.call(rbind,lapply(1:length(entropies.etc.city),function(i){
        entropies.etc.city[[i]][[2]]
    }))
    df$city = city.guide$city[city.index]
    df
    
}))


time.space = data.frame(do.call(rbind,strsplit(as.character(entropies.etc.st.df$condition),
                                                   " ",fixed=TRUE)))
colnames(time.space)=c("hour","space")
entropies.etc.st.df = cbind(entropies.etc.st.df,time.space) 


```#

```{r}
## plotting
data.pi.density = entropies.etc.df[which(entropies.etc.df$Condition %in% c("Hour","Space","ST")),c(1,3,4,6,9)]
data.pi.density = melt(data=data.pi.density,id.vars=c("Condition","City","Records"))
data.pi.density$Condition = as.character(data.pi.density$Condition)
data.pi.density$Condition[which(data.pi.density$Condition=="Hour")]="Time"
data.pi.density$Condition = factor(data.pi.density$Condition)
data.pi.density$Condition = relevel(data.pi.density$Condition,"Time")


pdf("plots/fig4.pi.density.pdf",width=7,height=3.5,colormodel="grey")
ggplot(data.pi.density)+
    geom_histogram(aes(x=value,y=..density..,group=variable),binwidth=0.01,fill=NA,
                   color="grey",position="identity")+
    geom_density(aes(x=value,group=variable,linetype=variable),adjust=3,alpha=0.5)+
    xlab(expression(atop(italic(Pi)) ) ) +
    scale_x_continuous(labels=percent)+
    ylab("Probability density")+
#     scale_y_sqrt()+
    scale_linetype_manual(name="",values=c(2,1),breaks=c("Pi.max","Pi.min"),
                          labels=c(expression(atop(italic(Pi))^"max" ),
                                   expression(atop(italic(Pi))^"min" )))+
    facet_grid(Condition~City)+theme_bw(base_size = 10)
dev.off()

```

entropy analysis from individual perspective
1. correlation
```{r}
data.individual.coor = melt(entropies.etc.df[which(entropies.etc.df$Condition=="ST"),c(3,4,6,9,10)],
                            id.vars=c(3,4))
data.individual.coor$variable<-factor(data.individual.coor$variable,
                                      labels=c("max","min",expression(delta)))

pdf("plots/fig5.coor.indiv.pdf",width=7,height=3.5,colormodel="grey")
ggplot(data.individual.coor,
       aes(x=Records,y=value))+
    facet_grid(variable~City,scale="free",
               labeller=labeller(variable=label_bquote(Pi["ST"]^.(x))))+
    geom_point(size=1)+scale_x_log10()+scale_y_continuous(labels=percent)+
    labs(x="Individual check-in frequency",y=expression(Pi["ST"]))+theme_bw(base_size = 10)
#     theme(axis.text = element_text(size=8),legend.position="none",
#               axis.title = element_text(size=10),plot.title = element_text(size=10))
dev.off()

```

entropy analysis from temporal perspective
1. correlation
```{r}
data.hour.coor = ddply(entropies.etc.st.df,.(city),function(city){
    stat = ddply(city,.(hour),function(hour){
        data.frame("pi.max"=with(hour,mean(pi.max)),
                   "pi.min"=with(hour,mean(pi.min)),
                    "record"=with(hour,sum(record)))
#         data.frame("record"=with(hour,sum(record)),
#                    "pi"=with(hour,c(mean(pi.min),mean(pi.max))),
#                    "type"=c("min","max"))
    })
#     stat$record = stat$record / sum(stat$record) 
    stat
})
data.hour.coor$delta = with(data.hour.coor,(pi.max-pi.min)/pi.min)
data.hour.coor=melt(data.hour.coor,id.vars=c(1,2,5))
data.hour.coor$variable<-factor(data.hour.coor$variable,
                                labels=c("max","min",expression(delta)))

eqs <- ddply(data.hour.coor,.(city,variable),function(i){
    model = lm(i,formula=value~log(record)) 
    eq <- substitute(atop(italic(r)^2~"="~r2*","~~italic(p)~"="~pvalue),
                     list(r2 = format(summary(model)$r.squared, digits = 3),
                          pvalue = formatC(summary(model)[[4]][[8]],format="f")))
    as.character(as.expression(eq));                 
})
eqs$pos.x=rep(c(5000,3500,20000),each=3)
eqs$pos.y=rep(c(0.88,0.5,0.75),3)
# eqs2 <- ddply(data.hour.coor,.(city,variable),function(i){
#     model = lm(i,formula=value~record) 
#     eq <- substitute(atop(italic(r)^2~"="~r2*","~~italic(p)~"="~pvalue),
#                      list(r2 = format(summary(model)$r.squared, digits = 3),
#                           pvalue = format(summary(model)[[4]][[8]],scientific=TRUE, digits = 3)))
#     as.character(as.expression(eq));                 
# })
# 
pdf("plots/fig6.coor.time.pdf",width=7,height=3.5,colormodel = "grey")
ggplot(data.hour.coor,aes(x=record,y=value))+#geom_point(size=3,shape=21,fill=NA)+
    facet_grid(variable~city,scales="free",
               labeller=labeller(variable=label_bquote(Pi["ST"]^.(x))))+
#     geom_smooth(method=loess,formula=y~x)+
    geom_smooth(method=lm,formula=y~log(x),se=F,color="grey")+
    geom_text(data=data.hour.coor[!(data.hour.coor$hour %in% c("01","05","19")),],aes(label=hour),size=2)+
    geom_text(data=data.hour.coor[which(data.hour.coor$hour %in% c("01","05","19")),],aes(label=hour),size=3)+
    xlab("Hourly check-in frequency") +
    ylab(expression(Pi["ST"])) +
#     ggtitle(expression(paste("Temporal Correlation (",Pi["ST"]," and Activeness)")))+
#     scale_x_log10()+
    scale_y_continuous(labels=percent)+
    geom_text(data=eqs,aes(x=pos.x,y=pos.y,label=V1),inherit.aes=FALSE,
                  parse = TRUE,hjust=0,vjust=1,size=3)+theme_bw(base_size = 10)
#     theme(axis.text = element_text(size=8),legend.position="none",
#               axis.title = element_text(size=10),plot.title = element_text(size=10))
dev.off()
```

2. histogram/density
```{r}
hour.stat = do.call(rbind,lapply(1:3,function(city.index){
    data.total = checkin.poly.list[[city.index]][,c("hour","cate_l1")]
#     data.stat = data.hour.coor[which(data.hour.coor$city==city.guide$city[city.index]),]
#     data.stat = merge(x=data.total,y=data.stat,
#                          by="hour",all.y=T)
    
    stat.byhour=ddply(data.total,.(hour),function(i){
        df = as.data.frame(xtabs(~cate_l1,data=i))
        df$prob = with(df,Freq/sum(Freq))
        df
    })
#     stat=as.data.frame(xtabs(~cate_l1,data=data.total))
#     stat$prob=with(stat,Freq/sum(Freq))
#     stat.byhour$mean.prob = rep(stat$prob,24)

    stat.byhour$city = city.guide$city[city.index]
    stat.byhour
}))
# hour.stat$group = c(rep(c(rep("A",4),rep("B",4),rep("C",3),rep("D",11),rep("C",2)),
#                         each=10),
#                     rep(c(rep("A",4),rep("B",4),rep("C",3),rep("D",11),rep("C",2)),
#                         each=10),
#                     rep(c(rep("A",4),rep("B",4),rep("C",4),rep("D",10),rep("C",2)),
#                         each=10))
# hour.stat2=ddply(hour.stat,.(group,city),function(i){
#     ddply(i,.(cate_l1),function(j){
#         sum(j$Freq)/sum(i$Freq)
#     })
# })

pdf("plots/fig7.hist.time.pdf",width=7,height=4.5,colormodel = "grey")
theme_new<-theme_set(theme_bw(base_size = 10))
theme_new<-theme_update(legend.position="none",
          axis.text.x=element_text(angle=50,vjust=1,hjust=1))
ggplot(hour.stat[which(hour.stat$hour %in% c("01","05","19")),])+
    facet_grid(city~hour)+
#     geom_freqpoly(aes(x=as.integer(cate_l1),group=hour,color=hour),binwidth=1)
    geom_histogram(aes(x=cate_l1,y=prob),stat="identity")+
    geom_text(aes(x=cate_l1,y=prob,label=Freq,size=log(Freq)),vjust=-0.2)+
    scale_y_continuous(limits=c(0,0.6))+
    scale_size_continuous(range=c(0.3,3))+
    labs(y="PMF [Pr(X=x)]",x="Venue Category")
#     geom_histogram(aes(x=cate_l1,y=mean.prob),stat="identity",fill=NA,color="grey")
#     coord_flip()+
#     scale_y_log10()
#     scale_y_sqrt(breaks=c(100,2500,10000))+
#     scale_y_continuous(limit=c(0,10000))+
#     scale_y_log10(breaks=c(1,10,100,1000,10000))+
#     annotate("rect",xmin=4.1,xmax=5.9,ymin=5000,ymax=10000,alpha=0.3)+
#     geom_text(aes(label=type),x=5,y=85,color="white")+
#     theme(axis.text = element_text(size=7),legend.position="none",
#           axis.text.x=element_text(angle=45,vjust=1,hjust=1),
#           axis.title = element_text(size=10),plot.title = element_text(size=10))
dev.off()
```

entropy analysis from spatial perspective
```{r}
outlier.list = data.frame(
    "name" = c("60609","60612","60629","60666",
                 "90045",
                 "10452","11368","11371","11430",""),
    "type" =  c("B","B","A","A",
                 "A",
                 "B","B","A","A","C"),
    "desc" = c(
                "Baseball park",
                "Sports arena",
                "Airport",
                "Airport",
                "Airport",
                "Baseball park",
                "Baseball park",
                "Airport",
                "Airport",
                ""),
    "desc.2" = c("US Celluar Field",
                "United Center",
                "Chicago Midway International Airport",
                "Oâ€™Hare International Airport",
                "Los Angeles International Airport",
                "Yanke Stadium",
                "Citi Field",
                "LaGuardia Airport",
                "John F. Kennedy International Airport",
                "")
    )
outlier.list$name=as.character(outlier.list$name)
outlier.list$desc = as.character(outlier.list$desc)
outlier.list$desc.2 = as.character(outlier.list$desc.2)

```

1. correlation
```{r}
data.space.coor = ddply(entropies.etc.st.df,.(city),function(city){
    stat = ddply(city,.(space),function(space){
        data.frame("pi.max"=with(space,mean(pi.max)),
                   "pi.min"=with(space,mean(pi.min)),
                   "record"=with(space,sum(record)))
    })
    stat
})
data.space.coor$delta = with(data.space.coor,(pi.max-pi.min)/pi.min)
data.space.coor=melt(data.space.coor,id.vars=c(1,2,5))
data.space.coor$variable<-factor(data.space.coor$variable,
                                labels=c("max","min",expression(delta)))
data.space.coor$outlier<-with(data.space.coor,
                             ifelse(space %in% outlier.list$name,T,F))
data.space.coor$label<-with(data.space.coor,
                            ifelse(outlier,as.character(space),""))
data.space.coor=merge(x=data.space.coor,y=outlier.list,
                      by.x="label",by.y="name",all.x=T)

smooth.space<-data.frame()
eqs.space <- ddply(data.space.coor,.(city,variable),function(i){
    if(i[1,"variable"]=="max"){ 
        model = lm(i,formula=value~atan(log(record/max(record))))
        eqs.pred=data.frame(record = c(min(i$record):max(i$record)))
        }
    else if(i[1,"variable"]=="delta"){ 
        model = lm(i,formula=value~atan(log(record/100)))
        eqs.pred=data.frame(record = c(min(i$record):max(i$record)))
        }
    else{
        model = lm(i,formula=value~atan(log(record*100)))
        eqs.pred=data.frame(record = c(min(i$record):max(i$record)))
        }
#     model =lm(i,formula=value~log(record))
    
    # regression model statistics
    eq <- substitute(atop(italic(r)^2~"="~r2*","~~italic(p)~"="~pvalue),
                     list(r2 = format(summary(model)$r.squared, digits = 3),
                          pvalue = formatC(summary(model)[[4]][[8]],format="f")))
    eq <- as.character(as.expression(eq));

    # regression line
    eqs.pred$y = predict(model, newdata = eqs.pred);
    eqs.pred$variable=i[1,"variable"]
    eqs.pred$city=i[1,"city"]
    smooth.space <<- rbind(smooth.space,eqs.pred)
    
    eq
})
eqs.space$pos.x=rep(c(10,3,10),each=3)
eqs.space$pos.y=rep(c(0.7,1.2,2.7),3)


pdf("plots/fig8.coor.space.pdf",width=8,height=4.5,colormodel = "grey")
theme_new<-theme_set(theme_bw(base_size = 10))
#theme_new<-theme_update(legend.position="none")
ggplot(data.space.coor,aes(x=record,y=value))+
    facet_grid(variable~city,scales="free",
               labeller=labeller(variable=label_bquote(Pi["ST"]^.(x))))+
    geom_line(data=smooth.space,aes(x=record,y=y),color="grey")+
#     geom_smooth(method=lm,formula=y~atan(x-2),se=F)+
    geom_point(aes(shape=type,size=type))+
    geom_text(aes(label=label),vjust=1.5,size=2.5)+
    xlab("Check-in frequency by zip code") +
    ylab(expression(Pi["ST"])) +
    scale_x_log10()+
    scale_y_continuous(labels=percent)+
    scale_shape_manual(name="",labels=c("Outlier A","Outlier B","Normal Region"),values=c(17,15,16))+
    scale_size_manual(name="",labels=c("Outlier A","Outlier B","Normal Region"),values=c(2,2,0.8))+
    geom_text(data=eqs.space,aes(x=pos.x,y=pos.y,label=V1),inherit.aes=FALSE,
                  parse = TRUE,hjust=0,vjust=1,size=3)
dev.off()
```

2. histogram/density
```{r}
space.stat = do.call(rbind,lapply(1:3,function(city.index){
    data.total = checkin.poly.list[[city.index]][,c(city.guide$spatial.attr[city.index],"cate_l1")]
    colnames(data.total)[1]="space"
    stat.byspace=ddply(data.total,.(space),function(i){
        df = as.data.frame(xtabs(~cate_l1,data=i))
        df$prob = with(df,Freq/sum(Freq))
        df
    })

    stat.byspace$city = city.guide$city[city.index]
    stat.byspace$space = as.character(stat.byspace$space)
    stat.byspace
}))
space.stat=merge(space.stat,outlier.list,by.x="space",by.y="name")

pdf("plots/fig9.hist.space.pdf",width=7,height=4.5,colormodel = "grey")
theme_new<-theme_set(theme_bw(base_size = 10))
theme_new<-theme_update(legend.position="none",
          axis.text.x=element_text(angle=50,vjust=1,hjust=1))
ggplot(space.stat)+
    facet_wrap(~city+space,nrow=3)+
    geom_histogram(aes(x=cate_l1,y=prob),stat="identity")+
    geom_text(aes(x=cate_l1,y=prob,label=Freq,size=log(Freq)),hjust=1.2,vjust=0.5,angle=90,color="white")+
    geom_text(aes(x=5,y=0.75,label=type),size=3)+
    scale_size_continuous(range=c(0.3,3))+
    labs(y="PMF [Pr(X=x)]",x="Venue Category")
dev.off()
```

3. map the outliers
```{r}
# make statstics for the points (not category) in the outliers
venue.stat = lapply(1:3,function(city.index){
    data.total = checkin.poly.list[[city.index]][,c(city.guide$spatial.attr[city.index],"lat","lon")]
    colnames(data.total)[1]="space"
    data.total$venue_loc = with(data.total,paste0(lat,lon))
    stat=as.data.frame(xtabs(~venue_loc,data=data.total))
    
    stat = merge(stat,
                 unique(data.total[,c("venue_loc","lat","lon","space")]),
                 by="venue_loc",all.x=T)
    
    stat$city = city.guide$city[city.index]
    stat$space = as.character(stat$space)

    stat=merge(stat,outlier.list,by.x="space",by.y="name")
    stat
})

outlier.map<-lapply(1:3,function(city.index){
    theme_new<-theme_set(theme_bw(base_size = 8))
    theme_new<-theme_update(legend.position="none",
                            plot.margin=unit(c(0,0.1,0,0.05), "cm"))
    google.map <- get_map(
        location = make_bbox(lon,lat,data=venue.stat[[city.index]]), 
        zoom = 11,maptype = 'roadmap', color = 'bw')
    al1MAP = ggmap(google.map)+
        geom_point(data=venue.stat[[city.index]],
                   aes(x=lon,y=lat,size=Freq),
                   shape=21,fill="white",alpha=0.5)+
        scale_size_continuous(range=c(0.5,5))+
        xlab("Longitude")+ylab("Latitude")+
        ggtitle(paste("Outliers for",city.guide$city[city.index]))
    
    sub.df = split(venue.stat[[city.index]],venue.stat[[city.index]]$space)
    submap<-lapply(sub.df,function(i){
        zoom.level = ifelse(i[1,"space"] %in% c("60609","60629","60666","90045"),
                            13,ifelse(i[1,"space"]=="11371",15,14))
        google.submap <- get_map(
            location=make_bbox(lon,lat,data=i,f=0.1),
            zoom=zoom.level,maptype='roadmap',color='bw')
        sub = ggmap(google.submap)+
            geom_point(data=i,aes(x=lon,y=lat,size=Freq),
                       shape=21,fill="white",alpha=0.5)+
            scale_size_continuous(range=c(1,12))+
            xlab("Longitude")+ylab("Latitude")+
            ggtitle(paste("Postal Region",i[1,"space"],
                          "(",as.character(i[1,"type"]),")"))
    })
    
    list(al1MAP,submap)
})


outlier.table <- data.frame(a=1:9, b=1:9)
outlier.table$city = c(rep(city.guide$city[1],4),city.guide$city[2],
                       rep(city.guide$city[3],4))
outlier.table <- cbind(outlier.table,outlier.list[1:9,])


colnames(outlier.table)[3:7]=c("City","Postal region","Outlier Type",
                               "Geographic Phenomena","Dominant Venue")
theme_new<-theme_update(axis.text=element_blank(),axis.ticks=element_blank(),
                        axis.line=element_blank(),axis.title=element_blank(),
                        panel.border=element_blank(),panel.grid=element_blank())
gg.outlier.table <- ggplot(outlier.table,aes(x=a,y=b)) +
    geom_blank()+
#   geom_point(colour="blue") + 
#   geom_point(data=mydata[10:13, ], aes(x=a, y=b), colour="red", size=5) + 
  annotation_custom(tableGrob(outlier.table[,c(3:7)],
                              padding.v = unit(3, "mm"),
                              gpar.coretext = gpar(cex = 0.6),
                              gpar.coltext = gpar(cex = 0.6, fontface = "bold"),
                              gpar.rowtext = gpar(cex = 0.6, fontface = "italic")),
                    xmin=1, xmax=9, ymin=1, ymax=9)



# pdf("plots/fig10.outliers.map.pdf",width=10,height=7,colormodel = "grey")
jpeg("plots/fig10.outliers.map.jpeg",width=10*ppi,height=7*ppi,res=ppi)
grid.arrange(
    arrangeGrob(outlier.map[[1]][[1]],
                outlier.map[[1]][[2]][[1]],outlier.map[[1]][[2]][[2]],
                outlier.map[[1]][[2]][[3]],outlier.map[[1]][[2]][[4]],
                ncol=5),
    arrangeGrob(outlier.map[[2]][[1]],
                outlier.map[[2]][[2]][[1]],
                gg.outlier.table,
                ncol=3,widths=c(1,1,3)),
    arrangeGrob(outlier.map[[3]][[1]],
                outlier.map[[3]][[2]][[1]],outlier.map[[3]][[2]][[2]],
                outlier.map[[3]][[2]][[3]],outlier.map[[3]][[2]][[4]],
                ncol=5),
    ncol=1,nrow=3
    )
dev.off()


```
old method to plot
```{r}
gg.outliers<-lapply(1:3,function(city.index){
    SPDF = readOGR(dsn = "../../global/data/shapefiles", layer = city.guide$shapefile.zip[city.index])
    shape.df = df.from.spdf(SPDF)
#     shape.df[city.guide$spatial.attr[city.index]]="space"
    # get the center of each polygon
    centroids.df=as.data.frame(coordinates(SPDF))
    colnames(centroids.df)=c("lon.cent","lat.cent")
    centroids.df$id=rownames(centroids.df)
    # get the outliers
    poly.attr=unique(data.space.coor[
        which(data.space.coor$city==city.guide$city[city.index]),c(1,2,3,7,8)])
    poly.attr$space=factor(poly.attr$space)
    poly.attr$space<-factor(poly.attr$space)    
    shape.df2 = merge(y=shape.df,x=poly.attr,
                     by.y=city.guide$spatial.attr[city.index],
                     by.x="space",all.y=T)
   
    shape.df2 = merge(x=shape.df2,y=centroids.df,by="id",all.x=T)
    shape.df2 = shape.df2[order(shape.df2$order),] 

    theme_new<-theme_set(theme_bw(base_size = 10))
    theme_new<-theme_update(legend.position="none")
    map1<-map.plot(mapdf=shape.df2,color="grey",
                   more.aes = aes(fill=type),size=0.3)+
        ggtitle(paste0(city.guide$city[city.index],": Outliers"))+
        labs(x="Longitude",y="Latitude")+
        geom_text(data=unique(shape.df2[,c("label","lon.cent","lat.cent")]),
                  size=3,
                  aes(x=lon.cent,y=lat.cent,label=label))

    if(city.index==2){
        map1<-map1+
        scale_fill_manual(values=c("#666666","#fafafa"),
                          name="Outliers")
        
    }else{
        map1<-map1+
        scale_fill_manual(values=c("#999999","#cccccc","#fafafa"),
                          name="Outliers")
    }
    map1
})
pdf("plots/fig10.outliers.map.pdf",width=10,height=4,colormodel = "grey")
grid.arrange(gg.outliers[[1]],gg.outliers[[2]],gg.outliers[[3]],
             ncol=3,nrow=1,widths=c(1,0.85,1.15))
dev.off()
```


earlier analysis from spatial perspective
```{r}
data2 = lapply(1:3,function(city.index){
    subdf = entropies.etc.st.df[which(entropies.etc.st.df$city==city.guide$city[city.index]),]
    subdf$space = factor(subdf$space)
    stat = do.call(rbind,lapply(split(subdf,subdf$space),function(regional){
        data.frame("space"=regional[1,"space"],
                   "pi.max"=with(regional,mean(pi.max)),
                   "pi.min"=with(regional,mean(pi.min)),
                   "record"=with(regional,sum(record)),
                   "city"=city.guide$city[city.index],
                   "delta"=with(regional,mean(pi.max)-mean(pi.min)))
    }))
#     stat$record = stat$record / sum(stat$record) 
    
    stat$resid.pi.max = resid(lm(stat,formula=pi.max~exp(log(record,base=10)) ))
    stat$resid.pi.min = resid(lm(stat,formula=log(pi.min)~log(record)))
    stat$resid.max.T = with(stat,abs(resid.pi.max)>4*mean(abs(resid.pi.max)))
    stat$resid.min.T = with(stat,abs(resid.pi.min)>4*mean(abs(resid.pi.min)))
    stat$disp.name.max=with(stat,ifelse(resid.max.T,as.character(space),""))
    stat$disp.name.min=with(stat,ifelse(resid.min.T,as.character(space),""))
    stat
})

data3 = do.call(rbind,data2)
#data3 = melt(data3[,c(1:5)],id.vars=c(1,4,5))

eqs3 <- ddply(data3,.(city),function(i){
    model = lm(i,formula=pi.max~exp(log(record,base=10)) )
    eq <- substitute(atop(italic(r)^2~"="~r2*","~~italic(p)~"="~pvalue),
                     list(r2 = format(summary(model)$r.squared, digits = 3),
                          pvalue = formatC(summary(model)[[4]][[8]],format="f")))
    as.character(as.expression(eq));                  
})
eqs4 <- ddply(data3,.(city),function(i){
    i$pi.min.log=log(i$pi.min)
    i$record.log=log(i$record)
    model = lm(i,formula=pi.min.log~record.log )
    eq <- substitute(atop(italic(r)^2~"="~r2*","~~italic(p)~"="~pvalue),
                     list(r2 = format(summary(model)$r.squared, digits = 3),
                          pvalue = formatC(summary(model)[[4]][[8]],format="f")))
    as.character(as.expression(eq));                 
})

theme_new<-theme_set(theme_bw(base_size = 10))
theme_new<-theme_update(legend.position="none")
gga2<-ggplot(data3,aes(x=record,y=pi.max))+facet_grid(~city,scales="free_y")+
    geom_point(aes(shape=resid.max.T),size=1)+
    geom_text(aes(label=disp.name.max),size=2.5,vjust=1.2,hjust=0.5)+
    geom_smooth(method=glm,formula=y~exp(x),se=F)+
    xlab("Activeness (Spatial records)\n(a)") +
    geom_text(data=eqs3,aes(x=1,y=0.67,label=V1),inherit.aes=FALSE,
                  parse = TRUE,hjust=0,size=3)+
    ylab(expression(Pi^max)) +
    ggtitle(expression(paste("Spatial Correlation (",Pi^"max"," and Activeness)")))+
    scale_x_log10()+scale_y_continuous(labels=percent)+
    scale_shape_manual(name="Outlier",values=c(16,2))
#     theme(axis.text = element_text(size=8),legend.position="none",
#           plot.margin=unit(c(.05,.05,.05,.05),"npc"),
#               axis.title = element_text(size=10),plot.title = element_text(size=10))
ggb2<-ggplot(data3,aes(x=record,y=pi.min))+facet_grid(~city,scales="free_y")+
    geom_point(aes(shape=resid.min.T),size=1)+
    geom_text(aes(label=disp.name.min),size=2.5,vjust=1.2,hjust=0.5)+
    geom_smooth(method=glm,formula=y~x,family=gaussian(link='log'),se=F)+
    xlab("Activeness (Spatial records)\n(b)") +
    geom_text(data=eqs4,aes(x=1,y=0.1,label=V1),inherit.aes=FALSE,
                  parse = TRUE,hjust=0,size=3)+
    ylab(expression(Pi^min)) +
    ggtitle(expression(paste("Spatial Correlation (",Pi^"min"," and Activeness)")))+
    scale_x_log10()+scale_y_continuous(limits=c(0.1,1),labels=percent)+
    scale_shape_manual(name="Outlier",values=c(16,2))
#     theme(axis.text = element_text(size=8),legend.position="none",
#           plot.margin=unit(c(.05,.05,.05,.05),"npc"),
#               axis.title = element_text(size=10),plot.title = element_text(size=10))
pdf("plots/fig8.coor.spatial.pdf",width=8,height=4.5,colormodel = "grey")
grid.arrange(gga2,ggb2,ncol=1,nrow=2,heights=c(1,1))
dev.off()





```

```{r}
# analyze the outliers
outlier.stat = do.call(rbind,lapply(1:3,function(city.index){
    data.total = checkin.poly.list[[city.index]]
#     data.stat = data2[[city.index]]
#     outlier = data.stat[which(data.stat$resid.max.T|data.stat$resid.min.T),
#                         c("space","resid.max.T","resid.min.T")]
    outlier = 
    data.outlier = merge(x=data.total,y=outlier,
                         by.x=city.guide$spatial.attr[city.index],
                         by.y="space",all.y=T)
    
    stat=ddply(data.outlier,city.guide$spatial.attr[city.index],function(i){
        df = as.data.frame(xtabs(~cate_l1,data=i))
        df$resid.max.T = i[1,"resid.max.T"]
        df$resid.min.T = i[1,"resid.min.T"]
        df
    })
    colnames(stat)[1:3]=c("space","category","Freq")
    stat$space=factor(stat$space)
    stat$city = city.guide$city[city.index]
    stat
}))
outlier.stat$type=rep(c("A","B","A","C","A","B","B","C","D","C","A","A"),each=10)

theme_new<-theme_set(theme_bw(base_size = 10))
theme_new<-theme_update(legend.position="none",
          axis.text.x=element_text(angle=45,vjust=1,hjust=1))
gg.outliers = ggplot(outlier.stat,aes(x=category,y=Freq))+facet_wrap(city~space)+
    geom_histogram(stat="identity",fill="black")+
    geom_text(aes(label=Freq,size=log(Freq)),hjust=1.2,vjust=0.5,angle=90,color="white")+
    scale_size_continuous(range=c(0.3,3))+
#     coord_flip()+
#     scale_y_sqrt(breaks=c(100,2500,10000))+
#     scale_y_continuous(limit=c(0,10000))+
    scale_y_log10(breaks=c(1,10,100,1000,10000))+
#     annotate("rect",xmin=5,xmax=6,ymin=3000,ymax=30000,alpha=0.3)+
    geom_text(aes(label=type),x=5.5,y=3.5,size=3.5)+
#     annotate("rect",xmin=4.1,xmax=5.9,ymin=5000,ymax=10000,alpha=0.3)+
#     geom_text(aes(label=type),x=5,y=85,color="white")+
    labs(y="Frequency",x="Venue Category")
#     theme(axis.text = element_text(size=7),legend.position="none",
#           axis.text.x=element_text(angle=45,vjust=1,hjust=1),
#               axis.title = element_text(size=10),plot.title = element_text(size=10))
    
pdf("plots/fig9.coor.spatial.2.pdf",width=7,height=4.5,colormodel = "grey")
gg.outliers 
dev.off()
```

```{r}
# map the outliers 
gg.maps<-lapply(1:3,function(city.index){
    SPDF = readOGR(dsn = "../../global/data/shapefiles", layer = city.guide$shapefile.zip[city.index])
    shape.df = df.from.spdf(SPDF)
    # get the center of each polygon
    centroids.df=as.data.frame(coordinates(SPDF))
    colnames(centroids.df)=c("lon.cent","lat.cent")
    centroids.df$id=rownames(centroids.df)
    outlier.stat.unique=merge(data2[[city.index]],unique(outlier.stat[,c(1,7)]),
                              by="space",all.x=T)
    outlier.stat.unique$type=as.factor(outlier.stat.unique$type)
    outlier.stat.unique$disp.name=with(outlier.stat.unique,
                                       ifelse(is.na(type),"",
                                          paste0(as.character(space),"(",type,")")))
    shape.df = merge(x=shape.df,y=outlier.stat.unique,
#                      y=outlier.stat.unique[which(outlier.stat.unique$city==city.guide$city[city.index]),],
#                      y=data2[[city.index]],
                     by.x=city.guide$spatial.attr[city.index],by.y="space",all.x=T)
   
    shape.df = merge(x=shape.df,y=centroids.df,by="id",all.x=T)
    shape.df = shape.df[order(shape.df$order),] 

    theme_new<-theme_set(theme_bw(base_size = 10))
    theme_new<-theme_update(legend.position="none")
    map1<-map.plot(mapdf=shape.df,more.aes=aes(fill=type),color="grey",size=0.3)+
        ggtitle(paste0(city.guide$city[city.index],": Outliers"))+
#         ggtitle(bquote(atop(.(city.guide$city[city.index])~": Outliers" )))+
        labs(x="Longitude",y="Latitude")+
        scale_fill_manual(values=c("#666666","#999999","#dddddd"),name="Outliers")+
        geom_text(data=shape.df,size=3,
                  aes(x=lon.cent,y=lat.cent,label=disp.name))
#         theme(axis.text = element_text(size=8),legend.position="none",
# #               legend.title=element_text(size=8),legend.text=element_text(size=8),
#               axis.title = element_text(size=10),plot.title = element_text(size=10),
#           plot.margin=unit(c(.05,.05,.05,.05),"npc"))

})
tiff("plots/fig10.outliers.map.tiff",width=10*ppi,height=4*ppi,res=ppi)
grid.arrange(gg.maps[[1]],gg.maps[[2]],gg.maps[[3]],
             ncol=3,nrow=1,widths=c(1,0.85,1.15))
dev.off()
```


CA analysis
```{r}
ca.time = lapply(1:3, function(city.index){
    data.ca = checkin.poly.list[[city.index]][,c("cate_l1","hour")]
    data.ca$hour=factor(data.ca$hour)
    data.ca$cate_l1=factor(data.ca$cate_l1)
    ca.model = ca(xtabs(~hour+cate_l1,data=data.ca))
    cm.sum= summary(ca.model)
    
    ca.hours = cm.sum$rows; colnames(ca.hours)[c(5,8)]=c("d1","d2")
    ca.hours$city = city.guide$city[city.index]
    ca.hours = cbind(ca.hours,ca.model$rowcoord)
    colnames(ca.hours)[12:20]=paste("dim",colnames(ca.hours)[12:20],sep="")
    data.point = entropies.etc.st.df[which(entropies.etc.st.df$city==city.guide$city),]
    stat = ddply(data.point,.(hour),function(hour){
        data.frame("pi.max"=with(hour,mean(pi.max)),
                   "pi.min"=with(hour,mean(pi.min)))

    })
    stat$delta.pi = with(stat,pi.max-pi.min)
    ca.hours = cbind(ca.hours,stat)
    
    ca.cates = cm.sum$columns; colnames(ca.cates)[c(5,8)]=c("d1","d2")
    ca.cates$city = city.guide$city[city.index]
    scree = as.data.frame(cm.sum$scree); colnames(scree)=c("dim","value","perc","cum.perc")
    
    m1 <- lm(data=ca.hours[,c(22,12:20)],formula=pi.max~.)
    m2 <- lm(data=ca.hours[,c(23,12:20)],formula=pi.min~.)
    
#     plot(ca.model, mass = TRUE, contrib = "absolute", map ="rowgreen", 
#      arrows = c(TRUE, FALSE)) # asymmetric map
    

    gg.ca <-ggplot()+
        geom_point(data=ca.hours,aes(x=d1/1000,y=d2/1000),shape=21,fill=NA)+
        geom_text(data=ca.hours,aes(x=d1/1000,y=d2/1000,label=name),color="blue",alpha=0.3)+
#         geom_point(data=ca.cates,aes(x=d1/1000,y=d2/1000),shape=2)+
        geom_text(data=ca.cates,aes(x=d1/1000,y=d2/1000,label=name),color="red")+
#         geom_segment(data=ca.hours,x=0,y=0,aes(xend=d1/1000,yend=d2/1000),
#                      color="grey",arrow=arrow(length=unit(0.3,"cm")))+
        labs(x=paste0("Dimension 1 (",formatC(scree$perc[1],digits=4),"%)"),
             y=paste0("Dimension 2 (",formatC(scree$perc[2],digits=4),"%)"))
        
    

    list(gg.ca,m1,m2)
})

ca.space = lapply(1:3, function(city.index){
    data.ca = checkin.poly.list[[city.index]][,c("cate_l1",city.guide$spatial.attr[city.index])]
    colnames(data.ca)[2]="space"
    data.ca$space=factor(data.ca$space)
    data.ca$cate_l1=factor(data.ca$cate_l1)
    ca.model = ca(xtabs(~space+cate_l1,data=data.ca))
    cm.sum= summary(ca.model)
    
    ca.spaces = cm.sum$rows; colnames(ca.spaces)[c(5,8)]=c("d1","d2")
    ca.spaces$city = city.guide$city[city.index]
    ca.spaces = cbind(ca.spaces,ca.model$rowcoord)
    colnames(ca.spaces)[12:20]=paste("dim",colnames(ca.spaces)[12:20],sep="")
    data.point = entropies.etc.st.df[which(entropies.etc.st.df$city==city.guide$city[city.index]),]
    stat = ddply(data.point,.(space),function(space){
        data.frame("pi.max"=with(space,mean(pi.max)),
                   "pi.min"=with(space,mean(pi.min)),
                   "record"=sum(space$record))

    })
    stat$delta.pi = with(stat,pi.max-pi.min)
    ca.spaces = cbind(ca.spaces,stat)
    
    ca.cates = cm.sum$columns; colnames(ca.cates)[c(5,8)]=c("d1","d2")
    ca.cates$city = city.guide$city[city.index]
    scree = as.data.frame(cm.sum$scree); colnames(scree)=c("dim","value","perc","cum.perc")
    
    
#     plot(ca.model, mass = TRUE, contrib = "absolute", map ="rowgreen", 
#      arrows = c(TRUE, FALSE)) # asymmetric map
    lmdata = as.data.frame(xtabs(data=data.ca,~space+cate_l1))
    lmdata$prob = with(lmdata,Freq/sum(Freq))
    lmdata = dcast(lmdata,space~cate_l1,value.var="prob")
    ca.spaces = cbind(ca.spaces[1:25],lmdata)
    colnames(ca.spaces)[27:36]=paste0("cate",c(1:10))
    m1 <- lm(data=ca.spaces[,c(22,27:36)],pi.max~.)
    m2 <- lm(data=ca.spaces[,c(23,27:36)],pi.min~.)
#     m3 <- lm(data=ca.spaces[,c(25,12:20)],delta.pi~.)
#     ca.spaces$resid = abs(resid(m2))

    gg.ca <-ggplot()+
        geom_point(data=ca.spaces,aes(x=d1/1000,y=d2/1000),shape=21,fill=NA)+
        geom_text(data=ca.spaces,aes(x=d1/1000,y=d2/1000,label=name),color="blue",alpha=0.2)+
#         geom_segment(data=ca.spaces,x=0,y=0,aes(xend=d1/1000,yend=d2/1000),
#                      color="grey",arrow=arrow(length=unit(0.3,"cm")))+
#         geom_point(data=ca.cates,aes(x=d1/1000,y=d2/1000),shape=2)+
        geom_text(data=ca.cates,aes(x=d1/1000,y=d2/1000,label=name),color="red")+
        labs(x=paste0("Dimension 1 (",formatC(scree$perc[1],digits=4),"%)"),
             y=paste0("Dimension 2 (",formatC(scree$perc[2],digits=4),"%)"))

    list(gg.ca,m1,m2)
})


gg.resid = lapply(1:3, function(city.index){
    data = checkin.poly.list[[city.index]][,c("cate_l1",spatial.attr[city.index])]
    colnames(data)[2]<-"zip"
    data$zip=factor(data$zip)
    data$cate_l1=factor(data$cate_l1)
    ca.model = ca(xtabs(~zip+cate_l1,data=data))
    
#     plot(ca.model, mass = TRUE, contrib = "absolute", map ="rowgreen", 
#      arrows = c(TRUE, FALSE)) # asymmetric map
    lm.model = lm(data2[[city.index]],formula=pi.min~log(record) )
    
    plot.df1 = data.frame(ca.model$rowcoord[,1:2],"name"=ca.model$rownames,
                         "resid"=resid(lm.model),"city"=city.names[city.index])
    plot.df1=plot.df1[order(abs(plot.df1$resid)),]
    plot.df1$resid.positive = ifelse(plot.df1$resid>0,">0","<=0")
    
    plot.df2 = data.frame(ca.model$colcoord[,1:2],"name"=ca.model$colnames,
                                            "city"=city.names[city.index])
    
    ggplot(plot.df1,aes(x=X1,y=X2))+
        geom_text(data=plot.df2,aes(label=name),size=2.5ï¼Œalpha=0.8)+
        geom_segment(data=plot.df2,x=0,y=0,aes(xend=X1,yend=X2),
                     color="grey",arrow=arrow(length=unit(0.5,"cm")))+
        geom_point(aes(size=abs(resid),shape=resid.positive),fill="white",alpha=0.7)+
        ggtitle(city.names[city.index])+
        labs(x=paste0("Dimension 1 (",formatC(summary(ca.model)$scree[1,3],digits=4),"%)"),
             y=paste0("Dimension 2 (",formatC(summary(ca.model)$scree[2,3],digits=4),"%)"))+
        scale_size_continuous(name="Residual\nValue")+
        scale_shape_manual(name="Residual\nSign",values=c(21,24))+
        theme(axis.text = element_text(size=6),legend.key.width=unit(1,"line"),
              legend.title=element_text(size=8),legend.text=element_text(size=7),
              axis.title = element_text(size=8),plot.title = element_text(size=8),
          plot.margin=unit(c(.05,.05,.05,.05),"npc"))+
        guides(shape=guide_legend(order=1),size=guide_legend(order=2))
})

tiff("plots/pi.min.spatial.tiff",width=12*ppi,height=12*ppi,res=ppi)
grid.arrange(arrangeGrob(
                 arrangeGrob(gg.maps[[1]][[1]],gg.maps[[2]][[1]],gg.maps[[3]][[1]],
                             main=textGrob(expression(paste("Spatial Distribution of ",Pi)),
                                           vjust=1.5,gp=gpar(fontsize=10)),
                             sub=textGrob("(a)",gp=gpar(fontsize=10),vjust=-1),
                             ncol=3,widths=c(1,0.9,1.12)),
                 arrangeGrob(gg.maps[[1]][[2]],gg.maps[[2]][[2]],gg.maps[[3]][[2]],
                             main=textGrob("Spatial Distribution of Check-ins",
                                           vjust=1.5,gp=gpar(fontsize=10)),
                             sub=textGrob("(b)",gp=gpar(fontsize=10),vjust=-1),
                             ncol=3,widths=c(1,0.9,1.12)),
#                  arrangeGrob(gg.maps[[1]][[3]],gg.maps[[2]][[3]],gg.maps[[3]][[3]],
#                              main=textGrob("Spatial Distribution of Redidual",
#                                            vjust=1.5,gp=gpar(fontsize=10)),
#                              sub=textGrob("(g)",gp=gpar(fontsize=10),vjust=-1),
#                              ncol=3,widths=c(1,0.9,1.12)),
             ncol=1),ggd,
             arrangeGrob(gg.resid[[1]],gg.resid[[2]],gg.resid[[3]],
                             main=textGrob("Relations of Spatial Semantics and Rediduals",
                                           vjust=1.5,gp=gpar(fontsize=10)),
                             sub=textGrob("(d)",gp=gpar(fontsize=10),vjust=-1),
                             ncol=3),
#              ggc,ggf,
            nrow=3,heights=c(2,0.8,1))
# rm(gga,ggb,data)
dev.off()



```

