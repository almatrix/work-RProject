ans = ifelse(entropy<=max(y),x[which( y>=entropy & yoffset<entropy )],
x[which(y==max(y))] )
} else{
ans = sapply(entropy,function(i){
ifelse(i<=max(y),x[which( y>=i & yoffset<i )],
x[which(y==max(y))] )
})
}
ans
}
prior.freq$p.adjusted = with(prior.freq,get.p.adjusted(p.joint.x, N, p.joint.y))
View(prior.freq)
entropy = with(venue.freq, sum(-1 * p.joint * log2(p.joint) + 1/2/N))
pi.max = get.pi.max.from.entropy(entropy, L)
pi.min = max(prior.freq$p.adjusted)
data.frame("Condition"="Uncorrelated","Entropy"=entropy,
"Pi.max"=pi.max,"Pi.min"=pi.min)
pi.max = get.pi.max.from.entropy(entropy, (L+1))
data.frame("Condition"="Uncorrelated","Entropy"=entropy,
"Pi.max"=pi.max,"Pi.min"=pi.min)
condition=c("hour","ZIP")
freq.joint = get.freq.dataframe(data=data,"venue_id",cond.col=condition,
p.cond=T,p.joint=F)
View(freq.joint)
N = sum(freq.joint$Freq)
L = length(unique(data$venue_id))
prior.freq=prior.st
rm(condition)
cond.col=c("hour","ZIP")
freq.joint = get.freq.dataframe(data=data,"venue_id",cond.col,
p.cond=T,p.joint=F)
cond.col=NA
length(cond.col)
get.freq.dataframe = function(data,obs.col,cond.col=NA,p.joint=T,p.cond=F,p.marg=F){
if(length(cond.col)==1){
if(is.na(cond.col)){
freq = as.data.frame(xtabs(data=data,paste("~",obs.col),drop.unused.levels=T))
}else{
data$new.col = paste(data[,obs.col],data[,cond.col],sep="@")
freq = as.data.frame(xtabs(data=data,~new.col,drop.unused.levels=T))
col.info = data.frame(do.call(rbind,strsplit(as.character(freq$new.col),
"@",fixed=TRUE)))
colnames(col.info)=c(obs.col,"condition")
freq = cbind(col.info,freq)
colnames(freq)=c(obs.col,"condition","obs","Freq")
marg.freq = ddply(freq,.(condition),function(in.condition){
sum(in.condition$Freq)
})
colnames(marg.freq)[2]="marg.freq"
freq = merge(x=freq,y=marg.freq,all.x=T)
}}else{
data$condition = do.call(paste,lapply(cond.col,function(col){
data[,col]
}))
data$new.col = paste(data[,obs.col],data$condition,sep="@")
freq = as.data.frame(xtabs(data=data,~new.col,drop.unused.levels=T))
col.info = data.frame(do.call(rbind,strsplit(as.character(freq$new.col),
"@",fixed=TRUE)))
freq = cbind(col.info,freq)
colnames(freq)=c(obs.col,"condition","obs","Freq")
marg.freq = ddply(freq,.(condition),function(in.condition){
sum(in.condition$Freq)
})
colnames(marg.freq)[2]="marg.freq"
freq = merge(x=freq,y=marg.freq,all.x=T)
}
#     freq = as.data.frame(xtabs(data=data,...))
#     if(drop.unused){
#         freq = subset(freq, Freq>0)
#     }
if(!is.na(cond.col)){
}
if(p.joint){
freq$p.joint = with(freq, Freq / sum(Freq))
}
if(p.cond){
if(is.na(cond.col)){stop("you must specify the condition for conditional probability.")}
freq$p.cond = with(freq, Freq / marg.freq)
}
if(p.marg){
if(is.na(cond.col)){stop("you must specify the condition for marginal probability.")}
freq$p.marg = with(freq, marg.freq / sum(Freq))
}
freq
}
get.freq.dataframe = function(data,obs.col,cond.col=NA,p.joint=T,p.cond=F,p.marg=F){
if(length(cond.col)==1){
if(is.na(cond.col)){
freq = as.data.frame(xtabs(data=data,paste("~",obs.col),drop.unused.levels=T))
}else{
data$new.col = paste(data[,obs.col],data[,cond.col],sep="@")
freq = as.data.frame(xtabs(data=data,~new.col,drop.unused.levels=T))
col.info = data.frame(do.call(rbind,strsplit(as.character(freq$new.col),
"@",fixed=TRUE)))
colnames(col.info)=c(obs.col,"condition")
freq = cbind(col.info,freq)
colnames(freq)=c(obs.col,"condition","obs","Freq")
marg.freq = ddply(freq,.(condition),function(in.condition){
sum(in.condition$Freq)
})
colnames(marg.freq)[2]="marg.freq"
freq = merge(x=freq,y=marg.freq,all.x=T)
}}else{
data$condition = do.call(paste,lapply(cond.col,function(col){
data[,col]
}))
data$new.col = paste(data[,obs.col],data$condition,sep="@")
freq = as.data.frame(xtabs(data=data,~new.col,drop.unused.levels=T))
col.info = data.frame(do.call(rbind,strsplit(as.character(freq$new.col),
"@",fixed=TRUE)))
freq = cbind(col.info,freq)
colnames(freq)=c(obs.col,"condition","obs","Freq")
marg.freq = ddply(freq,.(condition),function(in.condition){
sum(in.condition$Freq)
})
colnames(marg.freq)[2]="marg.freq"
freq = merge(x=freq,y=marg.freq,all.x=T)
}
#     freq = as.data.frame(xtabs(data=data,...))
#     if(drop.unused){
#         freq = subset(freq, Freq>0)
#     }
if(p.joint){
freq$p.joint = with(freq, Freq / sum(Freq))
}
if(p.cond){
if(is.na(cond.col)){stop("you must specify the condition for conditional probability.")}
freq$p.cond = with(freq, Freq / marg.freq)
}
if(p.marg){
if(is.na(cond.col)){stop("you must specify the condition for marginal probability.")}
freq$p.marg = with(freq, marg.freq / sum(Freq))
}
freq
}
freq.joint = get.freq.dataframe(data=data,"venue_id",cond.col,
p.cond=T,p.joint=F)
freq.joint = get.freq.dataframe(data=data,"venue_id",cond.col=c("hour","ZIP"),
p.cond=T,p.joint=F)
get.freq.dataframe = function(data,obs.col,cond.col=NA,p.joint=T,p.cond=F,p.marg=F){
if(length(cond.col)==1){
if(is.na(cond.col)){
freq = as.data.frame(xtabs(data=data,paste("~",obs.col),drop.unused.levels=T))
}else{
data$new.col = paste(data[,obs.col],data[,cond.col],sep="@")
freq = as.data.frame(xtabs(data=data,~new.col,drop.unused.levels=T))
col.info = data.frame(do.call(rbind,strsplit(as.character(freq$new.col),
"@",fixed=TRUE)))
colnames(col.info)=c(obs.col,"condition")
freq = cbind(col.info,freq)
colnames(freq)=c(obs.col,"condition","obs","Freq")
marg.freq = ddply(freq,.(condition),function(in.condition){
sum(in.condition$Freq)
})
colnames(marg.freq)[2]="marg.freq"
freq = merge(x=freq,y=marg.freq,all.x=T)
}}else{
data$condition = do.call(paste,lapply(cond.col,function(col){
data[,col]
}))
data$new.col = paste(data[,obs.col],data$condition,sep="@")
freq = as.data.frame(xtabs(data=data,~new.col,drop.unused.levels=T))
col.info = data.frame(do.call(rbind,strsplit(as.character(freq$new.col),
"@",fixed=TRUE)))
freq = cbind(col.info,freq)
colnames(freq)=c(obs.col,"condition","obs","Freq")
marg.freq = ddply(freq,.(condition),function(in.condition){
sum(in.condition$Freq)
})
colnames(marg.freq)[2]="marg.freq"
freq = merge(x=freq,y=marg.freq,all.x=T)
}
#     freq = as.data.frame(xtabs(data=data,...))
#     if(drop.unused){
#         freq = subset(freq, Freq>0)
#     }
if(p.joint){
freq$p.joint = with(freq, Freq / sum(Freq))
}
if(p.cond){
if(length(cond.col)==1 && is.na(cond.col)){
stop("you must specify the condition for conditional probability.")}
freq$p.cond = with(freq, Freq / marg.freq)
}
if(p.marg){
if(length(cond.col)==1 && is.na(cond.col)){
stop("you must specify the condition for marginal probability.")}
freq$p.marg = with(freq, marg.freq / sum(Freq))
}
freq
}
freq.joint = get.freq.dataframe(data=data,"venue_id",cond.col=c("hour","ZIP"),
p.cond=T,p.joint=F)
View(freq.joint)
cond.col=c("hour","ZUP")
cond.col=c("hour","ZIP")
N = sum(freq.joint$Freq)
L = length(unique(data$venue_id))
in.condition = ddply(freq.joint, .(condition), function(i){
n = sum(i$Freq)
l = length(unique(i$venue_id))
prior.freq = merge(x=i, y=prior.freq, by=c("obs"),all.x=T)
prior.freq$p.adjusted = with(prior.freq,
get.p.adjusted(p.cond.x, n, p.cond.y) )
#         # adjusted for small samples (Mueller, 1955)
H.v = with(i,sum( -1 * p.cond * log2(p.cond) + 1/2/n))
#         # adjusted for small samples ("entropy estimates of small datasets")
#         H.v2 = sum(apply(i,1,function(j){
#             ni = as.integer(j["Freq"]);n=as.integer(j["marg.freq"])
#             (ni+1)/(n+2) * sum(1/c((ni+2):(n+2)))
#         }))
#         H.v.init = with(i,sum( -1 * p.cond * log2(p.cond) ))
#         H.v = with(prior.freq, sum(-1 * p.adjusted * log2(p.adjusted)))
pi.max = get.pi.max.from.entropy(H.v, l+1) # one for "others"
#         pi.max.init = get.pi.max.from.entropy(H.v.init, l+1)
pi.min = max(prior.freq$p.adjusted)
#         pi.min.init = max(prior.freq$p.cond.x)
data.frame("condition.value"=as.character(i[1,condition]),
"entropy"=H.v,
"pi.min"=pi.min,
"pi.max"=pi.max,
"record"=prior.freq$marg.freq.x)
})
in.condition = ddply(freq.joint, .(condition), function(i){
n = sum(i$Freq)
l = length(unique(i$venue_id))
prior.freq = merge(x=i, y=prior.freq, by=c("obs"),all.x=T)
prior.freq$p.adjusted = with(prior.freq,
get.p.adjusted(p.cond.x, n, p.cond.y) )
#         # adjusted for small samples (Mueller, 1955)
H.v = with(i,sum( -1 * p.cond * log2(p.cond) + 1/2/n))
#         # adjusted for small samples ("entropy estimates of small datasets")
#         H.v2 = sum(apply(i,1,function(j){
#             ni = as.integer(j["Freq"]);n=as.integer(j["marg.freq"])
#             (ni+1)/(n+2) * sum(1/c((ni+2):(n+2)))
#         }))
#         H.v.init = with(i,sum( -1 * p.cond * log2(p.cond) ))
#         H.v = with(prior.freq, sum(-1 * p.adjusted * log2(p.adjusted)))
pi.max = get.pi.max.from.entropy(H.v, l+1) # one for "others"
#         pi.max.init = get.pi.max.from.entropy(H.v.init, l+1)
pi.min = max(prior.freq$p.adjusted)
#         pi.min.init = max(prior.freq$p.cond.x)
data.frame("condition.value"=as.character(i[1,"condition"]),
"entropy"=H.v,
"pi.min"=pi.min,
"pi.max"=pi.max,
"record"=prior.freq$marg.freq.x)
})
View(in.condition)
View(in.condition)
View(freq.joint)
freq.joint2 = get.freq.dataframe(data=data,"venue_id",cond.col="hour",
p.cond=T,p.joint=F)
View(freq.joint2)
# conditional entropy adjusted to small sample size
get.conditional.entropy.etc.v2 = function(data,cond.col,condition.name,prior.freq){
freq.joint = get.freq.dataframe(data=data,"venue_id",cond.col,
p.cond=T,p.joint=F)
N = sum(freq.joint$Freq)
L = length(unique(data$venue_id))
in.condition = ddply(freq.joint, .(condition), function(i){
n = sum(i$Freq)
l = length(unique(i$venue_id))
prior.freq = merge(x=i, y=prior.freq, by=c("obs"),all.x=T)
prior.freq$p.adjusted = with(prior.freq,
get.p.adjusted(p.cond.x, n, p.cond.y) )
#         # adjusted for small samples (Mueller, 1955)
H.v = with(i,sum( -1 * p.cond * log2(p.cond) + 1/2/n))
#         # adjusted for small samples ("entropy estimates of small datasets")
#         H.v2 = sum(apply(i,1,function(j){
#             ni = as.integer(j["Freq"]);n=as.integer(j["marg.freq"])
#             (ni+1)/(n+2) * sum(1/c((ni+2):(n+2)))
#         }))
#         H.v.init = with(i,sum( -1 * p.cond * log2(p.cond) ))
#         H.v = with(prior.freq, sum(-1 * p.adjusted * log2(p.adjusted)))
pi.max = get.pi.max.from.entropy(H.v, l+1) # one for "others"
#         pi.max.init = get.pi.max.from.entropy(H.v.init, l+1)
pi.min = max(prior.freq$p.adjusted)
#         pi.min.init = max(prior.freq$p.cond.x)
data.frame(#"condition.value"=as.character(i[1,"condition"]),
"entropy"=H.v,
"pi.min"=pi.min,
"pi.max"=pi.max,
"record"=prior.freq$marg.freq.x)
})
H = sum(with(in.condition, record / N * entropy))
pi.max = get.pi.max.from.entropy(H, N)
pi.min = with(in.condition, weighted.mean(pi.min, record))
list("mean"=data.frame("Condition"=condition.name,"Entropy"=H,
"Pi.max"=pi.max,"Pi.min"=pi.min),
"detail"=in.condition)
}
get.unc.entropy.etc.v2 = function(data,prior.freq){
# get the inital probability distribution
venue.freq = get.freq.dataframe(data=data,"venue_id")
N = sum(venue.freq$Freq)
L = length(unique(data$venue_id))
# get the adjusted probabilty distribution
prior.freq = merge(x=venue.freq, y=prior.freq, by=c("venue_id"),all.x=T)
prior.freq$p.adjusted = with(prior.freq,get.p.adjusted(p.joint.x, N, p.joint.y))
# calculate entropy based on probability
#     entropy.init = with(venue.freq, sum(-1 * p.joint * log2(p.joint) ))
#     entropy = with(prior.freq, sum(-1 * p.adjusted * log2(p.adjusted) ))
entropy = with(venue.freq, sum(-1 * p.joint * log2(p.joint) + 1/2/N))
# get pi.max based on entropy
#     pi.max.init = get.pi.max.from.entropy(entropy.init, L)
pi.max = get.pi.max.from.entropy(entropy, (L+1))
# get pi.min based on probability disitribution
pi.min = max(prior.freq$p.adjusted)
#     pi.min.init = max(venue.freq$p.joint)
#     u2 = 1 - sum(p^2)
data.frame("Condition"="Uncorrelated","Entropy"=entropy,
"Pi.max"=pi.max,"Pi.min"=pi.min)
}
get.rand.entropy.etc = function(data){
N = length(unique(data$venue_id))
entropy = log2(N)
pi.max = get.pi.max.from.entropy(entropy, N)
pi.min = 1 / N
#     u2 = 1 - 1 / N
data.frame("Condition"="Random","Entropy"=entropy,
"Pi.max"=pi.max,"Pi.min"=pi.min)
}
rm(data)
rm(freq.joint)
rm(freq.joint2)
rm(in.condition)
rm(venue.freq)
rm(L,N)
rm(cond.col)
rm(entropy)
rm(pi.max,pi.min)
lapply(user.list[1:2],function(i){
counter.print(2)
print("***********111***************")
rand = get.rand.entropy.etc(i)
unc = get.unc.entropy.etc.v2(i,prior.unc)
time = get.conditional.entropy.etc.v2(i,"hour","Hour",prior.time)
space = get.conditional.entropy.etc.v2(i,spatial.attr,"Space",prior.space)
print("***********222***************")
i$ST = paste(i$hour, i[,spatial.attr])
st = get.conditional.entropy.etc.v2(i,"ST","ST",prior.st)
print("***********333***************")
# some extra statistics
total.records = nrow(i)
unique.venues = length(unique(i$venue_id))
unique.users = length(unique(i$user_id))
df = rbind(rand,unc,time$mean,space$mean,st$mean)
df$Cluster.id = i[1,"cluster.id"]
df$Records = nrow(i)
df$Venues = length(unique(i$venue_id))
df$Users = length(unique(i$user_id))
df$City = city.guide$city[city.index]
#         time.detail = as.data.frame(time$detail)
#         time.detail$city = city.guide$city[city.index]
#         space.detail = as.data.frame(space$detail)
#         space.detail$city = city.guide$city[city.index]
st.detail = as.data.frame(st$detail)
#         st.detail=unique(st$detail[,-c(2,3,6)])
st.detail$city = city.guide$city[city.index]
list(df,st.detail)
})
counter.reset()
test=lapply(user.list[1:2],function(i){
counter.print(2)
print("***********111***************")
rand = get.rand.entropy.etc(i)
unc = get.unc.entropy.etc.v2(i,prior.unc)
time = get.conditional.entropy.etc.v2(i,"hour","Hour",prior.time)
space = get.conditional.entropy.etc.v2(i,spatial.attr,"Space",prior.space)
print("***********222***************")
i$ST = paste(i$hour, i[,spatial.attr])
st = get.conditional.entropy.etc.v2(i,"ST","ST",prior.st)
print("***********333***************")
# some extra statistics
total.records = nrow(i)
unique.venues = length(unique(i$venue_id))
unique.users = length(unique(i$user_id))
df = rbind(rand,unc,time$mean,space$mean,st$mean)
df$Cluster.id = i[1,"cluster.id"]
df$Records = nrow(i)
df$Venues = length(unique(i$venue_id))
df$Users = length(unique(i$user_id))
df$City = city.guide$city[city.index]
#         time.detail = as.data.frame(time$detail)
#         time.detail$city = city.guide$city[city.index]
#         space.detail = as.data.frame(space$detail)
#         space.detail$city = city.guide$city[city.index]
st.detail = as.data.frame(st$detail)
#         st.detail=unique(st$detail[,-c(2,3,6)])
st.detail$city = city.guide$city[city.index]
list(df,st.detail)
})
test[[1]]
test[[1]][[1]]
get.conditional.entropy.etc.v2(i,"hour","Hour",prior.time)
data=user.list[[1]]
get.conditional.entropy.etc.v2(data,"hour","Hour",prior.time)
cond.col="hour"
freq.joint = get.freq.dataframe(data=data,"venue_id",cond.col,
p.cond=T,p.joint=F)
N = sum(freq.joint$Freq)
L = length(unique(data$venue_id))
in.condition = ddply(freq.joint, .(condition), function(i){
n = sum(i$Freq)
l = length(unique(i$venue_id))
prior.freq = merge(x=i, y=prior.freq, by=c("obs"),all.x=T)
prior.freq$p.adjusted = with(prior.freq,
get.p.adjusted(p.cond.x, n, p.cond.y) )
#         # adjusted for small samples (Mueller, 1955)
H.v = with(i,sum( -1 * p.cond * log2(p.cond) + 1/2/n))
#         # adjusted for small samples ("entropy estimates of small datasets")
#         H.v2 = sum(apply(i,1,function(j){
#             ni = as.integer(j["Freq"]);n=as.integer(j["marg.freq"])
#             (ni+1)/(n+2) * sum(1/c((ni+2):(n+2)))
#         }))
#         H.v.init = with(i,sum( -1 * p.cond * log2(p.cond) ))
#         H.v = with(prior.freq, sum(-1 * p.adjusted * log2(p.adjusted)))
pi.max = get.pi.max.from.entropy(H.v, l+1) # one for "others"
#         pi.max.init = get.pi.max.from.entropy(H.v.init, l+1)
pi.min = max(prior.freq$p.adjusted)
#         pi.min.init = max(prior.freq$p.cond.x)
data.frame(#"condition.value"=as.character(i[1,"condition"]),
"entropy"=H.v,
"pi.min"=pi.min,
"pi.max"=pi.max,
"record"=prior.freq$marg.freq.x)
})
prior.freq=prior.time
in.condition = ddply(freq.joint, .(condition), function(i){
n = sum(i$Freq)
l = length(unique(i$venue_id))
prior.freq = merge(x=i, y=prior.freq, by=c("obs"),all.x=T)
prior.freq$p.adjusted = with(prior.freq,
get.p.adjusted(p.cond.x, n, p.cond.y) )
#         # adjusted for small samples (Mueller, 1955)
H.v = with(i,sum( -1 * p.cond * log2(p.cond) + 1/2/n))
#         # adjusted for small samples ("entropy estimates of small datasets")
#         H.v2 = sum(apply(i,1,function(j){
#             ni = as.integer(j["Freq"]);n=as.integer(j["marg.freq"])
#             (ni+1)/(n+2) * sum(1/c((ni+2):(n+2)))
#         }))
#         H.v.init = with(i,sum( -1 * p.cond * log2(p.cond) ))
#         H.v = with(prior.freq, sum(-1 * p.adjusted * log2(p.adjusted)))
pi.max = get.pi.max.from.entropy(H.v, l+1) # one for "others"
#         pi.max.init = get.pi.max.from.entropy(H.v.init, l+1)
pi.min = max(prior.freq$p.adjusted)
#         pi.min.init = max(prior.freq$p.cond.x)
data.frame(#"condition.value"=as.character(i[1,"condition"]),
"entropy"=H.v,
"pi.min"=pi.min,
"pi.max"=pi.max,
"record"=prior.freq$marg.freq.x)
})
View(in.condition)
H = sum(with(in.condition, record / N * entropy))
View(freq.joint)
in.condition = ddply(freq.joint, .(condition), function(i){
i
#         n = sum(i$Freq)
#         l = length(unique(i$venue_id))
#
#         prior.freq = merge(x=i, y=prior.freq, by=c("obs"),all.x=T)
#         prior.freq$p.adjusted = with(prior.freq,
#                                      get.p.adjusted(p.cond.x, n, p.cond.y) )
# #         # adjusted for small samples (Mueller, 1955)
#         H.v = with(i,sum( -1 * p.cond * log2(p.cond) + 1/2/n))
# #         # adjusted for small samples ("entropy estimates of small datasets")
# #         H.v2 = sum(apply(i,1,function(j){
# #             ni = as.integer(j["Freq"]);n=as.integer(j["marg.freq"])
# #             (ni+1)/(n+2) * sum(1/c((ni+2):(n+2)))
# #         }))
# #         H.v.init = with(i,sum( -1 * p.cond * log2(p.cond) ))
# #         H.v = with(prior.freq, sum(-1 * p.adjusted * log2(p.adjusted)))
#
#         pi.max = get.pi.max.from.entropy(H.v, l+1) # one for "others"
# #         pi.max.init = get.pi.max.from.entropy(H.v.init, l+1)
#
#         pi.min = max(prior.freq$p.adjusted)
# #         pi.min.init = max(prior.freq$p.cond.x)
#
#         data.frame(#"condition.value"=as.character(i[1,"condition"]),
#                    "entropy"=H.v,
#                    "pi.min"=pi.min,
#                    "pi.max"=pi.max,
#                    "record"=prior.freq$marg.freq.x)
})
View(in.condition)
in.condition = ddply(freq.joint, .(condition), function(i){
n = sum(i$Freq)
l = length(unique(i$venue_id))
prior.freq = merge(x=i, y=prior.freq, by=c("obs"),all.x=T)
prior.freq$p.adjusted = with(prior.freq,
get.p.adjusted(p.cond.x, n, p.cond.y) )
#         # adjusted for small samples (Mueller, 1955)
H.v = with(i,sum( -1 * p.cond * log2(p.cond) + 1/2/n))
#         # adjusted for small samples ("entropy estimates of small datasets")
#         H.v2 = sum(apply(i,1,function(j){
#             ni = as.integer(j["Freq"]);n=as.integer(j["marg.freq"])
#             (ni+1)/(n+2) * sum(1/c((ni+2):(n+2)))
#         }))
#         H.v.init = with(i,sum( -1 * p.cond * log2(p.cond) ))
#         H.v = with(prior.freq, sum(-1 * p.adjusted * log2(p.adjusted)))
pi.max = get.pi.max.from.entropy(H.v, l+1) # one for "others"
#         pi.max.init = get.pi.max.from.entropy(H.v.init, l+1)
pi.min = max(prior.freq$p.adjusted)
#         pi.min.init = max(prior.freq$p.cond.x)
data.frame(#"condition.value"=as.character(i[1,"condition"]),
"entropy"=H.v,
"pi.min"=pi.min,
"pi.max"=pi.max)
})
