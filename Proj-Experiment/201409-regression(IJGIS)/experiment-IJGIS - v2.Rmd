---
title: "Understanding Contextualized Mobility Patterns from Location-based Social Networks"
author: "Ming Li"
date: "Wednesday, September 10, 2014"
output: html_document
---

before trying infering mobility patterns from the dataset, the question must be answered: 

to what extend can mobility patterns be revealed from the dataset? 
is it predictable? how predictable is it?

```{r,echo=FALSE}
library(rgeos)
library(rgdal)
library(scales)
library(reshape2)
library(ggplot2)
library(gridExtra)
#library(TSA)
library(ca)

source("../../global/functions/prepare.checkin.R")
source("../../global/functions/basic.stats.plots.R")
source("../../global/functions/spatial.analysis.R")
source("../../global/functions/etc.R")

# ############
# global variable
ppi=300

crs.wgs84.str = "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
```


#### 1. some statistical results of the dataset

##### 1.1 Trajectory
```{r,echo=FALSE, fig.width=6, fig.height=6}
checkin.NY = prepare.checkin("../../global/data/csv-raw/NewYorkCity.csv",
                             is.raw=TRUE, weather.data=NA, 
                             convert.time=TRUE, add.history=FALSE)
checkin.LA = prepare.checkin("../../global/data/csv-raw/LosAngelesCity.csv",
                             is.raw=TRUE, weather.data=NA, 
                             convert.time=TRUE, add.history=FALSE)
checkin.CH = prepare.checkin("../../global/data/csv-raw/ChicagoCity.csv",
                             is.raw=TRUE, weather.data=NA, 
                             convert.time=TRUE, add.history=FALSE)

SPDF.NY = readOGR(dsn = "../../global/data/shapefiles", layer = "NYC_zipcode")
SPDF.LA = readOGR(dsn = "../../global/data/shapefiles", layer = "LA_Zipcodes")
SPDF.CH = readOGR(dsn = "../../global/data/shapefiles", layer = "Chicago-ZipCodes")

checkin.poly.NY = na.omit(point.in.poly(checkin.NY, SPDF.NY, copy.attr="POSTAL"))
checkin.poly.LA = na.omit(point.in.poly(checkin.LA, SPDF.LA, copy.attr="Zip_Num"))
checkin.poly.CH = na.omit(point.in.poly(checkin.CH, SPDF.CH, copy.attr="ZIP"))

tiff("plots/checkin_points_cities.png", width = 12*ppi, height = 4*ppi, 
     res=ppi,bg = "white")
grid.arrange(
    point.plot(checkin.NY,alpha=0.3,size=1,shape=21,fill="white",
               basemap = map.plot(mapdir="../../global/data/shapefiles",
                                  maplayer="NYC_borough_boundaries_WGS84",
                                  size=0.4,color="grey",fill=NA))+
        ggtitle("New York City")+
        xlab("Longitude")+ylab("Latitude")+
        annotate("text",x=-74.12,y=40.82,size=2.5,label="Area: 468.9 sq mi\nPopulation: 8,405,837(2013)\nCheck-ins: 579,786"),
    
    point.plot(checkin.LA,alpha=0.3,size=1,shape=21,fill="white",
               basemap = map.plot(mapdir="../../global/data/shapefiles", 
                                  maplayer="bounds_LA_City_WGS84",
                                  size=0.4,color="grey",fill=NA))+
        ggtitle("Los Angeles City")+
        xlab("Longitude")+ylab("Latitude")+
        annotate("text",x=-118.5,y=33.82,size=2.5,label="Area:503 sq mi\nPopulation:3,884,307(2013)\nCheck-ins:138,211"),
    
    point.plot(checkin.CH,alpha=0.3,size=1,shape=21,fill="white",
               basemap = map.plot(mapdir="../../global/data/shapefiles", 
                                  maplayer="bounds_ChicagoCity_WGS84",
                                  size=0.4,color="grey",fill=NA))+
        ggtitle("Chicago City")+
        xlab("Longitude")+ylab("Latitude")+
        annotate("text",x=-87.83,y=41.7,size=2.5,label="Area:234.0 sq mi\nPopulation:2,695,598(2010)\nCheck-ins:183,837"),
             nrow=1, ncol=3, widths=c(1.2,0.8,1))
dev.off()
```

##### 1.2 Data sparsity 

The check-in dataset, like other social media datasets, is sparse. Most of the records are created by only a small propotion of user. 

```{r}
user.freq = as.data.frame(table(checkin.poly.CH$user_id))
colnames(user.freq)=c("User","Freq")
count.freq = as.data.frame(table(user.freq$Freq))
colnames(count.freq)=c("Freq","Freq.Freq")
count.freq$Freq=as.integer(as.character(count.freq$Freq))
count.freq$Prob=count.freq$Freq.Freq/sum(count.freq$Freq.Freq)

model.personal.checkin = glm(data=count.freq,formula=Prob~log(Freq),
                             family=gaussian(link = 'log'))
a=model.personal.checkin$coefficients[1]
k=model.personal.checkin$coefficients[2]
model.summary1=as.character(as.expression(substitute(italic(y) == a %.% italic(x)^k,
                     list(a = format(model.personal.checkin$coefficients[1], digits = 3), 
                          k = format(model.personal.checkin$coefficients[2], digits = 3)))))
model.summary2="ANOVA: 
---------------------------------------------------------------------
                  Df    Deviance    Resid.Df    Resid.Dev    Pr(>Chi)
NULL                                          244      0.143432              
log(Freq)    1      0.14299           243      0.000438    < 2.2e-16 ***
---------------------------------------------------------------------
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1"
ggplot(count.freq,aes(x=Freq,y=Prob))+
    geom_point()+scale_x_log10()+scale_y_continuous(labels=percent)+
    geom_smooth(formula=y~x,method=glm,family=gaussian(link = 'log'))+
    xlab("Personal Number of Check-ins [x]")+
    ylab("Probability [y=Pr(x)]")+
    geom_text(aes(x=50,y=0.3,label=model.summary1),parse = TRUE,hjust=0)+
#     annotation_custom(grob=tableGrob(test),xmin=1,xmax=3,ymin=0.1,ymax=0.25)
    geom_text(aes(x=50,y=0.2,label=model.summary2),hjust=0)
    
```


##### 1.3 User similarity

1. each user is represented by a N-dimension vector describing the venues he/she has been to.
2. clustering the points in the N-dimension space

```{r}
user.list.all = split(checkin.poly.CH,checkin.poly.CH$user_id)

# user vector
counter.reset();  time0=Sys.time(); 
point.vectors = sapply(user.list.all,function(user){
    counter.print(500)
    table(user$cate_l2)
})
time.print(Sys.time(), time0, "creating user vectors takes")
```
```{r}
# now we should discuss how we decide the number of clusters.
# It should be a compromise between the between_variances and 
# the records in each cluster. We should keep as much variance 
# as possible, while make sure there are still enough records
# in each cluster for valid statistics.
# ! computational heavy !
nr.clusters = lapply(c(200,500,1000,2000,3000,4000,5000,6000),function(i){
   time0=Sys.time();
   clusters = kmeans(t(point.vectors),centers=i,iter.max = 1000)
   time.print(Sys.time(), time0, "creating user clusters takes")
        
   pct.ss = clusters$betweenss / clusters$totss
   user.sizes = sapply(user.list.all,function(user){
       nrow(user)
   })
   agg.sizes = as.data.frame(xtabs(data=data.frame("size"=user.sizes,
                                                   "cls"=clusters$cluster),
                                   size~cls))
        
   list(clusters,pct.ss,agg.sizes)
})
save(nr.clusters,file="data/nr.clusters.CH.Rda")
load("data/nr.clusters.CH.Rda")
```
```{r}
# xintercepts = c(500,280,140,55,33,20,13,10);idx=0;
stats=data.frame()
gg.usercls <- lapply(nr.clusters,function(i){
#     idx<<-idx+1
    
    gg<-ggplot(i[[3]])+
        geom_histogram(aes(x=Freq,y=..density..),binwidth=0.05,fill=NA,color="grey")+
        geom_density(aes(x=Freq),adjust=2)+
#         scale_x_log10(breaks=c(1,10,30,xintercepts[idx],100,1000),limits=c(2,3000))+
        scale_x_log10(breaks=c(10,30,100,1000),limits=c(2,3000))+
        scale_y_continuous(limits=c(0,1.8))+
        xlab("Number of Records in One Cluster")+
        ylab("Probability Density")+
        theme(axis.text.x=element_text(face=c("plain","bold","plain","plain")))+
#         geom_vline(xintercept=xintercepts[idx])+
        annotate("text",x=400,y=1.6,size=3.2,
                  label=paste("Between SS / Total SS:",format.percent(i[[2]]),
                              "\nClusters (> 30 records):",
                              format.percent(length(which(i[[3]]$Freq>=30))/nrow(i[[3]]))))+
        ggtitle(paste(nrow(i[[3]]),"Clusters"))

    stats <<- rbind(stats,data.frame("Clusters"=nrow(i[[3]]),
                        "SS preservation"=i[[2]],
                        "Over 30 records"=length(which(i[[3]]$Freq>=30))/nrow(i[[3]])))
    gg                     
})

gg.stat <- ggplot(melt(data=stats,id.vars="Clusters"),aes(x=Clusters,y=value))+
    geom_line(aes(group=variable))+geom_point(aes(shape=variable,group=variable))+
    theme_grey(base_size = 10)+
    ggtitle("Trend")+
    xlab("Number of Clusters")+
    ylab("Proportion")+
    scale_shape_discrete(name="Criteria")+
    scale_x_continuous(breaks=c(0,1000,2000,3000,4000,5000,6000))+
    scale_y_continuous(labels=percent)+
    theme(legend.justification=c(0,0),legend.position=c(0,0))

tiff("plots/user.clustering.tiff",width=12*ppi,height=6*ppi,res=ppi)
grid.arrange(gg.usercls[[1]],gg.usercls[[3]],gg.usercls[[4]],
             gg.usercls[[6]],gg.usercls[[8]],gg.stat,
             ncol=3,nrow=2)
dev.off()
                 
# user aggregation
counter.reset();
checkin.poly.CH$cluster.id = sapply(checkin.poly.CH$user_id,function(i){
    counter.print(4000)
    nr.clusters[[4]][[1]]$cluster[as.character(i)]
})
# re-split
user.list.ucls = split(checkin.poly.CH,checkin.poly.CH$cluster.id)

```


##### 1.4 relations between the number of unique venues in a cluster and the number of records in a cluster

Intuitively, if there are more unique venues, there tend to be more records in the same cluster. However, the relationship should not be linear, because the appearance of each unique venues will not be equal. Suppose we have $m$ venues in a cluster, each represented as $v_{i}, i \in \left \{ 1,2,...m \right \}$. The number of appearances of each venue is $x_{i}$. We have the total number of records in that cluster $R=\sum_{1}^{m}x_{i}$.

We find a power-law distribution in $x_{i}$, i.e.: $f(x_{i}) \sim x_{i}^k$
```{r}
# get venue list vi with appearance xi in each cluster
counter.reset()
xi_distribution = do.call(rbind, lapply(user.list.ucls,function(cluster){
    counter.print(20)
    # number of records:
    R = nrow(cluster)
    
    cluster$venue_id=factor(cluster$venue_id)
    vi_freq = as.data.frame(table(cluster$venue_id)) 
    colnames(vi_freq)=c("vi","xi") # note: sum(vi_freq$xi)==R
    # number of unique venues:
    m = nrow(vi_freq)
    
    xi_freq = as.data.frame(table(vi_freq$xi)) 
    colnames(xi_freq)=c("xi","Freq") # note: sum(xi_freq$Freq)==m
    xi_freq$xi = as.integer(as.character(xi_freq$xi))
    
    # power law distribution
    model=glm(data=xi_freq,formula=Freq~log(xi),family = gaussian(link='log'))
    
    goodness.null = 1-pchisq(model$null.deviance,model$df.null)
    goodness.power.law = 1-pchisq(model$deviance,model$df.residual)
    
    k = model$coefficients[2]
    a = model$coefficients[1]
    
    data.frame("a"=a,"k"=k,"R"=R,"m"=m,
               "goodness.power.law"=goodness.power.law,
               "power.law.sig"=goodness.power.law>0.05,
               "goodness.null"=goodness.null,
               "null.sig"=goodness.null>0.05)
}))

k.mean=mean(xi_distribution$k,na.rm=T);k.sd=sd(xi_distribution$k,na.rm=T)
ggplot(xi_distribution,aes(x=k))+
    geom_histogram(aes(y=..density..),fill=NA,color="grey",binwidth=0.05)+
    geom_density(adjust=2.5,linetype=1)+
    stat_function(fun=dnorm,args=list(mean=k.mean,sd=k.sd),linetype=2)+
    geom_segment(aes(x=-4,y=0.3,xend=-3.75,yend=0.27),
                 arrow=arrow(length=unit(0.5,"cm")))+
    geom_text(aes(x=-4.5,y=0.32),label=c("Reference:Normal distribution"))+
    geom_segment(aes(x=-1.9,y=0.4,xend=-2.2,yend=0.38),
                 arrow=arrow(length=unit(0.5,"cm")))+
    geom_text(aes(x=-1.5,y=0.42),label=c("Estimated distribution"))+
    ylab("Probability Density")
ggplot(xi_distribution,aes(x=m,y=R))+
    geom_point(alpha=0.5,shape=21,fill="white",size=1)+
    scale_y_log10()+
    geom_smooth(size=1,method=glm,family=gaussian(link='log'),formula=y~log(x))
model=glm(data=xi_distribution,formula=R~log(m),family = gaussian(link='log'))
```



#### 2. Entropy

##### 2.1 calculation of entropies

```{r,eval=FALSE}
##########################
# computing 
# random entropy
entropy.rand = sapply(user.list.ucls, function(i){
    N = length(unique(i$venue_id))
    log2(N) 
})

# uncorrelated entropy (heterogeneity pattern)
get.entropy.unc = function(data){
    venue.freq = as.data.frame(table(data$venue_id))
    p = venue.freq$Freq / sum(venue.freq$Freq)
    p = p[p>0]
    -1 * sum(p * log2(p))
}
entropy.unc = sapply(user.list.ucls, function(i){get.entropy.unc(i)})

# temprol entropy (temproal pattern)
get.conditional.entropy = function(data,condition){
    freq = as.data.frame(xtabs(data=data,
                               as.formula(paste("~",condition,"+venue_id"))))
    
    freq.marg = as.vector(table(data[,condition]))
    freq$Freq.marg = rep(freq.marg,length(unique(freq$venue_id)))
    freq = freq[freq$Freq>0,]
    
    p.joint = freq$Freq / sum(freq$Freq)

    sum(p.joint * log2(freq$Freq.marg / freq$Freq))
}

counter.reset()
conditional.entropies = sapply(user.list.ucls,function(i){
    counter.print(10)
    entropy.time = get.conditional.entropy(i,"hour")
    entropy.space = get.conditional.entropy(i,"ZIP")
    
    i$spacetime = paste(i$hour, i$ZIP)
    entropy.st = get.conditional.entropy(i,"spacetime")
    
    c(entropy.time,entropy.space,entropy.st)
})
conditional.entropies = as.data.frame(t(conditional.entropies))
colnames(conditional.entropies)=c("Time","Space","ST")


entropies = cbind("Random"=entropy.rand,"Uncorrelated"=entropy.unc,
                  conditional.entropies,
                  do.call(rbind,lapply(user.list.ucls,function(i){
                      data.frame("unique.venues"=length(unique(i$venue_id)),
                                 "records"=nrow(i))
                  })))

## plotting
tiff("plots/entropies.tiff",width=12*ppi,height=4*ppi,res=ppi)
data=melt(entropies,id.vars = c("records","unique.venues"))
grid.arrange(
    ggplot(data)+
        geom_histogram(aes(x=value,y=..density..),binwidth=0.1,fill=NA,
                       color="grey",position="identity")+
        geom_density(aes(x=value),adjust=2)+
        xlab("Entropy")+ylab("Probability Density")+
        facet_wrap(~variable,ncol=5),
#     ggplot(data,aes(x=value,y=records))+
#         geom_point(shape=21,fill="white",alpha=0.3,size=1)+
#         xlab("Entropy")+ylab("Number of Records")+
#         geom_smooth(method=lm)+
#         scale_y_log10(breaks=c(10,30,100,1000))+
#         theme(axis.text.y=element_text(face=c("plain","bold","plain","plain")))+
#         facet_wrap(~variable,ncol=5),
    ggplot(data,aes(x=value,y=unique.venues))+
        geom_point(shape=21,fill="white",alpha=0.3,size=1)+
        xlab("Entropy")+ylab("Unique Venues")+
        geom_smooth(method=lm)+
        scale_y_log10(breaks=c(10,100,1000))+
#         theme(axis.text.y=element_text(face=c("plain","bold","plain","plain")))+
        facet_wrap(~variable,ncol=5),
    nrow=2,heights=c(1,1.2))
dev.off()

tiff("plots/entropy.residual.tiff",width=12*ppi,height=6*ppi,res=ppi)
data=melt(entropies,id.vars = c("records","unique.venues"))
gg.residual<-lapply(split(data,data$variable),function(i){
    
    list(ggplot(i,aes(x=unique.venues,y=records))+
        geom_point(shape=21,fill="white",alpha=0.3,size=1)+
        geom_smooth(method=glm,formula=y~log(x))+
        xlab("Unique Venues")+ylab("Number of Records")+
        scale_y_log10()+
        theme_grey(base_size = 10)+
        ggtitle(i[1,"variable"]),
    ggplot(i,aes(x=unique.venues,y=value))+
        geom_point(shape=21,fill="white",alpha=0.3,size=1)+
        geom_smooth(method=glm,formula=y~log(x))+
        xlab("Unique Venues")+ylab("Entropy")+
        theme_grey(base_size = 10)+
        ggtitle(i[1,"variable"]),
    ggplot(i,aes(x=resid(glm(formula=records~log(unique.venues),
                          family=gaussian(link="log"))),
                 y=resid( glm(formula=value~log(unique.venues)))))+
        geom_point(shape=21,fill="white",alpha=0.3,size=1)+
#         geom_smooth(method=glm,formula=y~log(x))+
        xlab("Redisual: Records ~ Venues")+
        ylab("Redisual: Entropy ~ Venues")+
        theme_grey(base_size = 10)+
        scale_x_log10()+
        ggtitle(i[1,"variable"]))
})
grid.arrange(gg.residual[[1]][[1]],gg.residual[[2]][[1]],gg.residual[[3]][[1]],
             gg.residual[[4]][[1]],gg.residual[[5]][[1]],
             gg.residual[[1]][[2]],gg.residual[[2]][[2]],gg.residual[[3]][[2]],
             gg.residual[[4]][[2]],gg.residual[[5]][[2]],
             gg.residual[[1]][[3]],gg.residual[[2]][[3]],gg.residual[[3]][[3]],
             gg.residual[[4]][[3]],gg.residual[[5]][[3]],
             ncol=5,nrow=3)

dev.off()

####################
reduction.list = list(

ggplot(entropies,aes(x=Random,y=Uncorrelated))+
    geom_point(shape=21,fill="white",alpha=0.3,size=1)+
    geom_line(aes(x=c(1:8),y=c(1:8)),size=2,alpha=0.5,linetype="dashed")+
    geom_smooth(method=lm),
ggplot(entropies,aes(x=Uncorrelated,y=Time))+
    geom_point(shape=21,fill="white",alpha=0.3,size=1)+
    geom_line(aes(x=c(1:8),y=c(1:8)),size=2,alpha=0.5,linetype="dashed")+
    geom_smooth(method=lm),
ggplot(entropies,aes(x=Uncorrelated,y=Space))+
    geom_point(shape=21,fill="white",alpha=0.3,size=1)+
    geom_line(aes(x=c(1:8),y=c(1:8)),size=2,alpha=0.5,linetype="dashed")+
    geom_smooth(method=lm),
ggplot(entropies,aes(x=Uncorrelated,y=ST))+
    geom_point(shape=21,fill="white",alpha=0.3,size=1)+
    geom_line(aes(x=c(1:8),y=c(1:8)),size=2,alpha=0.5,linetype="dashed")+
    geom_smooth(method=lm),

ggplot(entropies,aes(x=Random,y=(Random-Uncorrelated)/Random))+
    geom_point(shape=21,fill="white",alpha=0.3,size=1)+
    geom_smooth(method=lm)+scale_y_continuous(labels=percent)+
    ylab("Reduction (Uncorrelated)"),
ggplot(entropies,aes(x=Uncorrelated,y=(Uncorrelated-Time)/Uncorrelated))+
    geom_point(shape=21,fill="white",alpha=0.3,size=1)+
    geom_smooth(method=lm)+scale_y_continuous(labels=percent)+
    ylab("Reduction (Time)"),
ggplot(entropies,aes(x=Uncorrelated,y=(Uncorrelated-Space)/Uncorrelated))+
    geom_point(shape=21,fill="white",alpha=0.3,size=1)+
    geom_smooth(method=lm)+scale_y_continuous(labels=percent)+
    ylab("Reduction (Space)"),
ggplot(entropies,aes(x=Uncorrelated,y=(Uncorrelated-ST)/Uncorrelated))+
    geom_point(shape=21,fill="white",alpha=0.3,size=1)+
    geom_smooth(method=lm)+scale_y_continuous(labels=percent)+
    ylab("Reduction (ST)")
)

tiff("plots/entropies.reduction.tiff",width=12*ppi,height=4*ppi,res=ppi)
grid.arrange(reduction.list[[1]],reduction.list[[2]],
             reduction.list[[3]],reduction.list[[4]],
             reduction.list[[5]],reduction.list[[6]],
             reduction.list[[7]],reduction.list[[8]],
             ncol=4,nrow=2)
dev.off()

```





#### 4. Regularity as lower bounds

##### 4.1 calculation of regularity

```{r,eval=FALSE}
# reg.s.t = lapply(usersets.active, function(user){
#     
#     cls.s = spatial.clustering(user)
#     cls.t = temporal.clustering(user)
#     cls = merge(x=cls.s, y=cls.t[c("id","hour.cls")], by.x="id", by.y="id", all.X=TRUE)
#     cls = merge(x=cls, y = user[c("gid","yearday")], by.x="id", by.y="gid", all.X=TRUE)
#     
#     cls$cate_l2=factor(cls$cate_l2)
#     cls$st = as.factor(paste(cls$hour,cls$sp))
#     
#    
# #     the most probable category for this user considering both spatial and temporal
#     pi.st = sapply(split(cls,cls$st),function(st){
#         cate_seq = st$cate_l2
#         # the most probable category
#         cate_freq = as.data.frame(table(cate_seq))
# #         if(length(unique(st$yearday))>1) 
#             ans = max(cate_freq$Freq)/sum(cate_freq$Freq)
# #         else ans = NA
#         ans
#     })
#     
# 
#     
#     freq.st = as.data.frame(table(cls$st))
#     p.st = freq.st$Freq/sum(freq.st$Freq)
#     
#     sum(p.st * pi.st,na.rm=TRUE)
# 
# #     c("temp"=pi.t,"sp"=pi.s)
# })

reg.t = sapply(usersets.active, function(user){

    
    user$cate_l2=factor(user$cate_l2)
    user$hour = factor(user$hour)
    
   
#     the most probable category for this user in that hour
    pi.t = sapply(split(user,user$hour),function(hour){
        cate_seq = hour$cate_l2
        # the most probable category
        cate_freq = as.data.frame(table(cate_seq))
#         if(length(unique(st$yearday))>1) 
            ans = max(cate_freq$Freq)/sum(cate_freq$Freq)
#         else ans = NA
        ans
    })
    

    
    freq.t = as.data.frame(table(user$hour))
    p.t = freq.t$Freq/sum(freq.t$Freq)
    
    sum(p.t * pi.t,na.rm=TRUE)

})

```
```{r,echo=FALSE,fig.width=4,fig.height=3}
# save(reg.t, file="D:\\Experiments\\R\\data\\reg.t.Rda")
load("D:\\Experiments\\R\\data\\reg.t.Rda")

gg.reg = ggplot(data.frame("regularity"=reg.t)) + 
    stat_density(aes(x = regularity, y = ..density..),
                 position="identity",adjust=2,alpha=0.5,fill="#56B4E9") +
    xlab(expression(atop(italic(Pi)^"max","\n\n(a)") ) )  +
    ylab("Density") +
#     theme(axis.title = element_text(size=10),legend.title=element_blank(),
#           legend.position="none",legend.background=element_blank())
    theme(axis.title = element_text(size=10),legend.title=element_blank(),
          legend.position="none",plot.title = element_text(size=11))+
    scale_x_continuous(limit=c(0,1))

gg.reg

manipulate(
    gg.reg  + geom_vline(xintercept =x.max,color = "#E69F00", linetype="dotted", size=2),
    x.max= slider(0.2,0.6)
    )


```



##### 4.2 relations with other factors 
```{r}
usersets.active = usersets[usersets.active.idx]
# relation between "regularity" and "hour"
reg.t.mat = sapply(usersets.active, function(user){
    
    user$cate_l2=factor(user$cate_l2)
   
#     the most probable category for this user in that hour
    pi.t = sapply(split(user,user$hour),function(hour){
        cate_seq = hour$cate_l2
        # the most probable category
        cate_freq = as.data.frame(table(cate_seq))
#         if(length(unique(st$yearday))>1) 
            ans = max(cate_freq$Freq)/sum(cate_freq$Freq)
#         else ans = NA
        ans
    })
    
    pi.t[is.na(pi.t)]=0
    
    pi.t

})
```
```{r,echo=FALSE,fig.width=4,fig.height=3}
gg.reg.hour <- ggplot(data.frame("hour"=c(0:23),"reg.mean"=colMeans(data.frame(t(reg.t.mat))),
       "se"=apply(data.frame(t(reg.t.mat)),2,sd)/sqrt(4422) ),
       aes(x=hour,y=reg.mean) )+
    geom_smooth(se=F, method = "lm", formula = y ~ poly(x, 12),color="#56B4E9",size=2) +
    geom_point(size=1.5) + 
    geom_errorbar(aes(ymin=reg.mean-2*se, ymax=reg.mean+2*se), colour ="black", size =.3, width=.5)+
    xlab("Hour of Day\n\n(b)") +
    ylab(expression(paste(Pi^min, "(", mu %+-% 2*sigma,")"))) +
#     theme(axis.title = element_text(size=10),legend.title=element_blank(),
#           legend.position="none",legend.background=element_blank())
    theme(axis.title = element_text(size=10),plot.title = element_text(size=11))
gg.reg.hour
```
```{r}
# relations between hour and N
n.t.mat = sapply(usersets.active, function(user){

    user$cate_l2=factor(user$cate_l2)
   
#   n in each hour for this user
    n.t = sapply(split(user,user$hour),function(hour){
        length(unique(hour$cate_l2))
    })
    
    n.t[is.na(n.t)]=0
    
    n.t

})
```
```{r,echo=FALSE,fig.height=3,fig.width=4}
gg.n.hour <- ggplot(data.frame("hour"=c(0:23),"n.mean"=colMeans(data.frame(t(n.t.mat))),
       "se"=apply(data.frame(t(n.t.mat)),2,sd)/sqrt(4422) ),
       aes(x=hour,y=n.mean) )+
    geom_smooth(se=F, method = "lm", formula = y ~ poly(x, 12),color="#56B4E9",size=2) +
    geom_point(size=1.5) + 
    geom_errorbar(aes(ymin=n.mean-2*se, ymax=n.mean+2*se), colour ="black", size =.3, width=.5)+
    xlab("Hour of Day\n\n(d)") +
    ylab(expression(paste("Hourly Number of Unique Categories(", mu %+-% 2*sigma,")"))) +
#     theme(axis.title = element_text(size=10),legend.title=element_blank(),
#           legend.position="none",legend.background=element_blank())
    theme(axis.title = element_text(size=10),plot.title = element_text(size=11))
gg.n.hour

# relations between regulatiry and N
df.reg.n = data.frame("x"=usersets.stats.act$cates,"y"=reg.t)
gg.reg.n <- ggplot(df.reg.n)+
    geom_point(aes(x=x,y=y),alpha=.3,size=1)+
    geom_smooth(aes(x=x,y=y),method="lm",formula=y~log10(x),size=2) +
    theme(axis.title = element_text(size=10),legend.title=element_blank(),
          legend.background=element_blank(),plot.title = element_text(size=11))+
#     ggtitle("Regularity V.S. Unique categories (N)")+
    xlab("Number of Unique Categories\n\n(c)") +
    ylab(expression(paste(Pi^min))) +
    scale_y_continuous(limits=c(0,1))+
#     scale_x_log10()+
    geom_text(aes(x = 75, y = 0.9, label = lm_eqn_log(df.reg.n)), size=3,parse = TRUE)
gg.reg.n

png(paste0(basedir,"regularity.png"), width = 8*ppi, height = 6*ppi, res=ppi)
multiplot(gg.reg,gg.reg.n,gg.reg.hour,gg.n.hour,cols=2)
dev.off()



# manipulate(
#     gg.reg  + geom_vline(xintercept =x.max,color = "#E69F00", linetype="dotted", size=2),
#     x.max= slider(0,1)
#     )
```
#### 5. upper bounds based on Fano's inequality

##### 5.1 Fano's inequality

Fano's inquality states:

$$H(X|Y)\leq H(e)+p(e)\log(N-1)$$

where

$$p(e)=p(X\neq Y)=1-\Pi$$

and 

$$H(e)=-p(e) \log p(e)-(1-p(e)) \log (1-p(e))$$

It is tranlated in our case to:

$$S^{real}\leq -\Pi \log \Pi - (1-\Pi)\log(1-\Pi)+(1-\Pi)\log(N-1)$$

The mathmatical relations is described in the following figure:

```{r,echo=FALSE,fig.width=5,fig.height=3}
x = seq(from=0,to=1,length=101)
N = c(1,2,5,20,100,500)
df = data.frame()
temp=lapply(N,function(n){
    y = rep(0,101) 
    if(n!=1){
        y[1]=log2(n-1)
        y[2:100] = (1-x[2:100])*log2(n-1) - x[2:100]*log2(x[2:100]) - (1-x[2:100])*log2(1-x[2:100]) 
    }
    newdf = data.frame("pi"=x, "S"=y, "N"=as.factor(n))
    df <<- rbind(df, newdf)
    NA
})
# png(paste0(basedir,"img\\ceus_relation_pi.s.N.png"), width = 5*ppi, height = 3*ppi, res=ppi,bg = "transparent")
gg.pi.s.n <- ggplot(df) + 
    geom_path(aes(x=pi,y=S,group=N,color=N)) +
    theme(legend.background=element_blank(),legend.title=element_blank(),
          axis.title = element_text(size=10))+
    scale_color_discrete( breaks = levels(df$N),
         labels=list(bquote(italic(N)==.(N[1])),bquote(italic(N)==.(N[2])),
                     bquote(italic(N)==.(N[3])),bquote(italic(N)==.(N[4])),
                     bquote(italic(N)==.(N[5])),bquote(italic(N)==.(N[6])))  )+
    geom_point(aes(x=0.5,y=5.48145),fill="#E69F00",color=NA,alpha=.5,size=4,shape=21)+
    annotate("text", label = "(italic(Pi)[0]~~italic(S)[0])",
             parse = TRUE,size=2, x = 0.62, y = 5.5, colour = "black") +
    annotate("text", label = ",",size=2, x = 0.62, y = 5.5, colour = "black") +
    xlab(bquote(atop(italic(Pi),"\n\n(a)") ) ) +
    ylab(bquote(italic(S)^max ) ) 
#     ggtitle(expression(paste("S"^"max", " ~ (", Pi, ", N)")))
# dev.off()
```


##### 5.2 calculation of upper bounds 

```{r,echo=FALSE}
counter.reset()
piset = do.call(rbind, lapply(seq_along(user.list.ucls),function(id){
    counter.print(10)
    N = length(unique(user.list.ucls[[id]]$venue_id))
 
    x = seq(from=0,to=1,length=1001)
    y = rep(0,1001)

    if(N!=1){
        y[1]=log2(N-1)
        y[2:1000] = (1-x[2:1000])*log2(N-1) - x[2:1000]*log2(x[2:1000]) - (1-x[2:1000])*log2(1-x[2:1000]) 
    }

    yoffset = y[2:length(y)]
    yoffset[length(y)]=-0.001
    
    pi.rand = ifelse(entropies[id,"Random"]<=max(y),
                     x[which( y>=entropies[id,"Random"] & yoffset<entropies[id,"Random"] )],
                     x[which(y==max(y))] )
    pi.unc = ifelse(entropies[id,"Uncorrelated"]<=max(y),
                     x[which( y>=entropies[id,"Uncorrelated"] & yoffset<entropies[id,"Uncorrelated"] )],
                     x[which(y==max(y))] )
    pi.time = ifelse(entropies[id,"Time"]<=max(y),
                     x[which( y>=entropies[id,"Time"] & yoffset<entropies[id,"Time"] )],
                     x[which(y==max(y))] )
    pi.space = ifelse(entropies[id,"Space"]<=max(y),
                     x[which( y>=entropies[id,"Space"] & yoffset<entropies[id,"Space"] )],
                     x[which(y==max(y))] )
    pi.st = ifelse(entropies[id,"ST"]<=max(y),
                     x[which( y>=entropies[id,"ST"] & yoffset<entropies[id,"ST"] )],
                     x[which(y==max(y))] )
    
    data.frame("id"=id,"Random"=pi.rand,"Uncorrelated"=pi.unc,"Time"=pi.time,
      "Space"=pi.space,"ST"=pi.st,"n"=N)
}))
piset$records=entropies$records
tiff("plots/pi.max.tiff",width=12*ppi,height=4*ppi,res=ppi)
data=melt(piset,id.vars =c("id","n","records"))
grid.arrange(
    ggplot(data)+
        geom_histogram(aes(x=value,y=..density..),binwidth=0.01,fill=NA,
                       color="grey",position="identity")+
            geom_density(aes(x=value),adjust=2)+
        xlab(expression(atop(italic(Pi)^"max") ) ) +
        ylab("Probability Density")+
        facet_wrap(~variable,ncol=5),
#     ggplot(data,aes(x=value,y=records))+
#         geom_point(shape=21,fill="white",alpha=0.3,size=1)+
#         xlab(expression(atop(italic(Pi)^"max") ) ) +
#         ylab("Number of Records")+
#         geom_smooth(method=lm,formula=y~I(log(x)))+
#         scale_y_log10(breaks=c(10,30,100,1000),limit=c(1,3000))+
#         theme(axis.text.y=element_text(face=c("plain","bold","plain","plain")))+
#         facet_wrap(~variable,ncol=5),
    ggplot(data,aes(x=value,y=n))+
        geom_point(shape=21,fill="white",alpha=0.3,size=1)+
        xlab(expression(atop(italic(Pi)^"max") ) ) +
        ylab("Unique Venues")+
        geom_smooth(method=lm,formula=y~log(x))+
        scale_y_log10(breaks=c(10,100,1000),limit=c(1,3000))+
#         theme(axis.text.y=element_text(face=c("plain","bold","plain","plain")))+
        facet_wrap(~variable,ncol=5),
    nrow=2,heights=c(1,1.2))
dev.off()
```


##### 5.3 relations with other factors

```{r}
gg.pi.n = ggplot(piset,aes(x=n,y=real))+
    geom_point(alpha=.3,size=1)+
    stat_quantile(aes(colour = ..quantile..), quantiles = seq(0, 1, by=0.25),size = 1.2,linetype="dashed") +
    scale_colour_gradient2(name="Quantile",midpoint = 0.5) + 
#     geom_smooth(aes(x=n,y=real),method="lm",formula=y~x,size=2) +
    theme(axis.title = element_text(size=10),
#           legend.title=element_blank(),legend.background=element_blank(),
          plot.title = element_text(size=11))+
#     ggtitle("Regularity V.S. Unique categories (N)")+
    xlab("Number of Unique Categories\n\n(a)") +
    ylab(expression(Pi^"max"))  +
    scale_y_continuous(limits=c(0.6,1)) 
#     scale_x_log10()+
#     geom_text(aes(x = 75, y = 0.65, label = lm_eqn(piset,piset$n,piset$real)), size=3,parse = TRUE)



mutual.imp.df = data.frame(
    "s.unc"= entropy.unc.act,
    "s.real"= entropy.real.act,
    "mutual"= entropy.unc.act-entropy.real.act,
    "pimin" = reg.t,
    "pimax" = piset$real,
    "imp1" = piset$real-reg.t,
    "imp2" = piset$real / reg.t,
    "n" = piset$n,
    "hc" = usersets.stats.act$hourcomp)


gg.pi.reg <- ggplot(mutual.imp.df,aes(x=pimin,y=pimax,color=log10(n)) )+
    geom_point(size=1) +
    geom_smooth( method = "lm",color="#56B4E9",formula = y~x,size=2) +
    scale_colour_gradient2(name="N",midpoint = 1.3, low="red",high="blue",
                           breaks=c(0,1,2),
                           labels=c(expression(10^0), expression(10^1),
                                    expression(10^2))) +   
#     geom_errorbar(aes(ymin=mu-2*se, ymax=mu+2*se), colour ="black", size =.3, width=.5)+
    xlab(expression(atop(italic(Pi)^"min","\n\n(b)") ) ) +
    ylab(expression(Pi^max)) +
#     geom_text(aes(x = 0.625, y = 0.7, 
#                   label = lm_eqn(mutual.imp.df,mutual.imp.df$pimin,mutual.imp.df$pimax)), 
#               size=3,parse = TRUE,color="black")+
#     theme(axis.title = element_text(size=10),legend.title=element_blank(),
#           legend.position="none",legend.background=element_blank())
    theme(axis.title = element_text(size=10),
          plot.title = element_text(size=11))

```



```{r}


# gg.pimin.s <- 



gg.mutual.pimin <- 
    ggplot(mutual.imp.df,aes(x=mutual,y=pimin,color=log10(n)) )+
    geom_point(size=1) +
    scale_colour_gradient2(name="N",midpoint = 1.3, low="red",high="blue",
                           breaks=c(0,1,2),
                           labels=c(expression(10^0), expression(10^1),
                                    expression(10^2))) +
#     geom_smooth( method = "lm",color="#56B4E9",formula = y~log(x),size=2) +
    xlab("Mutual Information\n\n(c)" )  +
    ylab(expression(italic(Pi)^"min" ) ) +
#     geom_text(aes(x = 0.625, y = 0.75, 
#                   label = lm_eqn_log(data.frame("x"=mutual.imp.df$mutual,"y"=mutual.imp.df$pimin))), 
#               size=3,parse = TRUE)+
    theme(axis.title = element_text(size=10),plot.title = element_text(size=11))


gg.mutual.pimax <- 
    ggplot(mutual.imp.df,aes(x=mutual,y=pimax,color=log10(n)) )+
    geom_point(size=1) +
    scale_colour_gradient2(name="N",midpoint = 1.3, low="red",high="blue",
                           breaks=c(0,1,2),
                           labels=c(expression(10^0), expression(10^1),
                                    expression(10^2))) +
#     geom_smooth( method = "lm",color="#56B4E9",formula = y~x,size=2) +
    xlab("Mutual Information\n\n(d)" )  +
    ylab(expression(italic(Pi)^"max" ) ) +
#     geom_text(aes(x = 0.625, y = 0.75, 
#                   label = lm_eqn_log(data.frame("x"=mutual.imp.df$mutual,"y"=mutual.imp.df$pimax))), 
#               size=3,parse = TRUE)+
    theme(axis.title = element_text(size=10),plot.title = element_text(size=11))


gg.mutual.imp1 <- 
    ggplot(mutual.imp.df,aes(x=mutual,y=imp1,color=log10(n)) )+
    geom_point(size=1) +
    scale_colour_gradient2(name="N",midpoint = 1.3, low="red",high="blue",
                           breaks=c(0,1,2),
                           labels=c(expression(10^0), expression(10^1),
                                    expression(10^2))) +
    geom_smooth( method = "lm",color="#56B4E9",formula = y~x,size=2) +
    xlab("Mutual Information\n\n(c)" )  +
    ylab(expression(Delta~Pi == italic(Pi)^"max" - italic(Pi)^"min") ) +
    geom_text(aes(x = 2, y = 0.75, 
                  label = lm_eqn(data.frame("x"=mutual.imp.df$mutual,"y"=mutual.imp.df$imp1))),
              size=3,parse = TRUE,color="black")+
    theme(axis.title = element_text(size=10),plot.title = element_text(size=11))

gg.mutual.imp2 <- 
    ggplot(mutual.imp.df,aes(x=mutual,y=imp1,color=log10(hc)) )+
    geom_point(size=1,alpha=0.8) +
    scale_colour_gradient2(name="H.C",midpoint = -0.2, low="red",high="blue",
                           breaks=c(-0.25,-0.15,-0.05,0),
                           labels=c(0.56,0.71,0.89,1) ) +
    geom_smooth( method = "lm",color="#56B4E9",formula = y~x,size=2) +
    xlab("Mutual Information\n\n(d)" )  +
    ylab(expression(Delta~Pi == italic(Pi)^"max" - italic(Pi)^"min") ) +
#     geom_text(aes(x = 2, y = 0.75, 
#                   label = lm_eqn(data.frame("x"=mutual.imp.df$mutual,"y"=mutual.imp.df$imp1))),
#               size=3,parse = TRUE,color="black")+
    theme(axis.title = element_text(size=10),plot.title = element_text(size=11))

png(paste0(basedir,"relations.pi.mutual.png"), width = 8*ppi, height = 6*ppi, res=ppi)
multiplot(gg.pi.n,gg.mutual.imp1,
          gg.pi.reg,gg.mutual.imp2,cols=2)
dev.off()
```

