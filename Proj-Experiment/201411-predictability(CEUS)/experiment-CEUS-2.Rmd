---
title: "[experiments-ceus]Multinomial logistic regression model for modeling user interest"
date: "Thursday, May 21, 2015"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE,eval=FALSE,message=FALSE,warning=FALSE)
```

This document is in line with the first revision for CEUS paper. The paper is about using multinomial logistic regression models to model/predict user interest (venue categoty) in LBSN, more specifically, Foursquare. 

The experimantal datasets are crawled from three cities in USA, namely Chicago, Los Angeles and New York City. The reason for choosing these three study sites is because users in these cities have high level of activity in both Foursquare and Twitter. (Foursquare only makes the check-ins that are published via Twitter public.)


```{r libraries.loading,eval=TRUE} 
# load the libraries,functions, and define the global variables
library(rgdal)
library(scales)
library(ggplot2)
library(reshape2)
library(gridExtra)
library(manipulate)
library(plyr)
library(vcd) # for chi-square and cramer's V
# source("D:\\GitRepos\\work\\fun\\multiplot.R")

source("../../global/functions/prepare.checkin.R")
source("../../global/functions/basic.stats.plots.R")
source("../../global/functions/spatial.analysis.R")
source("../../global/functions/etc.R")
source("../../global/functions/truncated.power.law.R")
source("../../global/functions/geom_textbox.R")

# ############
# global variable
crs.wgs84.str = "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
city.guide=data.frame("city"=c("Chicago","Los Angeles","New York City"),
                      "coord.x"=c(-87.92,-118.64,-74.23),
                      "coord.y"=c(41.7,33.82,40.85),
                      "spatial.attr" = c("ZIP","Zip_Num","POSTAL"),
                      "shapefile.boundary" = c("bounds_ChicagoCity_WGS84",
                                               "bounds_LA_City_WGS84","NYC_borough_boundaries_WGS84"),
                      "shapefile.zip" = c("Chicago-ZipCodes","LA_Zipcodes","NYC_zipcode"),
                      "csvfile"=c("ChicagoCity","LosAngelesCity","NewYorkCity"),
                      stringsAsFactors=FALSE)

```

#### 1. load the data

First, we load the data. The first several rows for each city is shown in the following to demonstrate the data structure. 

```{r data.loading,eval=TRUE,cache=TRUE,results='hide'}
checkin.poly.list = lapply(1:3, function(city.index){
    checkin = prepare.checkin(paste0("../../global/data/csv-raw/",
                                     city.guide$csvfile[city.index],".csv"),
                             is.raw=TRUE, weather.data=NA, 
                             convert.time=TRUE, add.history=TRUE)
    SPDF = readOGR(dsn = "../../global/data/shapefiles", 
                   layer = city.guide$shapefile.zip[city.index])
    
#     na.omit(
point.in.poly(checkin, SPDF, copy.attr=city.guide$spatial.attr[city.index])#)
})
```
```{r data.summary,eval=TRUE}
# summary
print(city.guide$city[1])
head(checkin.poly.list[[1]][,c("gid","user_id","venue_id","lat","lon","localtime","cate_l1","cate_l2",city.guide$spatial.attr[1],"last.cate_l1","last.cate_l2")])
print(city.guide$city[2])
head(checkin.poly.list[[2]][,c("gid","user_id","venue_id","lat","lon","localtime","cate_l1","cate_l2",city.guide$spatial.attr[2],"last.cate_l1","last.cate_l2")])
print(city.guide$city[3])
head(checkin.poly.list[[3]][,c("gid","user_id","venue_id","lat","lon","localtime","cate_l1","cate_l2",city.guide$spatial.attr[3],"last.cate_l1","last.cate_l2")])
```

#### 2. Demonstration of contextual influences 

This sections aims to show the influneces of differnt context on user interest visually. We include three types of context:

+ temporal: the time of check-in. It will be numerically represented by the hour of check-in.
+ spatial: the location of check-in. It will be numerically represented by the zip code of the check-in.
+ sequential: the sequence of check-in. It will be numerically represented by the category of the last check-in venue. 

Further, the user interest will be numerically represented by the category of check-in venue. The exploratory experiment will be carried out with the dataset from Chicago.

```{r exp.data,echo=TRUE,eval=TRUE}
exp.data = checkin.poly.list[[1]][,c("user_id","cate_l1","cate_l2","venue_id",
                                     "hour", city.guide$spatial.attr[1],
                                     "last.user_id","last.venue_id",
                                     "last.cate_l1","last.cate_l2",
                                     "time.interval","timestamps")]
```



##### 2.1. temporal influences

The following figure shows the fact that users have differnt interest towards geographic venues at differnt hour of day. The data comes from Chicago.

```{r temporal.fig,echo=TRUE,eval=TRUE,fig.width=10,fig.height=6}
# frequency statistics
cate.on.hour = xtabs2(data = exp.data, obs.col = "cate_l1", 
                          cond.col = "hour",p.joint = T,p.cond=T)

theme_new<-theme_set(theme_bw(base_size = 10))
gg.cate.on.hour <- ggplot(cate.on.hour, 
                          aes(x=abbreviate(cate_l1,2),y=p.cond,fill=cate_l1)) + 
    geom_bar(stat="identity") +
    labs(x="User interest",y="Conditional probability")+
    facet_wrap(~condition,ncol=6,nrow=4)+
    scale_y_sqrt()+
    coord_polar()

gg.cate.on.hour
```

This fact can also be demonstrated from chi-square test:
```{r temporal.chisq,echo=TRUE,eval=TRUE}
# frequency dataframe to contingency table
cate.on.hour.table = xtabs(Freq ~ condition+cate_l1, data=cate.on.hour)
chisq.test(cate.on.hour.table,simulate.p.value = T)
```

##### 2.2. spatial influences

Similary, we reveal the spatial influences on user interest both with graphics and chi-square test.

```{r spatial,echo=TRUE,eval=TRUE,fig.width=10,fig.height=4.5}
# frequency statistics
cate.on.zip = xtabs2(data = exp.data, obs.col = "cate_l1", 
                      cond.col = city.guide$spatial.attr[1],
                      p.joint = T,p.cond=T)


# for visualization purpose, we select only 18 most popular zip code regions (condition)
cate.on.zip.plot = cate.on.zip[which(cate.on.zip$condition %in% 
    unique(cate.on.zip[order(cate.on.zip$marg.freq,decreasing=T),"condition"])[1:18]),] 
theme_new<-theme_set(theme_bw(base_size = 10))
gg.cate.on.zip <- ggplot(cate.on.zip.plot, 
                         aes(x=abbreviate(cate_l1,2),y=p.cond,fill=cate_l1)) + 
    geom_bar(stat="identity") +
    labs(x="User interest",y="Conditional probability")+
    facet_wrap(~condition,ncol=6)+
    scale_y_sqrt()+
    coord_polar()
gg.cate.on.zip

# frequency dataframe to contingency table
cate.on.zip.table = xtabs(Freq ~ condition+cate_l1, data=cate.on.zip)
chisq.test(cate.on.zip.table,simulate.p.value = T)
```

##### 2.3. sequential influences

The sequential influences are a little bit tricky to see, because the fuzzy dataset from Foursquare would result into some problematic scenarios that must be excluede/considered:

- when a user kept checking in at the same place;
- when two check-ins are taken place not really "consequtively" (long time interval);
- other scenarios??

```{r sequential,echo=TRUE,eval=TRUE,fig.width=10,fig.height=3}
# add weight based on time interval
exp.data$wgt = 2 ^ ( -1 * exp.data$time.interval + 1)

# remove continuous check-ins by the same user (by changing the weight to 0)
exp.data$wgt = with(exp.data,
                    ifelse(venue_id==last.venue_id, 0, wgt))

# deal with the empty values
exp.data$wgt = with(exp.data, 
        ifelse(is.na(last.cate_l1)|time.interval>12,
               mean(wgt,na.rm=T),wgt))
exp.data$last.cate_l1 = with(exp.data,
        as.factor(ifelse(is.na(last.cate_l1)|time.interval>12,
            "Unknown",as.character(last.cate_l1))))
exp.data$last.cate_l2 = with(exp.data,
        as.factor(ifelse(is.na(last.cate_l2)|time.interval>12,
           "Unknown",as.character(last.cate_l2))))
exp.data$last.venue_id = with(exp.data,
        as.factor(ifelse(is.na(last.venue_id)|time.interval>12,
           "Unknown",as.character(last.venue_id))))

# weighted statistics for frequency/probability
cate.on.last = xtabs2(data = exp.data, obs.col = "cate_l1", 
                      cond.col = "last.cate_l1", wgt.col= "wgt",
                      p.joint = T,p.cond=T,p.marg=T)

theme_new<-theme_set(theme_bw(base_size = 10))
gg.cate.on.zip <- ggplot(cate.on.last, 
                         aes(x=abbreviate(cate_l1,2),y=p.cond,fill=cate_l1)) + 
    geom_bar(stat="identity") +
    labs(x="User interest",y="Conditional probability")+
    facet_wrap(~condition,ncol=6)+
    coord_polar()
gg.cate.on.zip

# frequency dataframe to contingency table
cate.on.last.table = xtabs(Freq ~ condition+cate_l1, data=cate.on.last)
chisq.test(cate.on.last.table,simulate.p.value = T)
```

#### 3. Dealing with high-cardinality categorical attributes

One of the shortcomings of regression (both linear and logistic) is that it doesn’t handle categorical variables with a very large number of possible values (for example, postal codes). We can get around this, of course, by going to another modeling technique, such as Naive Bayes; however, we lose some of the advantages of regression — namely, the model’s explicit estimates of variables’ explanatory value, and explicit insight into and control of variable to variable dependence.

[D.Micci-Barreca,2001](http://dl.acm.org/citation.cfm?id=507538) provides an approach to deal with this problem. A similar approach is also mentioned [here](http://www.win-vector.com/blog/2012/07/modeling-trick-impact-coding-of-categorical-variables-with-many-levels/) The basic idea behind this is to transform categorical variables into continuous values with the *impact of each category value* on the outcome. 

Here we first use spatial context (zip code) as an example.

```{r zip.impact, echo=TRUE,eval=TRUE,cache=TRUE}

categorical.scaler.zip <- xtabs2(data = exp.data,
                                 obs.col = "cate_l2", cond.col = "ZIP",
                                 p.cond=T, p.prior=T)
categorical.scaler.zip$prob.est.zip = with(categorical.scaler.zip,
    marg.freq/(marg.freq+4)*p.cond + 4/(marg.freq+4)*p.prior)
# apply
checkin.continuous = merge(exp.data,
                           categorical.scaler.zip[,c(1,2,9)],
                           by.x=c("cate_l2","ZIP"),
                           by.y=c("cate_l2","condition"),
                           all.x=T)

# without impact model
t0=Sys.time()
glmodel1 <- glm(cate_l2 ~  ZIP , 
               data=checkin.continuous, family=binomial(link="logit"))
Sys.time()-t0
summary(glmodel1)
1 - glmodel1$deviance/glmodel1$null.deviance

# with impact moidel
t0=Sys.time()
glmodel2 <- glm(cate_l2 ~  prob.est.zip , 
               data=checkin.continuous, family=binomial(link="logit"))
Sys.time()-t0
summary(glmodel2)
1 - glmodel2$deviance/glmodel2$null.deviance

```

The above experiments show that transforming the categorical levels into continous "impact model" can:

+ making the computation more timely efficient (from 1.03 min to 8 sec);
+ equally (even slightly better) explain the data; (~25%)
+ provide much more significant (and meaningful) regression coefficicent;

We now perform similar operations on the temporal context:

```{r hour.impact,echo=TRUE,eval=TRUE,cache=TRUE}
# impact model for temporal context
categorical.scaler.hour <- xtabs2(data = exp.data,
                                 obs.col = "cate_l2", cond.col = "hour",
                                 p.cond=T, p.prior=T)
categorical.scaler.hour$prob.est.hour = with(categorical.scaler.hour,
    marg.freq/(marg.freq+4)*p.cond + 4/(marg.freq+4)*p.prior)
# apply
checkin.continuous = merge(checkin.continuous,
                           categorical.scaler.hour[,c(1,2,9)],
                           by.x=c("cate_l2","hour"),
                           by.y=c("cate_l2","condition"),
                           all.x=T)

# without impact model
t0=Sys.time()
glmodel3 <- glm(cate_l2 ~  hour , 
               data=checkin.continuous, family=binomial(link="logit"))
Sys.time()-t0
summary(glmodel3)
1 - glmodel3$deviance/glmodel3$null.deviance

# with impact model
t0=Sys.time()
glmodel4 <- glm(cate_l2 ~  prob.est.hour , 
               data=checkin.continuous, family=binomial(link="logit"))
Sys.time()-t0
summary(glmodel4)
1 - glmodel4$deviance/glmodel4$null.deviance

```

And the conclusions for spatial context hold true also for temporal one (more efficient, more significant). More interestingly, the model after transformation provides much better describtion of the data: from merely 8% to 38%. 

Then how about if we consider both?

```{r st.model,echo=TRUE,eval=TRUE}
# adding impact model for both space and time
glmodel5 <- glm(cate_l2 ~  prob.est.hour + prob.est.zip , 
               data=checkin.continuous, family=binomial(link="logit"))
summary(glmodel5)
1 - glmodel5$deviance/glmodel5$null.deviance
```
The model can describe 41.6% variances in the dependent variable (i.e. user interest).


#### 4. Personal model V.S. global model

Users typically have differnt personal interest. Personal model would thus be able to yield better predictions for the user's personal interest. However, the LBSN dataset is sparse. In other words, most users only have one or several records in the dataset. A model based on these limited number of records would be sensible to changes. Hence, a global model built from the entire dataset would provide more confident clues regarding user interest in such circumstances. The overall model, however, should consider both situations and combinds personal models and global models.

Our approach is to build models from other users (less weight) and the target user (more weight). Consequently, we would have personalized models. 

```{r}
learning.size = as.integer(nrow(exp.data)*19/20)
exp.data = exp.data[order(exp.data$timestamps),]
exp.data.learning = exp.data[c(1:learning.size),]
exp.data.predicting = exp.data[c((learning.size+1):nrow(exp.data)),]

ddply(exp.data.predicting, .(user_id),function(user){
    # predictions for each user
    uid = user[1,"user_id"]
    
    # do we have historical data for this user?
    # if yes, we make the first prediction based on personal & global data
    if(uid %in% exp.data.learning$user_id){
        
    }
    
    # make predictions just based on global model
})
```





