---
title: "[experiments-ceus]Multinomial logistic regression model for modeling user interest"
date: "Thursday, May 21, 2015"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE,eval=FALSE,message=FALSE,warning=FALSE)
```

This document is in line with the first revision for CEUS paper. The paper is about using multinomial logistic regression models to model/predict user interest (venue categoty) in LBSN, more specifically, Foursquare. 

The experimantal datasets are crawled from three cities in USA, namely Chicago, Los Angeles and New York City. The reason for choosing these three study sites is because users in these cities have high level of activity in both Foursquare and Twitter. (Foursquare only makes the check-ins that are published via Twitter public.)


```{r libraries.loading,eval=TRUE} 
# load the libraries,functions, and define the global variables
library(rgdal)
library(scales)
library(ggplot2)
library(reshape2)
library(gridExtra)
library(manipulate)
library(plyr)
library(vcd) # for chi-square and cramer's V
# source("D:\\GitRepos\\work\\fun\\multiplot.R")

source("../../global/functions/prepare.checkin.R")
source("../../global/functions/basic.stats.plots.R")
source("../../global/functions/spatial.analysis.R")
source("../../global/functions/etc.R")
source("../../global/functions/truncated.power.law.R")
source("../../global/functions/geom_textbox.R")

# ############
# global variable
crs.wgs84.str = "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
city.guide=data.frame("city"=c("Chicago","Los Angeles","New York City"),
                      "coord.x"=c(-87.92,-118.64,-74.23),
                      "coord.y"=c(41.7,33.82,40.85),
                      "spatial.attr" = c("ZIP","Zip_Num","POSTAL"),
                      "shapefile.boundary" = c("bounds_ChicagoCity_WGS84",
                                               "bounds_LA_City_WGS84","NYC_borough_boundaries_WGS84"),
                      "shapefile.zip" = c("Chicago-ZipCodes","LA_Zipcodes","NYC_zipcode"),
                      "csvfile"=c("ChicagoCity","LosAngelesCity","NewYorkCity"),
                      stringsAsFactors=FALSE)

```

#### load the data

First, we load the data. The first several rows for each city is shown in the following to demonstrate the data structure. 

```{r data.loading,eval=TRUE,cache=TRUE,results='hide'}
checkin.poly.list = lapply(1:3, function(city.index){
    checkin = prepare.checkin(paste0("../../global/data/csv-raw/",
                                     city.guide$csvfile[city.index],".csv"),
                             is.raw=TRUE, weather.data=NA, 
                             convert.time=TRUE, add.history=TRUE)
    SPDF = readOGR(dsn = "../../global/data/shapefiles", 
                   layer = city.guide$shapefile.zip[city.index])
    
#     na.omit(
point.in.poly(checkin, SPDF, copy.attr=city.guide$spatial.attr[city.index])#)
})
```
```{r data.summary,eval=TRUE}
# summary
print(city.guide$city[1])
head(checkin.poly.list[[1]][,c("gid","user_id","venue_id","lat","lon","localtime","cate_l1","cate_l2",city.guide$spatial.attr[1])])
print(city.guide$city[2])
head(checkin.poly.list[[2]][,c("gid","user_id","venue_id","lat","lon","localtime","cate_l1","cate_l2",city.guide$spatial.attr[2])])
print(city.guide$city[3])
head(checkin.poly.list[[3]][,c("gid","user_id","venue_id","lat","lon","localtime","cate_l1","cate_l2",city.guide$spatial.attr[3])])
```

#### Demonstration of contextual influences 

This sections aims to show the influneces of differnt context on user interest visually. We include three types of context:

+ temporal: the time of check-in. It will be numerically represented by the hour of check-in.
+ spatial: the location of check-in. It will be numerically represented by the zip code of the check-in.
+ sequential: the sequence of check-in. It will be numerically represented by the category of the last check-in venue. 

Furthre, the user interest will be numerically represented by the category of check-in venue.


##### 1. temporal influences

The following figure shows the fact that users have differnt interest towards geographic venues at differnt hour of day. The data comes from Chicago.

```{r temporal.fig,echo=TRUE,eval=TRUE,fig.width=10,fig.height=6}
# frequency statistics
cate.on.hour = xtabs2(data = checkin.poly.list[[1]], obs.col = "cate_l1", 
                          cond.col = "hour",p.joint = T,p.cond=T)

theme_new<-theme_set(theme_bw(base_size = 10))
gg.cate.on.hour <- ggplot(cate.on.hour, 
                          aes(x=abbreviate(cate_l1,2),y=p.cond,fill=cate_l1)) + 
    geom_bar(stat="identity") +
    labs(x="User interest",y="Conditional probability")+
    facet_wrap(~condition,ncol=6,nrow=4)+
    coord_polar()

gg.cate.on.hour
```

This fact can also be demonstrated from chi-square test:
```{r temporal.chisq,echo=TRUE,eval=TRUE}
# frequency dataframe to contingency table
cate.on.hour.table = xtabs(Freq ~ condition+cate_l1, data=cate.on.hour)
chisq.test(cate.on.hour.table,simulate.p.value = T)
```

##### 2. spatial influences

Similary, we reveal the spatial influences on user interest both with graphics and chi-square test.

```{r spatial,echo=TRUE,eval=TRUE,fig.width=10,fig.height=8}
# frequency statistics
cate.on.zip = xtabs2(data = checkin.poly.list[[1]], obs.col = "cate_l1", 
                      cond.col = city.guide$spatial.attr[1],
                      p.joint = T,p.cond=T)

theme_new<-theme_set(theme_bw(base_size = 10))
gg.cate.on.zip <- ggplot(cate.on.zip, 
                         aes(x=abbreviate(cate_l1,2),y=p.cond,fill=cate_l1)) + 
    geom_bar(stat="identity") +
    labs(x="User interest",y="Conditional probability")+
    facet_wrap(~condition,ncol=8)+
    coord_polar()
gg.cate.on.zip

# frequency dataframe to contingency table
cate.on.zip.table = xtabs(Freq ~ condition+cate_l1, data=cate.on.zip)
chisq.test(cate.on.zip.table,simulate.p.value = T)
```

##### 3. sequential influences

The sequential influences are a little bit tricky to see, because the fuzzy dataset from Foursquare would result into some problematic scenarios that must be excluede/considered:

- when a user kept checking in at the same place;
- when two check-ins are taken place not really "consequtively" (long time interval);
- other scenarios??

```{r sequential,echo=TRUE,eval=TRUE,fig.width=10}
# remove mismatch pairs
data.seq = na.omit(checkin.poly.list[[1]])
# remove continuous check-ins by the same user
data.seq = data.seq[data.seq$venue_id!=data.seq$last.venue_id, ]
# add weight based on time interval
data.seq$wgt = with(data.seq, 2^(-1*time.interval))
# weighted statistics for frequency/probability
cate.on.last = xtabs2(data = data.seq, obs.col = "cate_l1", 
                      cond.col = "last.cate_l1", wgt.col= "wgt",
                      p.joint = T,p.cond=T)

theme_new<-theme_set(theme_bw(base_size = 10))
gg.cate.on.last <- ggplot(cate.on.last, 
                         aes(x=abbreviate(cate_l1,2),y=p.cond,fill=cate_l1)) + 
    geom_bar(stat="identity") +
    labs(x="User interest",y="Conditional probability")+
    facet_wrap(~condition,ncol=5)+
    coord_polar()
gg.cate.on.last

# frequency dataframe to contingency table
cate.on.last.table = xtabs(Freq ~ condition+cate_l1, data=cate.on.last)
chisq.test(cate.on.last.table,simulate.p.value = T)
```

#### Dealing with high-cardinality categorical attributes

One of the shortcomings of regression (both linear and logistic) is that it doesn’t handle categorical variables with a very large number of possible values (for example, postal codes). We can get around this, of course, by going to another modeling technique, such as Naive Bayes; however, we lose some of the advantages of regression — namely, the model’s explicit estimates of variables’ explanatory value, and explicit insight into and control of variable to variable dependence.

[D.Micci-Barreca,2001](http://dl.acm.org/citation.cfm?id=507538) provides an approach to deal with this problem. A similar approach is also mentioned [here](http://www.win-vector.com/blog/2012/07/modeling-trick-impact-coding-of-categorical-variables-with-many-levels/) The basic idea behind this is to transform categorical variables into continuous values with the *impact of each category value* on the outcome. 

```{r}
# return a model of the conditional probability 
# of dependent variable (depvar) by level 
# assumes outcome is logical and not null
impactModel = function(xcol, depvar) {
  n = length(depvar)
  p = sum(depvar)/n
  # duplicate output for NA (average NA towards grand uniform average) 
  x = c(xcol,xcol)
  y = c(depvar, depvar)
  x[(1+n):(2*n)] = NA
  levelcounts = table(x, y, useNA="always")
  condprobmodel = (levelcounts[,2]+p)/(levelcounts[,1]+levelcounts[,2]+1.0) 
  # apply model example: applyImpactModel(condprobmodel,data[,varname])
  condprobmodel
}

# apply model to column to essentially return condprobmodel[rawx]
# both NA's and new levels are smoothed to original grand average 
applyImpactModel = function(condprobmodel, xcol) {
  naval = condprobmodel[is.na(names(condprobmodel))]
  dim = length(xcol)
  condprobvec = numeric(dim) + naval
  for(nm in names(condprobmodel)) {
    if(!is.na(nm)) {
      condprobvec[xcol==nm] = condprobmodel[nm]
    }
  }
  condprobvec
}

# convert Address variable to its impact model outcome
impact_zip_mod = impactModel(data.seq$POSTAL, data.seq$cate_l1)
data.seq$impactZip = applyImpactModel(impact_zip_mod, data.seq$POSTAL)
```

