---
title: "Demonstration of Experiments (paper IJGI)"
author: "Ming Li"
date: "Tuesday, August 19, 2014"
output: html_document
---


```{r,echo=FALSE,include=FALSE}
library(rgeos)
library(scales)
library(reshape2)
library(ggplot2)
library(gridExtra)
#library(TSA)
library(ca)

source("../../global/functions/prepare.checkin.R")
source("../../global/functions/basic.stats.plots.R")
source("../../global/functions/spatial.analysis.R")
source("../../global/functions/etc.R")

# global variable
ppi=300

crs.wgs84.str = "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"

```

First, we load the data into the workspace. 

```{r,echo=FALSE}
# checkin.NY = prepare.checkin("../../global/data/csv-raw/NewYorkCity.csv",
#                              is.raw=TRUE, 
#                              weather.data="../../global/data/csv-raw/Weather_NY.csv", 
#                              convert.time=TRUE, add.history=TRUE)
# save(checkin.NY,file="../../global/data/Rda-saved/checkin.global.weather.NY.Rda")
load(file="../../global/data/Rda-saved/checkin.global.weather.NY.Rda")
```

Following is a summary of the dataset.

```{r,echo=FALSE,include=FALSE}
summary(checkin.NY)
```


### 1. Exploratory analysis 

#### Temporal: Frequency domain

The analysis in the frequency domain reveals a strong harmonics with a period 
of 24 hours. Therefore, it is reasonable to 

```{r, fig.width=8, fig.height=6, dpi=300}
freq.plot(checkin.NY,title="New York City")
```


#### Temproal: Time domain 

The distribution of the check-in categories accross one period (24 hours).

```{r, fig.width=8, fig.height=6, dpi=300}
time.distribution.plot(checkin.NY,title="New York City")
```

#### Temporal: Radial plot

The distribution of the check-in categories in 24 hours in radial plots.

```{r, fig.width=10, fig.height=4, dpi=300}
time.radial.plot(checkin.NY)
```

### Spatial

have a look at a intuitive spatiotemporal visualizaiton of the data...

```{r,eval=FALSE}
basemap = map.plot("../../global/data/shapefiles", 
                   "NYC_borough_boundaries_WGS84",
                   alpha=0.1,size=0.3,color="grey")

saveGIF({
  point.animation.plot(checkin.NY,title="New York City", 
                       basemap=basemap,axis.size=18,
                       title.size=24,legend.size=18,plot.size=3)
}, interval = 0.5, movie.name = "spatiotemporal.gif", 
ani.width = 1500, ani.height = 1200)

```

<img src="spatiotemporal.gif" />

What will the statistics say when we neglect the temporal factors?
First of all, if we try to find spatial clusters 
based on differnt checkin categories, the distribution of the founded clusters
are quite differnt. It indicates that each category has differnt correlations
with the geographic space.

```{r}
if(!exists("basemap")){
    basemap <- map.plot("../../global/data/shapefiles", 
                   "NYC_borough_boundaries_WGS84",
                   alpha=0.1,size=0.3,color="grey")
}

plots <- lapply(split(checkin.NY,checkin.NY$cate_l1),function(ci){
    # find out clusters for the type of category
    clusters = spatial.clustering(ci)
    
    centers = clusters[["centers"]]
    points = clusters[["point.unique"]]
    wss = clusters[["wss"]]
    pct = clusters[["pct"]]
    lbl = paste(nrow(centers),"clusters;\nWSS:",
                  formatC(wss,digits=2,format="f"),
                  "(",format.percent(pct),")")
    
    # add basic points
    gg.map <- point.plot(points,x="lon.x",y="lat.x",alpha=0.05, basemap=basemap)
    
    # add cluster information
    gg.map <- point.plot(centers, x="lon.center",y="lat.center",
                         color= "cid.ordered", color.concrete=FALSE,
                         point.size = log(centers$size,5),alpha = 0.7,
                         basemap=gg.map,
                         xlim=range(checkin.NY$lon,finite=TRUE),
                         ylim=range(checkin.NY$lat,finite=TRUE))
    
    # some plot configuration
    gg.map <- gg.map + ggtitle(ci[1,"cate_l1"]) +
        theme(legend.position="none") + 
        geom_text(aes(x = -74.13, y = 40.87), label = lbl, size=2) 
    
})

# png("categorized_clustering.png",width=15*ppi,height=6*ppi,res=ppi)
grid.arrange(plots[[1]],plots[[2]],plots[[3]],plots[[4]],
             plots[[5]],plots[[6]],plots[[7]],plots[[8]], 
             plots[[9]],plots[[10]],nrow=2, ncol=5)
# dev.off()
```

We could also see the most domimant categories in each part of the entire city.

```{r}
# prepare polygon data
SPDF = readOGR(dsn = "../../global/data/shapefiles", layer = "NYC_zipcode")
# intersect the checkin data with the each postal code region
checkin.NY = point.in.poly(checkin.NY, SPDF, copy.attr="POSTAL")
# get the distribution of categories in each postal code region
cate.distr=cate.distr.in.poly(checkin.NY)
# and use that information to implement the SPDF
SPDF = poly.with.category(SPDF, cate.distr=cate.distr, poly.attr="POSTAL")

# plot

mapdf=df.from.spdf(SPDF)
# mapdf$density=apply(mapdf,1,function(i){i[i["cate.dom"]]})
# mapdf$density=as.numeric(formatC(mapdf$density,digits=1,format = "f"))
png("main_category.png",height=6*ppi, width=16*ppi,res=ppi)
grid.arrange(
map.plot(mapdf=mapdf,
         more.aes=aes_string(fill="Category.1st"),
         color="grey",size=0.3,alpha=0.9)+
    theme_bw()+
    xlab("")+ylab("")+
    theme(legend.title = element_text(size=8),
          legend.text = element_text(size = 8)),
map.plot(mapdf=mapdf,
         more.aes=aes_string(fill="Category.2nd"),
         color="grey",size=0.3,alpha=0.7)+
    theme_bw()+
    xlab("")+ylab("")+
    theme(legend.title = element_text(size=8),
          legend.text = element_text(size = 8)),
nrow=1, ncol=2, widths=c(1,1) )
dev.off()

```


### Meteorological: Correspondence analysis

```{r, fig.width=8, fig.height=6}
cate.conds = xtabs(~conds+cate_l1, data=checkin.NY)
#prop.table(cate.conds, 1) # row percentages
#prop.table(cate.conds, 2) # column percentages
fit <- ca(cate.conds)
#print(fit) # basic results
summary(fit) # extended results
#plot(fit) # symmetric map
plot(fit, mass = TRUE, contrib = "absolute", map ="rowgreen", 
     arrows = c(TRUE, FALSE)) # asymmetric map

```


### 3. Model -  derivation and corresponding functions

Under the assumption $H=i$ is independent from $W=j$
$$P(C=k|H=i,W=j)=\frac{P(C=k,H=i,W=j)}{P(H=i,W=j)}=\frac{P(H=i,W=j|C=k)*P(C=k)}{P(H=i)*P(W=j)}  (1) $$ 


since $H=i$ is independent from $W=j$,
$$Exp[P(H=i,W=j|C=k)]=P(H=i|C=k)*P(W=j|C=k)  (2)$$

therefore, 
$$Exp[P(C=k|H=i,W=j)]=Exp[ \frac{P(H=i,W=j|C=k)*P(C=k)} {P(H=i)*P(W=j)}] \\\
=\frac{P(H=i|C=k)*P(W=j|C=k)*P(C=k)}{P(H=i)*P(W=j)} \\\
=\frac{\frac{P(H=i,C=k)}{P(C=k)}*\frac{P(W=j,C=k)}{P(C=k)}*P(C=k)}{P(H=i)*P(W=j)} \\\
=\frac{P(C=k|H=i)*P(H=i)*P(C=k|W=j)*P(W=j)}{P(H=i)*P(W=j)*P(C=k)} \\\
=\frac{P(C=k|H=i)*P(C=k|W=j)}{P(C=k)} (3)$$

* relevance contextualized by temporal factor 

$$P_{u}(C=k|H=i)=\frac{\Phi_{u}(C=k,H=i)}{\Phi_{u} (H=i)} (4)$$

```{r}
get.temporal.impact <- function(dataframe,hour){
    # dataframe.in.hour = checkin.single[which(checkin.single$hour==hour),]
    dataframe.in.hour = dataframe[which(dataframe$hour==hour),]
    phi.h = nrow(dataframe.in.hour)
    
    list.category = split(dataframe.in.hour, dataframe.in.hour$cate_l2)
    sapply(list.category, function(i){
        nrow(i)/phi.h
    })
}

```

* relevance contextualized by (unweighted meteorological) factor 

$$P_{u}(C=k|W=j)=\frac{Intercept(C=k,W=j)}{\sum Intercept(C,W=j)} (5)$$

```{r}
get.meteorological.impact <- function(fit,conds){
    
    conds.id = which(fit[["rownames"]]==conds)
    ref.vec = fit[["rowcoord"]][conds.id,1:8]
    cate.all = fit[["colcoord"]][,1:8]
    
    intercepts = apply(cate.all, 1, function(x){ 
        (x[1]*ref.vec[1] + x[2]*ref.vec[2] + x[3]*ref.vec[3] + x[4]*ref.vec[4] +
             x[5]*ref.vec[5] + x[6]*ref.vec[6] + x[7]*ref.vec[7] + x[8]*ref.vec[8] ) /
            (ref.vec[1]^2 + ref.vec[2]^2 + ref.vec[3]^2 + ref.vec[4]^2 +
                 ref.vec[5]^2 + ref.vec[6]^2 + ref.vec[7]^2 + ref.vec[8]^2 ) 
        } )
    
#     vec = intercepts / sum(intercepts)  !!!wrong!!! intercetp can be negative
    vec = (intercepts - min(intercepts)) / (max(intercepts)-min(intercepts)) # scale into [0,1]
    names(vec) = fit[["colnames"]]
    
    vec
}

get.meteorological.impact2 <- function(dataframe,conds){
    
    dataframe.in.conds = dataframe[which(dataframe$conds==conds),]
    phi.c = nrow(dataframe.in.conds)
    
    list.category = split(dataframe.in.conds, dataframe.in.conds$cate_l2)
    sapply(list.category, function(i){
        nrow(i)/phi.c
    })
}

```


* weighted meteorological factor 

$$P_{u}^{*} (C=k|W=j)= w_{j}*[P_{u}(C=k|W=j)-\bar P_{u}]+\bar P_{u}$$

```{r}

get.weather.weight <- function(fit){
    
    conds.all = fit[["rowcoord"]][,1:2]
    
    mag = apply(conds.all, 1, function(x){ 
        sqrt( (x[1]^2+x[2]^2) )
        } )
    
    mag / sum(mag)
}

get.weighted.meteorological.impact <- function(fit, conds, weight){
    
#     cate.conds = xtabs(~conds+cate_l2, data=dataframe)
#     fit <- ca(cate.conds)
    
    unweighted = get.meteorological.impact(fit,conds)
#     weights = get.weather.weight(fit)
    
#     conds.id = which(fit[["rownames"]]==conds)
    
#     vec = weights[conds.id] * (unweighted-mean(unweighted)) + mean(unweighted)
    vec = weight * (unweighted-mean(unweighted)) + mean(unweighted)
    names(vec) = fit[["colnames"]]
    
    vec
    
}


get.weighted.meteorological.impact2 <- function(dataframe, conds, weight){
    
    
    unweighted = get.meteorological.impact2(dataframe, conds)

    weight * (unweighted-mean(unweighted)) + mean(unweighted)
    
    
}

```

* denominator (unweighted relevance)

$$P(C=k)=\frac{\Phi_{u} (C=k) }{\Phi_{u} }$$


```{r}
get.denominator <- function(dataframe){
    
    phi.h = nrow(dataframe)
    
    list.category = split(dataframe, dataframe$cate_l2)
    denominator = sapply(list.category, function(i){
        nrow(i)/phi.h
    })
    
    denominator

    
}
```

* the final result 

$$E[P(C=k|H=i,W=j)]=\frac{P(C=k|H=i)*P(C=k|W=j)}{P(C=k)}$$

**update @2014.09.04**

* This equation has been somehow verified :
    + although `p.k` has a relatively high performance, multipling it with others do not bringer a higher one; instead, dividing does.
* There should be some problems in the meteorological impacts. 
    + The performance based on calculation of interception is so poor that it totally brings down the whole performance. Instead, if just the simple probability distribution is adopted, the performance is getting better. 

**update @2014.09.05**

* problem has been found out. In the earlier function `get.meteorological.impact()`, I used `intercept/sum(intercept)`, where `intercept` can be negative, and the `sum(intercept)` can be nonsense. Now it is fixed to `(intercepts - min(intercepts)) / (max(intercepts)-min(intercepts)) ` to scale into [0,1].
* besides, about the equation again:
    + if `p.kj` is calculated by just using the probability distribution, then the equation `p.kij = p.ki * p.kj / p.k` is correct (which confirmes our derivation);
    + if `p.kj` is calculated by personal CA, it should be noticed that the *scaled interception* does not describe *the probability of C=k under the condition W=j*. Instead, it describes the negative/positive impacts. Hence, the conditional probability should be the *scaled interception* multipled by the priori `p.k`. Therefore, here the final result should be `p.kij = p.ki * p.kj`.
    + with a little differnt equations and different methods of getting `p.j`, the above two ways have very similar results (which is expected!).
    

```{r}
get.overall.relevance <- function(dataframe,hour, conds){
    
    p.k = get.denominator(dataframe)
    p.ki = get.temporal.impact(dataframe, hour)
    cates.list = names(p.k)
    
    #cate.conds = xtabs(~conds+cate_l2, data=checkin.global)
    cate.conds = xtabs(~conds+cate_l2, data=dataframe)
    cate.conds = 100*cate.conds+1  # avoid 0s
    fit = ca(cate.conds)
    weight = get.weather.weight(fit)[which(fit[["rownames"]]==conds)]
    
    
    p.kj = get.weighted.meteorological.impact(fit,conds,weight)[cates.list]
    p.kij = p.ki * p.kj
    
#     p.kj = get.meteorological.impact2(dataframe, conds)
#     p.kij = p.ki * p.kj / p.k
     
    list("init"=p.k, "overall"=p.kij)
}
```




* generate prediction list for specified time (n-sized)

```{r}
generate.list = function(dataframe, hour, day){
    
    reference.data = dataframe[which(dataframe$hour==hour & dataframe$yearday == day),] 
    conds = reference.data[1,"conds"]
    
    places.been.to = unique(reference.data$cate_l2)
    
    probs = get.overall.relevance(dataframe, hour, conds)
    
    probs.overall = probs[["overall"]]
    probs.overall = probs.overall[probs.overall>0]
    places.predicted.overall = names(probs.overall)[order(probs.overall,decreasing=TRUE)]
    
    probs.init = probs[["init"]]
    probs.init = probs.init[probs.init>0]
    places.predicted.init = names(probs.init)[order(probs.init,decreasing=TRUE)]
    
    
#     probs = probs[probs>0]
#     places.predicted = names(probs)[order(probs,decreasing=TRUE)]
     
    list("hour"=hour, "conds"=conds, "real"=places.been.to, 
         "pred.init" =places.predicted.init,
         "pred.overall" =places.predicted.overall)
    
}
```


* verification function

```{r}
evaluation.vec = function(gen.list, n){
    
    real = gen.list[["real"]]
    pred.init = gen.list[["pred.init"]][1:n]
    pred.init = pred.init[!is.na(pred.init)]
    pred.overall = gen.list[["pred.overall"]][1:n]
    pred.overall = pred.overall[!is.na(pred.overall)]

    correct.init = 0; correct.overall=0
    for(i in  1:length(pred.init)){
        prediction = pred.init[i]
        if(length(which(real==prediction)))
            correct.init = correct.init+1
    }
    for(i in  1:length(pred.overall)){
        prediction = pred.overall[i]
        if(length(which(real==prediction)))
            correct.overall = correct.overall+1
    }
    
    real.count = length(real)
    pred.init.count = length(pred.init)
    pred.overall.count = length(pred.overall)

    c("list.size"=n,"real"=real.count,
      "pred.init"=pred.init.count,"cor.init"=correct.init,
      "pred.overall"=pred.overall.count,"cor.overall"=correct.overall)
    
}
```

### Experiment


```{r, echo=FALSE}

checkin.single = read.csv( paste0(basedir, "data\\UserA.csv"), 
                       header=TRUE, sep=",",  na.strings = "none",
                       colClasses = c("numeric","numeric","factor","factor", "numeric","numeric",
                                      "numeric","character","factor","factor")
)
checkin.single$datetime = strptime( strtrim(checkin.single$localtime,19), format="%Y-%m-%d %H:%M:%S")

## weather data 
weather = read.csv( paste0(basedir, "data\\weather.csv"), 
                       header=TRUE, sep=",", na.strings = c("-9999","Unknown"),
                       colClasses = c("numeric","numeric","numeric","character","numeric","factor",
                                      "numeric","numeric","numeric","numeric","numeric","numeric",
                                      "numeric","numeric")
) 
## deal with logical data
weather$fog=as.logical(weather$fog)
weather$rain=as.logical(weather$rain)
weather$snow=as.logical(weather$snow)
weather$thunder=as.logical(weather$thunder)
weather$tornado=as.logical(weather$tornado)

generate.dataframe = function(checkin.global, weather){
    ## join checkin data with weather data based on timestamps 
    checkin.global = joindfsbytime(checkin.global, weather)
    
    ## deal with time 
    checkin.global$hour = as.factor(format(checkin.global$datetime,"%H"))
    checkin.global$yearday = format(checkin.global$datetime,"%j")
    checkin.global$weekday = format(checkin.global$datetime,"%w")
    checkin.global$isweekend = as.factor(ifelse( ( checkin.global$weekday>5 ), "Saturday", 
        ifelse( ( checkin.global$weekday<1 ),"Sunday","Workday")))
    
    ## add record for last checkin
    # checkin.global = copylastcheckinrec(checkin.global)
    checkin.global = checkin.global[complete.cases(checkin.global$conds),]
    
    checkin.global
}

checkin.single = generate.dataframe(checkin.single, weather)

```

* Experiment with a single user (with verification) 

```{r, fig.width=8, fig.height=4, dpi=300}

# random sample
all.id = 1:nrow(checkin.single)
size.tests = 100
size.list = 10

sample.id = sample(all.id, size.tests)

performance = data.frame()
    
for (i in 1:size.tests){
    id = sample.id[i]
    hour = checkin.single[id,"hour"]
    day = checkin.single[id,"yearday"]
        
    # prediction list
    pred.list = generate.list(checkin.single, hour, day)
        
    for(n in 1:size.list){
        performance=rbind(performance, evaluation.vec(pred.list,n))
    }
      
}
colnames(performance)=c("list.size","real","pred.init","cor.init","pred.overall","cor.overall")
    

performance.by.size = split(performance,performance$list.size)
precision.recall = sapply(performance.by.size, function(i){
    n = i[1,"list.size"]
    precision.init = sum(i$cor.init) / sum(i$pred.init)
    recall.init = sum(i$cor.init) / sum(i$real)
    precision.overall = sum(i$cor.overall) / sum(i$pred.overall)
    recall.overall = sum(i$cor.overall) / sum(i$real)
    
    c("n"=i[1,"list.size"],"precision.priori"=precision.init,"recall.priori"=recall.init,
      "precision.contextualized"=precision.overall,"recall.contextualized"=recall.overall)    
})

precision.recall=as.data.frame(t(precision.recall))
precision.recall= melt(precision.recall,id.vars="n")


# plot
# ppi=300
# png(paste0(basedir,"img\\plot_precision_recall_userD.png"), width = 8*ppi, height = 4*ppi, res=ppi)
# ggplot(data=precision.recall, aes(x=n, y=value, group = variable, colour = variable)) +
#     geom_line() +
#     geom_point( size=4, shape=21, fill="white")
# dev.off()

ggplot(data=precision.recall, aes(x=n, y=value, group = variable, colour = variable)) +
    geom_line() +
    geom_point( size=4, shape=21, fill="white")

```



