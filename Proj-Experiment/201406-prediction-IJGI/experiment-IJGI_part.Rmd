---
title: "IJGI"
author: "Ming Li"
date: "Thursday, January 15, 2015"
output: html_document
---

---
title: "Demonstration of Experiments (paper IJGI)"
author: "Ming Li"
date: "Tuesday, August 19, 2014"
output: html_document
---


```{r,echo=FALSE,include=FALSE}
library(rgeos)
library(scales)
library(reshape2)
library(ggplot2)
library(gridExtra)
#library(TSA)
library(ca)

source("../../global/functions/prepare.checkin.R")
source("../../global/functions/basic.stats.plots.R")
source("../../global/functions/spatial.analysis.R")
source("../../global/functions/etc.R")

# global variable
ppi=300

crs.wgs84.str = "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"

```

First, we load the data into the workspace. 

```{r,echo=FALSE}
# checkin.NY = prepare.checkin("../../global/data/csv-raw/NewYorkCity.csv",
#                              is.raw=TRUE, 
#                              weather.data="../../global/data/csv-raw/Weather_NY.csv", 
#                              convert.time=TRUE, add.history=TRUE)
# save(checkin.NY,file="../../global/data/Rda-saved/checkin.global.weather.NY.Rda")
load(file="../../global/data/Rda-saved/checkin.global.weather.NY.Rda")
```

Following is a summary of the dataset.

```{r}
summary(checkin.NY)
```


### 1. Exploratory analysis 

#### Temporal: Frequency domain

 

```{r}
freq.plot(checkin.NY,title="New York City")
```

The analysis in the frequency domain reveals a strong harmonics with a period 
of 24 hours. Therefore, it is reasonable to analyze the data by 24 hours.

#### Temproal: Time domain 

The distribution of the check-in categories accross one period (24 hours).

```{r}
time.distribution.plot(checkin.NY,title="New York City")
```

#### Temporal: Radial plot

The distribution of the check-in categories in 24 hours in radial plots.

```{r}
time.radial.plot(checkin.NY)
```

### Spatial

have a look at a intuitive spatiotemporal visualizaiton of the data...

```{r,eval=FALSE}
basemap = map.plot("../../global/data/shapefiles", 
                   "NYC_borough_boundaries_WGS84",
                   alpha=0.1,size=0.3,color="grey")

saveGIF({
  point.animation.plot(checkin.NY, more.aes=aes(color=cate_l1),
                       basemap=basemap, size=3) + 
      ggtile("New York City")
}, interval = 0.5, movie.name = "spatiotemporal.gif", 
ani.width = 1500, ani.height = 1200)

```

<img src="spatiotemporal.gif" />

What will the statistics say when we neglect the temporal factors?
First of all, if we try to find spatial clusters 
based on differnt checkin categories, the distribution of the founded clusters
are quite differnt. It indicates that each category has differnt correlations
with the geographic space.

```{r}
if(!exists("basemap")){
    basemap <- map.plot("../../global/data/shapefiles", 
                   "NYC_borough_boundaries_WGS84",
                   alpha=0.1,size=0.3,color="grey")
}

plots <- lapply(split(checkin.NY,checkin.NY$cate_l1),function(ci){
    # find out clusters for the type of category
    clusters = spatial.clustering(ci)
    
    centers = clusters[["centers"]]
    points = clusters[["point.unique"]]
    wss = clusters[["wss"]]
    pct = clusters[["pct"]]
    lbl = paste(nrow(centers),"clusters;\nWSS:",
                  formatC(wss,digits=2,format="f"),
                  "(",format.percent(pct),")")
    
    # add basic points
    gg.map <- point.plot(points,x="lon.x",y="lat.x",alpha=0.05, basemap=basemap)
    
    # add cluster information
    gg.map <- point.plot(centers, x="lon.center",y="lat.center",
                         more.aes = aes(color=cid.ordered),
                         basemap=gg.map,
                         xlim=range(checkin.NY$lon,finite=TRUE),
                         ylim=range(checkin.NY$lat,finite=TRUE),
                         size = log(centers$size,5),alpha = 0.7)
    
    # some plot configuration
    gg.map <- gg.map + ggtitle(ci[1,"cate_l1"]) +
        theme(legend.position="none") + 
        geom_text(aes(x = -74.13, y = 40.87), label = lbl, size=2) 
    
})


# png("categorized_clustering.png",width=15*ppi,height=6*ppi,res=ppi)
# grid.arrange(plots[[1]],plots[[2]],plots[[3]],plots[[4]],
#              plots[[5]],plots[[6]],plots[[7]],plots[[8]], 
#              plots[[9]],plots[[10]],nrow=4, ncol=3)
# dev.off()

plots[[1]];plots[[2]];plots[[3]];plots[[4]];plots[[5]];plots[[6]];plots[[7]];plots[[8]]; plots[[9]];plots[[10]]
```

We could also see the most domimant categories in each part of the entire city.

```{r}

# plot the map, and each polygon is filled with its dominant category
# SPDF: a SpatialPolygonDataFrame, which should include the information for 
# filling options

# prepare polygon data
SPDF = readOGR(dsn = "../../global/data/shapefiles", layer = "NYC_zipcode")
# classify the polygons in the SPDF by the overlapped point data 
SPDF = classify.polygon.by.point(point=checkin.NY, SPDF, clsfy.attr="cate_l1")
mapdf=df.from.spdf(SPDF)
        # mapdf$density=apply(mapdf,1,function(i){i[i["cate.dom"]]})
        # mapdf$density=as.numeric(formatC(mapdf$density,digits=1,format = "f"))

map.plot(mapdf = mapdf, more.aes = aes(fill=Category.1st),
         color="grey",size=0.3,alpha=0.7)+
    xlab("")+ylab("")


```

Such map can also be visualized with time to show the impact of both time and
space...

```{r}
map.animation.plot(point=checkin.NY,SPDF,more.aes=aes(fill=Category.1st))
```


### Meteorological: Correspondence analysis

```{r}
cate.conds = xtabs(~conds+cate_l1, data=checkin.NY)
#prop.table(cate.conds, 1) # row percentages
#prop.table(cate.conds, 2) # column percentages
fit <- ca(cate.conds)
#print(fit) # basic results
summary(fit) # extended results
#plot(fit) # symmetric map
plot(fit, mass = TRUE, contrib = "absolute", map ="rowgreen", 
     arrows = c(TRUE, FALSE)) # asymmetric map

```


### 3. Model -  derivation and corresponding functions

Under the assumption $H=i$ is independent from $W=j$
$$P(C=k|H=i,W=j)=\frac{P(C=k,H=i,W=j)}{P(H=i,W=j)}=\frac{P(H=i,W=j|C=k)*P(C=k)}{P(H=i)*P(W=j)}  (1) $$ 


since $H=i$ is independent from $W=j$,
$$Exp[P(H=i,W=j|C=k)]=P(H=i|C=k)*P(W=j|C=k)  (2)$$

therefore, 
$$Exp[P(C=k|H=i,W=j)]=Exp[ \frac{P(H=i,W=j|C=k)*P(C=k)} {P(H=i)*P(W=j)}] \\\
=\frac{P(H=i|C=k)*P(W=j|C=k)*P(C=k)}{P(H=i)*P(W=j)} \\\
=\frac{\frac{P(H=i,C=k)}{P(C=k)}*\frac{P(W=j,C=k)}{P(C=k)}*P(C=k)}{P(H=i)*P(W=j)} \\\
=\frac{P(C=k|H=i)*P(H=i)*P(C=k|W=j)*P(W=j)}{P(H=i)*P(W=j)*P(C=k)} \\\
=\frac{P(C=k|H=i)*P(C=k|W=j)}{P(C=k)} (3)$$

* relevance contextualized by temporal factor 

$$P_{u}(C=k|H=i)=\frac{\Phi_{u}(C=k,H=i)}{\Phi_{u} (H=i)} (4)$$

```{r}
get.temporal.impact <- function(dataframe,hour){
    # dataframe.in.hour = checkin.single[which(checkin.single$hour==hour),]
    dataframe.in.hour = dataframe[which(dataframe$hour==hour),]
    phi.h = nrow(dataframe.in.hour)
    
    list.category = split(dataframe.in.hour, dataframe.in.hour$cate_l2)
    sapply(list.category, function(i){
        nrow(i)/phi.h
    })
}

```

* relevance contextualized by (unweighted meteorological) factor 

$$P_{u}(C=k|W=j)=\frac{Intercept(C=k,W=j)}{\sum Intercept(C,W=j)} (5)$$

```{r}
get.meteorological.impact <- function(fit,conds){
    
    conds.id = which(fit[["rownames"]]==conds)
    ref.vec = fit[["rowcoord"]][conds.id,1:8]
    cate.all = fit[["colcoord"]][,1:8]
    
    intercepts = apply(cate.all, 1, function(x){ 
        (x[1]*ref.vec[1] + x[2]*ref.vec[2] + x[3]*ref.vec[3] + x[4]*ref.vec[4] +
             x[5]*ref.vec[5] + x[6]*ref.vec[6] + x[7]*ref.vec[7] + x[8]*ref.vec[8] ) /
            (ref.vec[1]^2 + ref.vec[2]^2 + ref.vec[3]^2 + ref.vec[4]^2 +
                 ref.vec[5]^2 + ref.vec[6]^2 + ref.vec[7]^2 + ref.vec[8]^2 ) 
        } )
    
#     vec = intercepts / sum(intercepts)  !!!wrong!!! intercetp can be negative
    vec = (intercepts - min(intercepts)) / (max(intercepts)-min(intercepts)) # scale into [0,1]
    names(vec) = fit[["colnames"]]
    
    vec
}

get.meteorological.impact2 <- function(dataframe,conds){
    
    dataframe.in.conds = dataframe[which(dataframe$conds==conds),]
    phi.c = nrow(dataframe.in.conds)
    
    list.category = split(dataframe.in.conds, dataframe.in.conds$cate_l2)
    sapply(list.category, function(i){
        nrow(i)/phi.c
    })
}

```


* weighted meteorological factor 

$$P_{u}^{*} (C=k|W=j)= w_{j}*[P_{u}(C=k|W=j)-\bar P_{u}]+\bar P_{u}$$

```{r}

get.weather.weight <- function(fit){
    
    conds.all = fit[["rowcoord"]][,1:2]
    
    mag = apply(conds.all, 1, function(x){ 
        sqrt( (x[1]^2+x[2]^2) )
        } )
    
    mag / sum(mag)
}

get.weighted.meteorological.impact <- function(fit, conds, weight){
    
#     cate.conds = xtabs(~conds+cate_l2, data=dataframe)
#     fit <- ca(cate.conds)
    
    unweighted = get.meteorological.impact(fit,conds)
#     weights = get.weather.weight(fit)
    
#     conds.id = which(fit[["rownames"]]==conds)
    
#     vec = weights[conds.id] * (unweighted-mean(unweighted)) + mean(unweighted)
    vec = weight * (unweighted-mean(unweighted)) + mean(unweighted)
    names(vec) = fit[["colnames"]]
    
    vec
    
}


get.weighted.meteorological.impact2 <- function(dataframe, conds, weight){
    
    
    unweighted = get.meteorological.impact2(dataframe, conds)

    weight * (unweighted-mean(unweighted)) + mean(unweighted)
    
    
}

```

* denominator (unweighted relevance)

$$P(C=k)=\frac{\Phi_{u} (C=k) }{\Phi_{u} }$$


```{r}
get.denominator <- function(dataframe){
    
    phi.h = nrow(dataframe)
    
    list.category = split(dataframe, dataframe$cate_l2)
    denominator = sapply(list.category, function(i){
        nrow(i)/phi.h
    })
    
    denominator

    
}
```

* the final result 

$$E[P(C=k|H=i,W=j)]=\frac{P(C=k|H=i)*P(C=k|W=j)}{P(C=k)}$$

**update @2014.09.04**

* This equation has been somehow verified :
    + although `p.k` has a relatively high performance, multipling it with others do not bringer a higher one; instead, dividing does.
* There should be some problems in the meteorological impacts. 
    + The performance based on calculation of interception is so poor that it totally brings down the whole performance. Instead, if just the simple probability distribution is adopted, the performance is getting better. 

**update @2014.09.05**

* problem has been found out. In the earlier function `get.meteorological.impact()`, I used `intercept/sum(intercept)`, where `intercept` can be negative, and the `sum(intercept)` can be nonsense. Now it is fixed to `(intercepts - min(intercepts)) / (max(intercepts)-min(intercepts)) ` to scale into [0,1].
* besides, about the equation again:
    + if `p.kj` is calculated by just using the probability distribution, then the equation `p.kij = p.ki * p.kj / p.k` is correct (which confirmes our derivation);
    + if `p.kj` is calculated by personal CA, it should be noticed that the *scaled interception* does not describe *the probability of C=k under the condition W=j*. Instead, it describes the negative/positive impacts. Hence, the conditional probability should be the *scaled interception* multipled by the priori `p.k`. Therefore, here the final result should be `p.kij = p.ki * p.kj`.
    + with a little differnt equations and different methods of getting `p.j`, the above two ways have very similar results (which is expected!).
    