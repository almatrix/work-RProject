---
title: "IJGI"
author: "Ming Li"
date: "Thursday, January 15, 2015"
output: html_document
---


```{r,echo=FALSE,include=FALSE}
library(rgeos)
library(scales)
library(reshape2)
library(ggplot2)
library(gridExtra)
#library(TSA)
library(ca)

source("../../global/functions/prepare.checkin.R")
source("../../global/functions/basic.stats.plots.R")
source("../../global/functions/spatial.analysis.R")
source("../../global/functions/etc.R")

# global variable
ppi=300

crs.wgs84.str = "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"

```

First, we load the data into the workspace. 

```{r,echo=FALSE}
checkin.NY = prepare.checkin("../../global/data/csv-raw/NewYorkCity.csv",
                             is.raw=TRUE, 
#                              weather.data="../../global/data/csv-raw/Weather_NY.csv", 
                             convert.time=TRUE, add.history=FALSE)
# save(checkin.NY,file="../../global/data/Rda-saved/checkin.global.weather.NY.Rda")
# load(file="../../global/data/Rda-saved/checkin.global.weather.NY.Rda")
```

Following is a summary of the dataset.

```{r}
summary(checkin.NY)
```


### 1. Exploratory analysis 

#### Temporal: Frequency domain

 

```{r}
freq.plot(checkin.NY,title="New York City")
```

The analysis in the frequency domain reveals a strong harmonics with a period 
of 24 hours. Therefore, it is reasonable to analyze the data by 24 hours.

#### Temproal: Time domain 

The distribution of the check-in categories accross one period (24 hours).

```{r}
time.distribution.plot(checkin.NY,title="New York City")
```

#### Temporal: Radial plot

The distribution of the check-in categories in 24 hours in radial plots.

```{r}
time.radial.plot(checkin.NY)
```

### Spatial

have a look at a intuitive spatiotemporal visualizaiton of the data...

```{r,eval=FALSE}
basemap = map.plot("../../global/data/shapefiles", 
                   "NYC_borough_boundaries_WGS84",
                   alpha=0.1,size=0.3,color="grey")

saveGIF({
  point.animation.plot(checkin.NY, more.aes=aes(color=cate_l1),
                       basemap=basemap, size=3, title="New York City")
}, interval = 0.5, movie.name = "spatiotemporal.gif", 
ani.width = 1500, ani.height = 1200)


```

<img src="spatiotemporal.gif" />

What will the statistics say when we neglect the temporal factors?
First of all, if we try to find spatial clusters 
based on differnt checkin categories, the distribution of the founded clusters
are quite differnt. It indicates that each category has differnt correlations
with the geographic space.

```{r}
if(!exists("basemap")){
    basemap <- map.plot("../../global/data/shapefiles", 
                   "NYC_borough_boundaries_WGS84",
                   alpha=0.1,size=0.3,color="grey")
}

plots <- lapply(split(checkin.NY,checkin.NY$cate_l1),function(ci){
    # find out clusters for the type of category
    clusters = spatial.clustering(ci)
    
    centers = clusters[["centers"]]
    points = clusters[["point.unique"]]
    wss = clusters[["wss"]]
    pct = clusters[["pct"]]
    lbl = paste(nrow(centers),"clusters;\nWSS:",
                  formatC(wss,digits=2,format="f"),
                  "(",format.percent(pct),")")
    
    # add basic points
    gg.map <- point.plot(points,x="lon.x",y="lat.x",alpha=0.05, basemap=basemap)
    
    # add cluster information
    gg.map <- point.plot(centers, x="lon.center",y="lat.center",
                         more.aes = aes(color=cid.ordered),
                         basemap=gg.map,
                         xlim=range(checkin.NY$lon,finite=TRUE),
                         ylim=range(checkin.NY$lat,finite=TRUE),
                         size = log(centers$size,5),alpha = 0.7)
    
    # some plot configuration
    gg.map <- gg.map + ggtitle(ci[1,"cate_l1"]) +
        theme(legend.position="none") + 
        geom_text(aes(x = -74.13, y = 40.87), label = lbl, size=2) 
    
})


# png("categorized_clustering.png",width=15*ppi,height=6*ppi,res=ppi)
# grid.arrange(plots[[1]],plots[[2]],plots[[3]],plots[[4]],
#              plots[[5]],plots[[6]],plots[[7]],plots[[8]], 
#              plots[[9]],plots[[10]],nrow=4, ncol=3)
# dev.off()

plots[[1]];plots[[2]];plots[[3]];plots[[4]];plots[[5]];plots[[6]];plots[[7]];plots[[8]]; plots[[9]];plots[[10]]
```

We could also see the most domimant categories in each part of the entire city.

```{r}

# plot the map, and each polygon is filled with its dominant category
# SPDF: a SpatialPolygonDataFrame, which should include the information for 
# filling options

# prepare polygon data
SPDF = readOGR(dsn = "../../global/data/shapefiles", layer = "NYC_zipcode")
# classify the polygons in the SPDF by the overlapped point data, 
# and convert the result to normal R dataframe for plotting
mapdf=df.from.spdf(classify.polygon.by.point(point=checkin.NY, 
                                             SPDF, clsfy.attr="cate_l1"))
        # mapdf$density=apply(mapdf,1,function(i){i[i["cate.dom"]]})
        # mapdf$density=as.numeric(formatC(mapdf$density,digits=1,format = "f"))

map.plot(mapdf = mapdf, more.aes = aes(fill=Category.1st),
         color="grey",size=0.3,alpha=0.7)+
    xlab("")+ylab("")


```

Such map can also be visualized with time to show the impact of both time and
space...

```{r}

saveGIF({
  map.animation.plot(point=checkin.NY,SPDF,more.aes=aes(fill=Category.1st),
                     color="grey",size=0.3,alpha=0.7)
}, interval = 0.5, movie.name = "cateogrized.polygon.spatiotemporal.gif", 
ani.width = 1500, ani.height = 1200)

```
<img src="cateogrized.polygon.spatiotemporal.gif" />

### Meteorological: Correspondence analysis

```{r}
cate.conds = xtabs(~conds+cate_l2, data=checkin.NY)
#prop.table(cate.conds, 1) # row percentages
#prop.table(cate.conds, 2) # column percentages
fit <- ca(cate.conds)
#print(fit) # basic results
summary(fit) # extended results
#plot(fit) # symmetric map
plot(fit, mass = TRUE, contrib = "absolute", map ="rowgreen", 
     arrows = c(TRUE, FALSE)) # asymmetric map

```


### 3. Model -  derivation and corresponding functions

Under the assumption $H=i$ is independent from $W=j$
$$P(C=k|H=i,W=j)=\frac{P(C=k,H=i,W=j)}{P(H=i,W=j)}=\frac{P(H=i,W=j|C=k)*P(C=k)}{P(H=i)*P(W=j)}  (1) $$ 


since $H=i$ is independent from $W=j$,
$$Exp[P(H=i,W=j|C=k)]=P(H=i|C=k)*P(W=j|C=k)  (2)$$

therefore, 
$$Exp[P(C=k|H=i,W=j)]=Exp[ \frac{P(H=i,W=j|C=k)*P(C=k)} {P(H=i)*P(W=j)}] \\\
=\frac{P(H=i|C=k)*P(W=j|C=k)*P(C=k)}{P(H=i)*P(W=j)} \\\
=\frac{\frac{P(H=i,C=k)}{P(C=k)}*\frac{P(W=j,C=k)}{P(C=k)}*P(C=k)}{P(H=i)*P(W=j)} \\\
=\frac{P(C=k|H=i)*P(H=i)*P(C=k|W=j)*P(W=j)}{P(H=i)*P(W=j)*P(C=k)} \\\
=\frac{P(C=k|H=i)*P(C=k|W=j)}{P(C=k)} (3)$$

* relevance contextualized by temporal factor 

$$P_{u}(C=k|H=i)=\frac{\Phi_{u}(C=k,H=i)}{\Phi_{u} (H=i)} (4)$$

```{r}
get.temporal.impact <- function(dataframe){
    
    hours = formatC(c(0:23),width=2,flag="0")
    
    do.call(rbind, lapply(hours, function(hour){
        data.in.hour = dataframe[dataframe$hour==hour,]
        freq.tab = as.data.frame(table(data.in.hour$cate_l2))
        freq.df = data.frame(t(freq.tab$Freq / sum(freq.tab$Freq) ))
        colnames(freq.df) = freq.tab$Var1
        
        freq.df
    }) )

}


```


* relevance contextualized by spatial factor
```{r}
checkin.in.poly = na.omit(point.in.poly(checkin.NY, SPDF, copy.attr="POSTAL"))
get.spatial.impact <- function(point.in.poly, 
                               poly.attr="POSTAL",cate.attr="cate_l2"){
    mat = cate.distr.in.poly(point.in.poly, poly.attr=poly.attr,cate.attr=cate.attr)
    mat[,1:(ncol(mat)-4)]
}

```

* relevance contextualized by previous visiting 
```{r}
get.previous.impact <- function(dataframe){

    # weight by time interval
    dataframe$weight = 2^(-1* dataframe$time.interval / 3600)
    
    tabs = xtabs(weight ~ last.cate_l2 + cate_l2, data=dataframe)

    as.data.frame.matrix( t(apply(tabs,1, function(i){i/(sum(i)+1E-6)})))
}

```


* relevance contextualized by (unweighted meteorological) factor 

$$P_{u}(C=k|W=j)=\frac{Intercept(C=k,W=j)}{\sum Intercept(C,W=j)} (5)$$

```{r}
get.meteorological.impact <- function(fit){
    
    conds = fit[["rownames"]]
    
    do.call(rbind, lapply(conds, function(cond){
        conds.id = which(fit[["rownames"]]==conds)
        ref.vec = fit[["rowcoord"]][conds.id,1:8]
        cate.all = fit[["colcoord"]][,1:8]
        
        intercepts = apply(cate.all, 1, function(x){ 
        (x[1]*ref.vec[1] + x[2]*ref.vec[2] + x[3]*ref.vec[3] + x[4]*ref.vec[4] +
             x[5]*ref.vec[5] + x[6]*ref.vec[6] + x[7]*ref.vec[7] + x[8]*ref.vec[8] ) /
            (ref.vec[1]^2 + ref.vec[2]^2 + ref.vec[3]^2 + ref.vec[4]^2 +
                 ref.vec[5]^2 + ref.vec[6]^2 + ref.vec[7]^2 + ref.vec[8]^2 ) 
        } )
        
        # vec = intercepts / sum(intercepts)  !!!wrong!!! intercetp can be negative
        # scale into [0,1]
        vec = (intercepts - min(intercepts)) / (max(intercepts)-min(intercepts)) 

        df = data.frame(t(vec))
        colnames(df) = fit[["colnames"]]
        rownames(df) = cond
        
        df
    }) )

}

get.meteorological.impact2 <- function(dataframe){

    do.call(rbind, lapply(split(dataframe,dataframe$conds), function(conds){
        freq.tab = as.data.frame(table(conds$cate_l2))
        freq.df = data.frame(t(freq.tab$Freq / sum(freq.tab$Freq) ))
        colnames(freq.df) = freq.tab$Var1
        
        freq.df
    }) )
}

```


* weighted meteorological factor 

$$P_{u}^{*} (C=k|W=j)= w_{j}*[P_{u}(C=k|W=j)-\bar P_{u}]+\bar P_{u}$$

```{r}

get.weather.weight <- function(fit){
    conds.all = fit[["rowcoord"]][,1:2]
    
    mag = apply(conds.all, 1, function(x){ 
        sqrt( (x[1]^2+x[2]^2) )
        } )
    
    vec = mag / sum(mag)
    names(vec) = fit[["rownames"]]
    vec
}

get.weighted.meteorological.impact <- function(fit){
    
#     cate.conds = xtabs(~conds+cate_l2, data=dataframe)
#     fit <- ca(cate.conds)
    conds = fit[["rownames"]]
    
    unweighted = get.meteorological.impact(fit)
    weights = get.weather.weight(fit)
    
#     conds.id = which(fit[["rownames"]]==conds)
    
#     vec = weights[conds.id] * (unweighted-mean(unweighted)) + mean(unweighted)
    do.call(rbind, lapply(conds, function(cond){
        weight = weights[cond]
        unweighted.cond = unlist(unweighted[cond,])
        vec = weight * (unweighted.cond-mean(unweighted.cond,na.rm = TRUE)) + 
            mean(unweighted.cond,na.rm = TRUE)

        df = data.frame(t(vec))
        colnames(df) = fit[["colnames"]]
        rownames(df) = cond
        
        df
    }))
}


get.weighted.meteorological.impact2 <- function(dataframe, conds, weight){
    
    
    unweighted = get.meteorological.impact2(dataframe, conds)

    weight * (unweighted-mean(unweighted)) + mean(unweighted)
    
    
}

```

* denominator (unweighted relevance)

$$P(C=k)=\frac{\Phi_{u} (C=k) }{\Phi_{u} }$$


```{r}
get.denominator <- function(dataframe){
    
    freq.tab = as.data.frame(table(dataframe$cate_l2))
    
    df = data.frame(t(freq.tab$Freq / sum(freq.tab$Freq)))
    colnames(df) = freq.tab$Var1
    
    df
}
```

* the final result 

$$E[P(C=k|H=i,W=j)]=\frac{P(C=k|H=i)*P(C=k|W=j)}{P(C=k)}$$

1. global & individual

```{r}

# order the data by timestamps
checkin.in.poly = checkin.in.poly[order(checkin.in.poly$timestamps),]

# using the data from the first four months as learning data
learning.length = length(which(checkin.in.poly$yearday<152))
learning.all.data = checkin.in.poly[1:learning.length,]
# and the data from the last month as prediction data
experiment.data = checkin.in.poly[(learning.length+1):nrow(checkin.in.poly),]
# add a flag for spatiotemporal context in the prediction data 
experiment.data$flag = paste(experiment.data$hour,experiment.data$POSTAL)

# get the global matrix
pr.t.mat.global = get.temporal.impact(learning.all.data)
pr.s.mat.global = get.spatial.impact(learning.all.data)
pr.d.global = unlist(get.denominator(learning.all.data))+ 1E-6

# making predictions (both globally and individually) for each record
counter = 0; counter2=0;
prediction= do.call(rbind, lapply(
    split(experiment.data,experiment.data$user_id),function(user){
        
        # information output
        counter <<- counter + nrow(user); counter2 <<- counter2 +1;
        print(paste(Sys.time(),"User",counter2,"; Finished", counter,"records."))
        
        # leaning from the historical data of the individual
        user.id = user[1,"user_id"]
        # is there any learning data for this individual?
        learning = learning.all.data[which(learning.all.data$user_id==user.id),]
        learning.length = nrow(learning)
        if(learning.length==0){ # using global matrix 
            pr.t.mat.indiv = pr.t.mat.global
            pr.s.mat.indiv = pr.s.mat.global
            pr.d.indiv = pr.d.global
        }else{ # using his individual matrix
            pr.t.mat.indiv = get.temporal.impact(learning) 
            pr.s.mat.indiv = get.spatial.impact(learning) 
            pr.d.indiv = unlist(get.denominator(learning)) * 0.7 + 0.3 * pr.d.global
        }
        
        # making predictions
        do.call(rbind, lapply(split(user, user$flag),function(i){
            # context information
            hour = i[1,"hour"]
            polygon = i[1,"POSTAL"]
            # the real checkin category
            real.list = i$cate_l2
            
            # get the vector
            pr.t.global = unlist(pr.t.mat.global[hour,]) # global information
            if(is.na(pr.t.mat.indiv[hour,1])){ # no information for that hour
                pr.t.indiv = pr.t.global # using the global information
                flag.t = FALSE
            }else{
                pr.t.indiv = unlist(pr.t.mat.indiv[hour,]) * 0.7 + 0.3 * pr.t.global
                flag.t = TRUE
            }
            # get the vector
            pr.s.global = unlist(pr.s.mat.global[polygon,]) # global informaiton
            if(is.na(pr.s.mat.indiv[polygon,1])){ # no information for that place
                pr.s.indiv = pr.s.global # using the global information 
                flag.s = FALSE
            }else{
                pr.s.indiv = unlist(pr.s.mat.indiv[polygon,]) * 0.7 + 0.3 * pr.s.global
                flag.s = TRUE
            }
            
            # the average evaluation
            pr.st.global = pr.t.global * pr.s.global / pr.d.global 
            if(flag.s + flag.t == 2) {
                pr.st.indiv = pr.t.indiv * pr.s.indiv / pr.d.indiv
                pr.st.indiv2 = pr.st.indiv
                pr.st.indiv3 = pr.st.indiv
                }
            if(flag.s + flag.t == 1){
                pr.st.indiv = pr.t.indiv * pr.s.indiv 
                pr.st.indiv2 = pr.t.indiv * pr.s.indiv / pr.d.global
                pr.st.indiv3 = pr.t.indiv * pr.s.indiv / pr.d.indiv
                 }
            if(flag.s + flag.t == 0){
                pr.st.indiv = pr.t.indiv * pr.s.indiv / pr.d.global
                pr.st.indiv2 = pr.st.indiv
                pr.st.indiv3 = pr.st.indiv
                }
            # order the probability vector            
            pred.d.global = sort(pr.d.global, decreasing = TRUE)
            pred.t.global = sort(pr.t.global, decreasing = TRUE)
            pred.s.global = sort(pr.s.global, decreasing = TRUE)
            pred.st.global = sort(pr.st.global, decreasing = TRUE)
            
            pred.d.indiv = sort(pr.d.indiv, decreasing = TRUE)
            pred.t.indiv = sort(pr.t.indiv, decreasing = TRUE)
            pred.s.indiv = sort(pr.s.indiv, decreasing = TRUE)
            pred.st.indiv = sort(pr.st.indiv, decreasing = TRUE)
            pred.st.indiv2 = sort(pr.st.indiv2, decreasing = TRUE)
            pred.st.indiv3 = sort(pr.st.indiv3, decreasing = TRUE)
            
            df = do.call(rbind, lapply(real.list,function(real){
                
                # find the correct position (required length to be correct)
                p1 =  which(names(pred.d.global)==as.character(real))
                p2 =  which(names(pred.t.global)==as.character(real))
                p3 =  which(names(pred.s.global)==as.character(real))
                p4 =  which(names(pred.st.global)==as.character(real))
                
                p5 =  which(names(pred.d.indiv)==as.character(real))
                p6 =  which(names(pred.t.indiv)==as.character(real))
                p7 =  which(names(pred.s.indiv)==as.character(real))
                p8 =  which(names(pred.st.indiv)==as.character(real))
                p9 =  which(names(pred.st.indiv2)==as.character(real))
                p10 =  which(names(pred.st.indiv3)==as.character(real))
          
                data.frame("None.Global"=p1,
                           "None.Individual"=p5,
                           "Temporal.Global"=p2,
                           "Temporal.Individual"=p6,
                           "Spatial.Global"=p3,
                           "Spatial.Individual"=p7,
                           "ST.Global"=p4,
                           "ST.Individual"=p8,
                           "ST.Individual2"=p9,
                           "ST.Individual3"=p10,
                           "Hour"=hour,
                           "Postal"=polygon,
                           "Real.Category"=real,
                           "Learning.Length"=learning.length,
                           "Learning.T"=flag.t,
                           "Learning.S"=flag.s)

            }))
            
            # additional information 
            df$user = user.id
            df$gid = i$gid
            
            df
    }))
    
}))



```



2. Analysis

compare the result with other results that did not consider or consider only
one of the contextual factors

```{r}

# show how context can improve prediction
prediction.melt = melt(prediction[,c(1,2,7,9)])
prediction.melt$Context = rep(c("No Context","With Context"),each=nrow(prediction)*2)
prediction.melt$Context = factor(prediction.melt$Context,
                                 levels = c("No Context","With Context"))
prediction.melt$Group = rep(rep(c("Global","Individual"),each=nrow(prediction)),2)

png("group.comparision.png",width=6*ppi,height=3*ppi,res=ppi)
ggplot(prediction.melt,aes(x=value))+
    geom_histogram(aes(y=..density..),binwidth=1,fill=NA, color="#666666")+
    geom_density()+
    theme_gray(base_size=8)+
    xlim(1,30)+
    xlab("Length of Prediction List for Correct Prediction")+
    ylab("Count")+
    facet_grid(Group~Context)
dev.off()


rate = do.call( rbind, lapply(1:30, function(N){ # list length: N
    # number of correct
    c1 = length(which(prediction$None.Global<=N)) / nrow(prediction)
    c2 = length(which(prediction$None.Individual<=N)) / nrow(prediction)
    c3 = length(which(prediction$ST.Global<=N)) / nrow(prediction)
#     c4 = length(which(prediction$ST.Individual<=N)) / nrow(prediction)
    c5 = length(which(prediction$ST.Individual2<=N)) / nrow(prediction)
#     c6 = length(which(prediction$ST.Individual3<=N)) / nrow(prediction)

    data.frame("Length"=c(N,N,N,N),"value"=c(c1,c2,c3,c5),
               "Group"=c("NoContext.Global","NoContext.Individual",
                         "Context.Global","Context.Individual"))
}) )
rate$Group = factor(rate$Group,levels=c("Context.Individual","Context.Global",
                                        "NoContext.Individual","NoContext.Global" ))
# png("precision.png",width=6*ppi,height=3*ppi,res=ppi)
gg0<-ggplot(rate,aes(x=Length,y=value,color=Group))+
    geom_line()+
    geom_point(shape=21,fill="white")+
    theme_gray(base_size = 12)+
    ylab("Precision")
# dev.off()

# show that the approach is effective to users without learning data
# prediction$Learning.T.str = ifelse(prediction$Learning.T, 
#                                "Temporal Learning",
#                                "No Temporal Learning")
# prediction$Learning.S.str = ifelse(prediction$Learning.S, 
#                                "Spatial Learning",
#                                "No Spatial Learning")
# png("learning.length.comparision.png",width=6*ppi,height=3*ppi,res=ppi)
# ggplot(prediction,aes(x=ST.Individual2))+
#     geom_histogram(binwidth=1,fill=NA, color="#666666")+
# 
#     theme_gray(base_size=8)+
#     xlim(1,30)+
#     xlab("Length of Prediction List for Correct Prediction")+
#     ylab("Count")+
#     facet_grid(Learning.S.str~Learning.T.str)
# dev.off()

group1 = prediction[prediction$Learning.T + prediction$Learning.S==2,]
group2 = prediction[prediction$Learning.T + prediction$Learning.S==1,]
group3 = prediction[prediction$Learning.T + prediction$Learning.S==0,]
rate2 = do.call( rbind, lapply(1:30, function(N){ # list length: N
    # number of correct
    c1 = length(which(group1$ST.Individual2<=N)) / nrow(group1)
    c2 = length(which(group2$ST.Individual2<=N)) / nrow(group2)
    c3 = length(which(group3$ST.Individual2<=N)) / nrow(group3)

    data.frame("Length"=c(N,N,N),"value"=c(c1,c2,c3),
               "Individual.Available.Context"=c("Both","One","None"))
}) )
rate2$Individual.Available.Context = factor(rate2$Individual.Available.Context,
                                            levels=c("Both","One","None"))
# png("precision.learning.length.png",width=6*ppi,height=3*ppi,res=ppi)
gg1<-ggplot()+
    geom_point(data=rate2,
               aes(x=Length,y=value,shape=Individual.Available.Context),size=1.2)+
    geom_line(data=rate2,aes(x=Length,y=value,group=Individual.Available.Context),size=.3)+
    geom_line(data=rate[rate$Group %in% c("NoContext.Individual","Context.Individual"),],
              aes(x=Length,y=value,color=Group))+
    theme_gray(base_size = 12)+
    ylab("Precision")
# dev.off()

png("precision.combined.png",width=12*ppi,height=3*ppi,res=ppi)
grid.arrange(gg0,gg1,ncol=2)
dev.off()

# performance.vs.learning = do.call(rbind, lapply(split(prediction,prediction$user),
#                                             function(i){
#    data.frame("Precision"=length(which(i$ST.Individual<=5)) / nrow(i),
#               "User"=i[1,"user"],
#               "Learning.length"=i[1,"Learning.Length"])
# }))
# 
# ggplot(performance.vs.learning,aes(x=Learning.length,y=Precision))+
#     geom_point(alpha=.5,size=0.7)+
#     theme_gray(base_size=6)+
#     geom_smooth(method=glm)+
#     ggtitle("Precision @ N=5 V.S. Learning Length")+
#     scale_x_sqrt(breaks=c(0,1,25,100,200,400,800),limit=c(0,750))

# show how the performance is related with the popularity of a category
performance.vs.cate = do.call(rbind,
                              lapply(split(prediction,prediction$Real.Category),
                                     function(i){
                                         counter<<-counter+1
                                         print(counter)
    if(!is.na(i[1,"Real.Category"])){
        data.frame("Precision"=length(which(i$ST.Individual2<=5)) / nrow(i),
               "Real.Category"=i[1,"Real.Category"],
                "Massive.predicting"=nrow(i),
               "Massive.Learning"=length(which(learning.all.data$cate_l2==
                                                   i[1,"Real.Category"])))}
}))
# png("precision.caetgory.popularity.png",width=6*ppi,height=3*ppi,res=ppi)
gg2<-ggplot(performance.vs.cate,aes(x=Massive.Learning,y=Massive.predicting))+
    geom_point(shape=21,fill="white",aes(color=Precision))+
    scale_x_log10()+
    scale_y_log10()+
    theme_gray(base_size=10)+
    scale_color_gradient2(midpoint = 0.25)+
    ggtitle("Precision Along Category (Length=5)")
# dev.off()



```

We might consider: are there any connections between the precision and the variance (for a 
specific hour or polygon). Then we wonder: is it possible to quantify the "variance"
of each hour/polygon like what we did for numeric variables?

The anwser is found in this paper: Kadar & Perry: Variability for categorical variables

We implement their algorithms in the following function.

```{r}
unalikeability <- function(observations){
    n <- length(observations)
    pi <- sapply(unique(observations),function(category.i){
        length(which(observations==category.i)) /  n 
    })
    
    1 - sum(pi ^2)
}
```


```{r}
# perforamnce (on length of 5) v.s. hour
performance.vs.hour = do.call(rbind,lapply(split(prediction,prediction$Hour),function(i){
    
    hour = i[1,"Hour"]
    # learning data in that hour
    learning.sub = learning.all.data[learning.all.data$hour==hour,]
    data.frame("Precision"=length(which(i$ST.Individual2<=5)) / nrow(i),
               "Hour"=hour,
               "Unalikeability" = unalikeability(learning.sub$cate_l2),
               "Massive.predicting"=nrow(i),
               "Massive.Learning"=nrow(learning.sub))
}))

performance.vs.space = do.call(rbind,lapply(split(prediction,prediction$Postal), function(i){
    
    polygon = i[1,"Postal"]
    if(!is.na(polygon)){
        learning.sub = learning.all.data[learning.all.data$POSTAL==polygon,]
        data.frame("Precision"=length(which(i$ST.Individual2<=5)) / nrow(i),
               "Postal"=polygon,
               "Unalikeability" = unalikeability(learning.sub$cate_l2),
               "Massive.predicting"=nrow(i),
               "Massive.Learning"=nrow(learning.sub))
    }
}))

# png("precision.t.png",width=6*ppi,height=3*ppi,res=ppi)
gg3<-ggplot(performance.vs.hour,aes(x=Massive.Learning,y=Unalikeability))+
    geom_point(shape=21,fill="white",aes(color=Precision))+
    geom_text(aes(label=Hour),hjust=-0.3, vjust=0,size=3)+
    ggtitle("Precision Along Time (Length=5)")+
    theme_gray(base_size=10)+
    scale_color_gradient2(midpoint = 0.6)
# dev.off()

# png("precision.s.png",width=6*ppi,height=3*ppi,res=ppi)
gg4<-ggplot(performance.vs.space,aes(x=Massive.Learning,y=Unalikeability))+
    geom_point(shape=21,fill="white",aes(color=Precision))+
    ggtitle("Precision Along Space (Length=5)")+
    ylim(0.7,0.98)+
#     scale_x_sqrt()+
#     geom_smooth(method=glm)+
    scale_x_log10(breaks=c(10,100,1000,10000))+
    theme_gray(base_size=10)+
    scale_color_gradient2(midpoint = 0.6)
# dev.off()

png("performance.combined.png",width=12*ppi,height=3*ppi,res=ppi)
grid.arrange(gg2,gg3,gg4,ncol=3,nrow=1)
dev.off()
```

    